{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "519624fc-7c7a-4e54-92f6-4963a6cdd9d3",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da8ba5e8-287e-4b18-8a36-2e7f9896f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminary modules\n",
    "import base64 \n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import pause\n",
    "import os\n",
    "import io\n",
    "from io import BytesIO\n",
    "from dateutil import parser\n",
    "from dateutil.relativedelta import *\n",
    "from datetime import timedelta\n",
    "from PIL import Image\n",
    "import math\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import copy\n",
    "\n",
    "#OpenAI\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "#Streamlit\n",
    "import streamlit as st\n",
    "#from streamlit_gsheets import GSheetsConnection\n",
    "#from streamlit.components.v1 import html\n",
    "import streamlit_ext as ste\n",
    "\n",
    "#PandasAI\n",
    "#from dotenv import load_dotenv\n",
    "from pandasai import SmartDataframe\n",
    "from pandasai import Agent\n",
    "#from pandasai.llm import BambooLLM\n",
    "from pandasai.llm.openai import OpenAI\n",
    "import pandasai as pai\n",
    "from pandasai.responses.streamlit_response import StreamlitResponse\n",
    "from pandasai.helpers.openai_info import get_openai_callback as pandasai_get_openai_callback\n",
    "\n",
    "#Excel\n",
    "import openpyxl\n",
    "from pyxlsb import open_workbook as open_xlsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060e311a-e871-4e97-a59c-909c43fc3820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_functions import check_questions_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df8463a-f127-471c-ae05-af2efde62cf0",
   "metadata": {},
   "source": [
    "# gpt-3.5, 4o-mini and 4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f01e0d-be0c-4c2c-aaa1-94d932bde51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions for GPT are capped at 1000 characters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Upperbound on the length of questions for GPT\n",
    "\n",
    "question_characters_bound = 2000\n",
    "\n",
    "print(f\"Questions for GPT are capped at {question_characters_bound} characters.\\n\")\n",
    "\n",
    "#Upperbound on number of judgments to scrape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74be555a-f376-4d43-9bb0-0e12ef726522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function to split a string into a list by line\n",
    "def split_by_line(x):\n",
    "    y = x.split('\\n')\n",
    "    for i in y:\n",
    "        if len(i) == 0:\n",
    "            y.remove(i)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b56a336-77a9-40f8-bc0c-af9b1ac77100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function to split a list into a dictionary for list items longer than 10 characters\n",
    "#Apply split_by_line() before the following function\n",
    "def GPT_label_dict(x_list):\n",
    "    GPT_dict = {}\n",
    "    for i in x_list:\n",
    "        if len(i) > 10:\n",
    "            GPT_index = x_list.index(i) + 1\n",
    "            i_label = 'GPT question ' + f'{GPT_index}'\n",
    "            GPT_dict.update({i_label: i})\n",
    "    return GPT_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250b2292-f622-426d-a1ea-66bb29fb7a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check validity of API key\n",
    "\n",
    "@st.cache_data\n",
    "def is_api_key_valid(key_to_check):\n",
    "    openai.api_key = key_to_check\n",
    "    \n",
    "    try:\n",
    "        completion = openai.chat.completions.create(\n",
    "            #model=\"gpt-3.5-turbo-0125\",\n",
    "            model = 'gpt-4o-mini', \n",
    "            messages=[{\"role\": \"user\", \"content\": 'Hi'}], \n",
    "            max_tokens = 1\n",
    "        )\n",
    "    except:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d514f02e-4163-4afa-94da-6f01e996b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define input and output costs, token caps and maximum characters\n",
    "#each token is about 4 characters\n",
    "\n",
    "def gpt_input_cost(gpt_model):\n",
    "    if gpt_model == \"gpt-3.5-turbo-0125\":\n",
    "        gpt_input_cost = 1/1000000*0.5\n",
    "        \n",
    "    if gpt_model == \"gpt-4o\":\n",
    "        gpt_input_cost = 1/1000000*5\n",
    "\n",
    "    if gpt_model == \"gpt-4o-mini\":\n",
    "        gpt_input_cost = 1/1000000*0.15\n",
    "    return gpt_input_cost\n",
    "\n",
    "def gpt_output_cost(gpt_model):\n",
    "    if gpt_model == \"gpt-3.5-turbo-0125\":\n",
    "        gpt_output_cost = 1/1000000*1.5\n",
    "        \n",
    "    if gpt_model == \"gpt-4o\":\n",
    "        gpt_output_cost = 1/1000000*15\n",
    "\n",
    "    if gpt_model == \"gpt-4o-mini\":\n",
    "        gpt_output_cost = 1/1000000*0.6\n",
    "    \n",
    "    return gpt_output_cost\n",
    "\n",
    "#As of 2024-06-07, questions are capped at about 1000 characters ~ 250 tokens, role_content/system_instruction is about 115 tokens, json_direction is about 11 tokens, answers_json is about 8 tokens plus 30 tokens per question \n",
    "\n",
    "def tokens_cap(gpt_model):\n",
    "    #This is the global cap for each model, which will be shown to users\n",
    "    #Leaving 1000 tokens to spare\n",
    "    \n",
    "    if gpt_model == \"gpt-3.5-turbo-0125\":\n",
    "        \n",
    "        tokens_cap = int(16385 - 3000) #For GPT-3.5-turbo, token limit covering BOTH input and output is 16385,  while the output limit is 4096.\n",
    "    \n",
    "    if gpt_model == \"gpt-4o\":\n",
    "        tokens_cap = int(128000 - 3000) #For gpt-4o, token limit covering both BOTH and output is 128000, while the output limit is 4096.\n",
    "\n",
    "    if gpt_model == \"gpt-4o-mini\":\n",
    "        tokens_cap = int(128000 - 3000) #For gpt-4o-mini, token limit covering both BOTH and output is 128000, while the output limit is 4096.\n",
    "\n",
    "    return tokens_cap\n",
    "\n",
    "def max_output(gpt_model, messages_for_GPT):\n",
    "\n",
    "    if gpt_model == \"gpt-3.5-turbo-0125\":\n",
    "        \n",
    "        max_output_tokens = int(16385 - num_tokens_from_string(str(messages_for_GPT), \"cl100k_base\")) #For GPT-3.5-turbo, token limit covering BOTH input and output is 16385,  while the output limit is 4096.\n",
    "    \n",
    "    if gpt_model == \"gpt-4o\":\n",
    "        \n",
    "        max_output_tokens = int(128000 - num_tokens_from_string(str(messages_for_GPT), \"cl100k_base\")) #For gpt-4o, token limit covering both BOTH and output is 128000, while the output limit is 4096.\n",
    "    \n",
    "    if gpt_model == \"gpt-4o-mini\":\n",
    "        \n",
    "        max_output_tokens = int(128000 - num_tokens_from_string(str(messages_for_GPT), \"cl100k_base\")) #For gpt-4o-mini, token limit covering both BOTH and output is 128000, while the output limit is 4096.\n",
    "\n",
    "    return min(4096, abs(max_output_tokens))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "891018f7-b3c1-4af0-a666-b12a487d6896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokens estimate preliminaries\n",
    "\n",
    "#encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "#encoding = tiktoken.encoding_for_model(gpt_model)\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    #Tokens estimate function\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    \n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e331f47e-cc3b-4762-a743-bb82d766cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judgment_prompt_json(judgment_json, gpt_model):\n",
    "\n",
    "    #Remove hyperlink\n",
    "    for key in judgment_json.keys():\n",
    "        if 'hyperlink' in key.lower():\n",
    "            judgment_json[key] = ''\n",
    "            break\n",
    "    \n",
    "    #Turn judgment to string\n",
    "    if isinstance(judgment_json[\"judgment\"], list):\n",
    "        judgment_to_string = '\\n'.join(judgment_json[\"judgment\"])\n",
    "        \n",
    "    elif isinstance(judgment_json[\"judgment\"], str):\n",
    "        judgment_to_string = judgment_json[\"judgment\"]\n",
    "        \n",
    "    else:\n",
    "        judgment_to_string = str(judgment_json[\"judgment\"])\n",
    "\n",
    "    #Truncate judgment if needed\n",
    "    judgment_content = f'Based on the metadata and judgment in the following JSON: \"\"\" {json.dumps(judgment_json, default=str)} \"\"\"'\n",
    "\n",
    "    judgment_content_tokens = num_tokens_from_string(judgment_content, \"cl100k_base\")\n",
    "    \n",
    "    if judgment_content_tokens <= tokens_cap(gpt_model):\n",
    "        \n",
    "        return judgment_content\n",
    "\n",
    "    else:\n",
    "        \n",
    "        meta_data_len = judgment_content_tokens - num_tokens_from_string(judgment_to_string, \"cl100k_base\")\n",
    "\n",
    "        intro_len = num_tokens_from_string('Based on the metadata and judgment in the following JSON: \"\"\"  \"\"\"', \"cl100k_base\")\n",
    "        \n",
    "        judgment_chars_capped = int(round((tokens_cap(gpt_model) - meta_data_len - intro_len)*4))\n",
    "        \n",
    "        judgment_string_trimmed = judgment_to_string[ :int(judgment_chars_capped/2)] + judgment_to_string[-int(judgment_chars_capped/2): ]\n",
    "\n",
    "        judgment_json[\"judgment\"] = judgment_string_trimmed     \n",
    "        \n",
    "        judgment_content_capped = f'Based on the metadata and judgment in the following JSON:  \"\"\" {json.dumps(judgment_json, default=str)} \"\"\"'\n",
    "        \n",
    "        return judgment_content_capped\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2252c08-65de-4b69-b132-64d5c72f411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check questions for potential privacy infringement\n",
    "\n",
    "questions_check_system_instruction = \"\"\"\n",
    "You are a compliance officer helping a human ethics committee to ensure that no personally identifiable information will be exposed. \n",
    "You will be given questions to check in JSON form. Please provide labels for these questions based only on information contained in the JSON.\n",
    "Where a question seeks information about a person's birthday or address, you label \"1\". If a question does not seek such information, you label \"0\". If you are not sure, label \"unclear\".\n",
    "\"\"\"\n",
    "\n",
    "#More general below\n",
    "#questions_check_system_instruction = \"\"\"\n",
    "#You are a compliance officer helping a human ethics committee to ensure that no personally identifiable information will be exposed. \n",
    "#You will be given questions to check in JSON form. \n",
    "#Based only on information contained in the JSON, please check each question for whether it seeks a person's birthday, address, or other personally identifiable information. \n",
    "#Where a question indeed seeks personally identifiable information, you label \"1\". \n",
    "#Where a question does not seek personally identifiable information, you label \"0\". \n",
    "#If you are not sure, label \"unclear\".\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2735d310-08d5-4a3c-af07-7864eaa2e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check questions for potential privacy infringement\n",
    "\n",
    "@st.cache_data\n",
    "def GPT_questions_check(questions_json, gpt_model, questions_check_system_instruction):\n",
    "    #'question_json' variable is a json of questions to GPT\n",
    "    #'jugdment' variable is a judgment_json   \n",
    "\n",
    "    #judgment_for_GPT = [{\"role\": \"user\", \"content\": judgment_prompt_json(judgment_json, gpt_model)}]\n",
    "\n",
    "    json_direction = [{\"role\": \"user\", \"content\": 'Label the following questions in JSON form.'}]\n",
    "\n",
    "    #Create answer format\n",
    "    \n",
    "    q_keys = [*questions_json]\n",
    "    \n",
    "    labels_json = {}\n",
    "    \n",
    "    for q_index in q_keys:\n",
    "        labels_json.update({q_index: 'Your label for the question with index ' + q_index})\n",
    "    \n",
    "    #Create questions, which include the answer format\n",
    "    \n",
    "    question_to_check = [{\"role\": \"user\", \"content\": json.dumps(questions_json, default = str) + ' Return labels in the following JSON form: ' + json.dumps(labels_json, default = str)}]\n",
    "    \n",
    "    #Create messages in one prompt for GPT\n",
    "    \n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": questions_check_system_instruction}]\n",
    "    #messages_for_GPT = intro_for_GPT + judgment_for_GPT + json_direction + question_to_check\n",
    "    messages_for_GPT = intro_for_GPT + json_direction + question_to_check\n",
    "    \n",
    "#   return messages_for_GPT\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    try:\n",
    "        #completion = client.chat.completions.create(\n",
    "        completion = openai.chat.completions.create(\n",
    "            model = gpt_model,\n",
    "            messages = messages_for_GPT, \n",
    "            response_format = {\"type\": \"json_object\"}, \n",
    "            max_tokens = max_output(gpt_model, messages_for_GPT), \n",
    "            temperature = 0.1, \n",
    "            #top_p = 0.1\n",
    "        )\n",
    "        \n",
    "#        return completion.choices[0].message.content #This gives answers as a string containing a dictionary\n",
    "        \n",
    "        #To obtain a json directly, use below\n",
    "        labels_dict = json.loads(completion.choices[0].message.content)\n",
    "        \n",
    "        #Obtain tokens\n",
    "        output_tokens = completion.usage.completion_tokens\n",
    "        \n",
    "        prompt_tokens = completion.usage.prompt_tokens\n",
    "        \n",
    "        return [labels_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "    except Exception as error:\n",
    "        \n",
    "        for q_index in q_keys:\n",
    "            labels_json[q_index] = error\n",
    "        \n",
    "        return [labels_json, 0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba47e604-6133-4a1f-8f30-844246ee5355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to replace unchecked questions with checked questions\n",
    "def checked_questions_json(questions_json, gpt_labels_output):\n",
    "    \n",
    "    checked_questions_json = questions_json\n",
    "\n",
    "    for q_key in gpt_labels_output[0]:\n",
    "        \n",
    "        if str(gpt_labels_output[0][q_key]) == '1':\n",
    "            \n",
    "            checked_questions_json[q_key] = 'Say Say \"n/a\" only.'\n",
    "    \n",
    "    return checked_questions_json\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "768bc5f3-d1ef-4cd8-a353-86a4e6822ea5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Check questions for potential privacy infringement\n",
    "\n",
    "answers_check_system_instruction = \"\"\"\n",
    "You are a compliance officer helping an academic researcher to redact birthdays and addresses. \n",
    "You will be given text to check in JSON form. Please check the text based only on information contained in the JSON. \n",
    "Where any part of the text contains a birthday or an address, you replace that part with \"[redacted]\". \n",
    "You then return the remainder of the text unredacted.\n",
    "\"\"\"\n",
    "\n",
    "#If more general below\n",
    "#answers_check_system_instruction = \"\"\"\n",
    "#You are a compliance officer helping an academic researcher to redact personally identifiable information. \n",
    "#You will be given text to check in JSON form. \n",
    "#Based only on information contained in the JSON, please check each text for whether it contains a person's birthday, address, or other personally identifiable information. \n",
    "#Where any part of the text contains personally identifiable information, you replace that part with \"[redacted]\". \n",
    "#You then return the remainder of the text unredacted.\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeaf1a7f-9229-4843-b341-1f29fab94b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check answers_to_check_json for potential privacy infringement\n",
    "\n",
    "@st.cache_data\n",
    "def GPT_answers_check(answers_to_check_json, gpt_model, answers_check_system_instruction):\n",
    "    #'question_json' variable is a json of answers_to_check_json to GPT\n",
    "    #'jugdment' variable is a judgment_json   \n",
    "\n",
    "    #judgment_for_GPT = [{\"role\": \"user\", \"content\": judgment_prompt_json(judgment_json, gpt_model)}]\n",
    "\n",
    "    json_direction = [{\"role\": \"user\", \"content\": 'Check the following text in JSON form.'}]\n",
    "\n",
    "    #Create answer format\n",
    "    \n",
    "    q_keys = [*answers_to_check_json]\n",
    "    \n",
    "    redacted_answers_json = {}\n",
    "    \n",
    "    for q_index in q_keys:\n",
    "        \n",
    "        redacted_answers_json.update({q_index: 'Your answer for the question with index ' + q_index})\n",
    "    \n",
    "    #Create answers_to_check_json, which include the answer format\n",
    "    \n",
    "    question_to_check = [{\"role\": \"user\", \"content\": json.dumps(answers_to_check_json, default = str) + ' Answer in the following JSON form: ' + json.dumps(redacted_answers_json, default = str)}]\n",
    "    \n",
    "    #Create messages in one prompt for GPT\n",
    "    \n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": answers_check_system_instruction}]\n",
    "    #messages_for_GPT = intro_for_GPT + judgment_for_GPT + json_direction + question_to_check\n",
    "    messages_for_GPT = intro_for_GPT + json_direction + question_to_check\n",
    "    \n",
    "#   return messages_for_GPT\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    try:\n",
    "        #completion = client.chat.completions.create(\n",
    "        completion = openai.chat.completions.create(\n",
    "            model = gpt_model,\n",
    "            messages = messages_for_GPT, \n",
    "            response_format = {\"type\": \"json_object\"}, \n",
    "            max_tokens = max_output(gpt_model, messages_for_GPT), \n",
    "            temperature = 0.1, \n",
    "            #top_p = 0.1\n",
    "        )\n",
    "        \n",
    "#        return completion.choices[0].message.content #This gives answers as a string containing a dictionary\n",
    "        \n",
    "        #To obtain a json directly, use below\n",
    "        answers_dict = json.loads(completion.choices[0].message.content)\n",
    "        \n",
    "        #Obtain tokens\n",
    "        output_tokens = completion.usage.completion_tokens\n",
    "        \n",
    "        prompt_tokens = completion.usage.prompt_tokens\n",
    "        \n",
    "        return [answers_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "    except Exception as error:\n",
    "        \n",
    "        for q_index in q_keys:\n",
    "            redacted_answers_json[q_index] = error\n",
    "        \n",
    "        return [redacted_answers_json, 0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bae612f3-eabf-4dfe-abbb-2d97adf5558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For modern judgments, define system role content for GPT\n",
    "role_content = \"You are a legal research assistant helping an academic researcher to answer questions about a public judgment. You will be provided with the judgment and metadata in JSON form. Please answer questions based only on information contained in the judgment and metadata. Where your answer comes from specific paragraphs, pages or sections of the judgment or metadata, include a reference to those paragraphs, pages or sections. If you cannot answer the questions based on the judgment or metadata, do not make up information, but instead write 'answer not found'. \"\n",
    "\n",
    "#safeguards = \"Where you are asked to identify a party's birthday, address, or other personally identifiable information, answer 'potential privacy violation'. \" \n",
    "\n",
    "#role_content = role_content_raw# + safeguards\n",
    "\n",
    "#role_content = 'You are a legal research assistant helping an academic researcher to answer questions about a public judgment. You will be provided with the judgment and metadata in JSON form. Please answer questions based only on information contained in the judgment and metadata. Where your answer comes from a part of the judgment or metadata, include a reference to that part of the judgment or metadata. If you cannot answer the questions based on the judgment or metadata, do not make up information, but instead write \"answer not found\". '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b06e2156-74f5-41df-94c0-a539ad91a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT answer function for answers in json form, YES TOKENS\n",
    "#IN USE\n",
    "\n",
    "@st.cache_data\n",
    "def GPT_json(questions_json, judgment_json, gpt_model, system_instruction):\n",
    "    #'question_json' variable is a json of questions to GPT\n",
    "    #'jugdment' variable is a judgment_json   \n",
    "\n",
    "    judgment_for_GPT = [{\"role\": \"user\", \"content\": judgment_prompt_json(judgment_json, gpt_model)}]\n",
    "\n",
    "    json_direction = [{\"role\": \"user\", \"content\": 'You will be given questions to answer in JSON form.'}]\n",
    "\n",
    "    #Create answer format\n",
    "    \n",
    "    q_keys = [*questions_json]\n",
    "    \n",
    "    answers_json = {}\n",
    "    \n",
    "    for q_index in q_keys:\n",
    "        answers_json.update({questions_json[q_index]: f'Your answer to this question. (The paragraphs, pages or sections from which you obtained your answer)'})\n",
    "    \n",
    "    #Create questions, which include the answer format\n",
    "    \n",
    "    question_for_GPT = [{\"role\": \"user\", \"content\": json.dumps(questions_json, default = str) + ' Give responses in the following JSON form: ' + json.dumps(answers_json, default = str)}]\n",
    "    \n",
    "    #Create messages in one prompt for GPT\n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction}]\n",
    "    messages_for_GPT = intro_for_GPT + judgment_for_GPT + json_direction + question_for_GPT\n",
    "    \n",
    "#   return messages_for_GPT\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    try:\n",
    "        #completion = client.chat.completions.create(\n",
    "        completion = openai.chat.completions.create(\n",
    "            model = gpt_model,\n",
    "            messages = messages_for_GPT, \n",
    "            response_format = {\"type\": \"json_object\"}, \n",
    "            max_tokens = max_output(gpt_model, messages_for_GPT), \n",
    "            temperature = 0.1, \n",
    "            #top_p = 0.1\n",
    "        )\n",
    "        \n",
    "#        return completion.choices[0].message.content #This gives answers as a string containing a dictionary\n",
    "        \n",
    "        #To obtain a json directly, use below\n",
    "        answers_dict = json.loads(completion.choices[0].message.content)\n",
    "        \n",
    "        #Obtain tokens\n",
    "        output_tokens = completion.usage.completion_tokens\n",
    "        \n",
    "        prompt_tokens = completion.usage.prompt_tokens\n",
    "        \n",
    "        #return [answers_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "        #Check answers\n",
    "\n",
    "        if check_questions_answers() > 0:\n",
    "            \n",
    "            try:\n",
    "                redacted_output = GPT_answers_check(answers_dict, gpt_model, answers_check_system_instruction)\n",
    "        \n",
    "                redacted_answers_dict = redacted_output[0]\n",
    "        \n",
    "                redacted_answers_output_tokens = redacted_output[1]\n",
    "        \n",
    "                redacted_answers_prompt_tokens = redacted_output[2]\n",
    "        \n",
    "                return [redacted_answers_dict, output_tokens + redacted_answers_output_tokens, prompt_tokens + redacted_answers_prompt_tokens]\n",
    "\n",
    "                print('Answers checked.')\n",
    "                \n",
    "            except Exception as e:\n",
    "    \n",
    "                print('Answers check failed.')\n",
    "    \n",
    "                print(e)\n",
    "    \n",
    "                return [answers_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "        else:\n",
    "\n",
    "            print('Answers not checked.')\n",
    "            \n",
    "            return [answers_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "    except Exception as error:\n",
    "        \n",
    "        print('GPT failed to produce answers.')\n",
    "        \n",
    "        for q_index in q_keys:\n",
    "            \n",
    "            answers_json[q_index] = error\n",
    "        \n",
    "        return [answers_json, 0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71837ede-4057-4eae-abd6-b386a0b701d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT function for each respondent's dataframe, index by judgment then question, with input and output tokens given by GPT itself\n",
    "#IN USE\n",
    "\n",
    "#The following function DOES NOT check for existence of questions for GPT\n",
    "    # To so check, active line marked as #*\n",
    "\n",
    "@st.cache_data\n",
    "def engage_GPT_json(questions_json, df_individual, GPT_activation, gpt_model, system_instruction):\n",
    "    # Variable questions_json refers to the json of questions\n",
    "    # Variable df_individual refers to each respondent's df\n",
    "    # Variable activation refers to status of GPT activation (real or test)\n",
    "    # The output is a new JSON for the relevant respondent with new columns re:\n",
    "        # f\"Judgment length in tokens (up to {tokens_cap(gpt_model)} given to GPT)\"\n",
    "        # 'GPT cost estimate (USD excl GST)'\n",
    "        # 'GPT time estimate (seconds)'\n",
    "        # GPT questions/answers\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "\n",
    "    #Make a copy of questions for making headings later\n",
    "    unchecked_questions_json = questions_json.copy()\n",
    "\n",
    "    #Check questions for privacy violation\n",
    "\n",
    "    if check_questions_answers() > 0:\n",
    "    \n",
    "        try:\n",
    "    \n",
    "            labels_output = GPT_questions_check(questions_json, gpt_model, questions_check_system_instruction)\n",
    "    \n",
    "            labels_output_tokens = labels_output[1]\n",
    "    \n",
    "            labels_prompt_tokens = labels_output[2]\n",
    "        \n",
    "            questions_json = checked_questions_json(questions_json, labels_output)\n",
    "\n",
    "            print('Questions checked.')\n",
    "    \n",
    "        except Exception as e:\n",
    "            \n",
    "            print('Questions check failed.')\n",
    "            \n",
    "            print(e)\n",
    "    \n",
    "            labels_output_tokens = 0\n",
    "            \n",
    "            labels_prompt_tokens = 0\n",
    "\n",
    "    else:\n",
    "\n",
    "        print('Questions not checked.')\n",
    "        \n",
    "        labels_output_tokens = 0\n",
    "        \n",
    "        labels_prompt_tokens = 0\n",
    "\n",
    "    #Process questions\n",
    "    \n",
    "    for judgment_index in df_individual.index:\n",
    "        \n",
    "        judgment_json = df_individual.to_dict('index')[judgment_index]\n",
    "        \n",
    "        #Calculate and append number of tokens of judgment, regardless of whether given to GPT\n",
    "        judgment_tokens = num_tokens_from_string(str(judgment_json), \"cl100k_base\")\n",
    "        df_individual.loc[judgment_index, f\"Judgment length in tokens (up to {tokens_cap(gpt_model)} given to GPT)\"] = judgment_tokens       \n",
    "\n",
    "        #Indicate whether judgment truncated\n",
    "        \n",
    "        df_individual.loc[judgment_index, \"Judgment truncated (if given to GPT)?\"] = ''       \n",
    "        \n",
    "        if judgment_tokens <= tokens_cap(gpt_model):\n",
    "            \n",
    "            df_individual.loc[judgment_index, \"Judgment truncated (if given to GPT)?\"] = 'No'\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            df_individual.loc[judgment_index, \"Judgment truncated (if given to GPT)?\"] = 'Yes'\n",
    "\n",
    "        #Create columns for respondent's GPT cost, time\n",
    "        df_individual.loc[judgment_index, 'GPT cost estimate (USD excl GST)'] = ''\n",
    "        df_individual.loc[judgment_index, 'GPT time estimate (seconds)'] = ''\n",
    "                \n",
    "        #Calculate GPT start time\n",
    "\n",
    "        GPT_start_time = datetime.now()\n",
    "\n",
    "        #Depending on activation status, apply GPT_json function to each judgment, gives answers as a string containing a dictionary\n",
    "\n",
    "        if int(GPT_activation) > 0:\n",
    "            GPT_output_list = GPT_json(questions_json, judgment_json, gpt_model, system_instruction) #Gives [answers as a JSON, output tokens, input tokens]\n",
    "            answers_dict = GPT_output_list[0]\n",
    "\n",
    "            #Calculate and append GPT finish time and time difference to individual df\n",
    "            GPT_finish_time = datetime.now()\n",
    "            \n",
    "            GPT_time_difference = GPT_finish_time - GPT_start_time\n",
    "    \n",
    "            df_individual.loc[judgment_index, 'GPT time estimate (seconds)'] = GPT_time_difference.total_seconds()        \n",
    "\n",
    "        else:\n",
    "            answers_dict = {}\n",
    "            \n",
    "            question_keys = [*questions_json]\n",
    "            \n",
    "            for q_index in question_keys:\n",
    "                #Increases judgment index by 2 to ensure consistency with Excel spreadsheet\n",
    "                answer = 'Placeholder answer for ' + ' judgment ' + str(int(judgment_index) + 2) + ' ' + str(q_index)\n",
    "                answers_dict.update({q_index: answer})\n",
    "            \n",
    "            #Own calculation of GPT costs for Placeholder answer fors\n",
    "\n",
    "            #Calculate capped judgment tokens\n",
    "\n",
    "            judgment_capped_tokens = num_tokens_from_string(judgment_prompt_json(judgment_json, gpt_model), \"cl100k_base\")\n",
    "\n",
    "            #Calculate questions tokens and cost\n",
    "\n",
    "            questions_tokens = num_tokens_from_string(json.dumps(questions_json, default = str), \"cl100k_base\")\n",
    "\n",
    "            #Calculate other instructions' tokens\n",
    "\n",
    "            other_instructions = system_instruction + 'you will be given questions to answer in JSON form.' + ' Give responses in the following JSON form: '\n",
    "\n",
    "            other_tokens = num_tokens_from_string(other_instructions, \"cl100k_base\") + len(question_keys)*num_tokens_from_string(\"GPT question x:  Your answer to the question with index GPT question x. The paragraph or page numbers in the judgment, or sections of the metadata from which you obtained your answer. \", \"cl100k_base\")\n",
    "\n",
    "            #Calculate number of tokens of answers\n",
    "            answers_tokens = num_tokens_from_string(str(answers_dict), \"cl100k_base\")\n",
    "\n",
    "            input_tokens = judgment_capped_tokens + questions_tokens + other_tokens\n",
    "            \n",
    "            GPT_output_list = [answers_dict, answers_tokens, input_tokens]\n",
    "\n",
    "    \t#Create GPT question headings, append answers to individual spreadsheets, and remove template answers\n",
    "        \n",
    "        for answer_index in answers_dict.keys():\n",
    "\n",
    "            #Check any question override\n",
    "            if 'Say \"n/a\" only' in str(answer_index):\n",
    "                answer_header = 'GPT question: ' + 'Not answered due to potential privacy violation'\n",
    "            else:\n",
    "                answer_header = 'GPT question: ' + answer_index\n",
    "            \n",
    "            df_individual.loc[judgment_index, answer_header] = answers_dict[answer_index]\n",
    "            \n",
    "        #Calculate GPT costs\n",
    "\n",
    "        #If no check for questions\n",
    "        #GPT_cost = GPT_output_list[1]*gpt_output_cost(gpt_model) + GPT_output_list[2]*gpt_input_cost(gpt_model)\n",
    "\n",
    "        #If check for questions\n",
    "        GPT_cost = (GPT_output_list[1] + labels_output_tokens/len(df_individual))*gpt_output_cost(gpt_model) + (GPT_output_list[2] + labels_prompt_tokens/len(df_individual))*gpt_input_cost(gpt_model)\n",
    "\n",
    "        #Calculate and append GPT cost to individual df\n",
    "        df_individual.loc[judgment_index, 'GPT cost estimate (USD excl GST)'] = GPT_cost\n",
    "    \n",
    "    return df_individual\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e8b51-2d15-4be5-9d0a-f37dd3d25dd1",
   "metadata": {},
   "source": [
    "# Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02a9b3f7-84a5-4852-9515-b0a3ee0a7178",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Tokens counter\n",
    "\n",
    "def get_image_dims(image):\n",
    "    if re.match(r\"data:image\\/\\w+;base64\", image):\n",
    "        image = re.sub(r\"data:image\\/\\w+;base64,\", \"\", image)\n",
    "        image = Image.open(BytesIO(base64.b64decode(image)))\n",
    "        return image.size\n",
    "    else:\n",
    "        raise ValueError(\"Image must be a base64 string.\")\n",
    "\n",
    "def calculate_image_token_cost(image, detail=\"auto\"):\n",
    "    # Constants\n",
    "    LOW_DETAIL_COST = 85\n",
    "    HIGH_DETAIL_COST_PER_TILE = 170\n",
    "    ADDITIONAL_COST = 85\n",
    "\n",
    "    if detail == \"auto\":\n",
    "        # assume high detail for now\n",
    "        detail = \"high\"\n",
    "\n",
    "    if detail == \"low\":\n",
    "        # Low detail images have a fixed cost\n",
    "        return LOW_DETAIL_COST\n",
    "    elif detail == \"high\":\n",
    "        # Calculate token cost for high detail images\n",
    "        width, height = get_image_dims(image)\n",
    "        # Check if resizing is needed to fit within a 2048 x 2048 square\n",
    "        if max(width, height) > 2048:\n",
    "            # Resize the image to fit within a 2048 x 2048 square\n",
    "            ratio = 2048 / max(width, height)\n",
    "            width = int(width * ratio)\n",
    "            height = int(height * ratio)\n",
    "        # Further scale down to 768px on the shortest side\n",
    "        if min(width, height) > 768:\n",
    "            ratio = 768 / min(width, height)\n",
    "            width = int(width * ratio)\n",
    "            height = int(height * ratio)\n",
    "        # Calculate the number of 512px squares\n",
    "        num_squares = math.ceil(width / 512) * math.ceil(height / 512)\n",
    "        # Calculate the total token cost\n",
    "        total_cost = num_squares * HIGH_DETAIL_COST_PER_TILE + ADDITIONAL_COST\n",
    "        return total_cost\n",
    "    else:\n",
    "        # Invalid detail_option\n",
    "        raise ValueError(\"Invalid value for detail parameter. Use 'low' or 'high'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ecd5f-d8bc-4840-95e3-83ebab16ab89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
