{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "519624fc-7c7a-4e54-92f6-4963a6cdd9d3",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da8ba5e8-287e-4b18-8a36-2e7f9896f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminary modules\n",
    "import base64 \n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import pause\n",
    "import os\n",
    "import io\n",
    "from io import BytesIO\n",
    "from dateutil import parser\n",
    "from dateutil.relativedelta import *\n",
    "from datetime import timedelta\n",
    "from PIL import Image\n",
    "import math\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "#OpenAI\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "#Streamlit\n",
    "import streamlit as st\n",
    "#from streamlit_gsheets import GSheetsConnection\n",
    "#from streamlit.components.v1 import html\n",
    "import streamlit_ext as ste\n",
    "\n",
    "#PandasAI\n",
    "#from dotenv import load_dotenv\n",
    "from pandasai import SmartDataframe\n",
    "from pandasai import Agent\n",
    "#from pandasai.llm import BambooLLM\n",
    "from pandasai.llm.openai import OpenAI\n",
    "import pandasai as pai\n",
    "from pandasai.responses.streamlit_response import StreamlitResponse\n",
    "from pandasai.helpers.openai_info import get_openai_callback as pandasai_get_openai_callback\n",
    "\n",
    "#Excel\n",
    "import openpyxl\n",
    "from pyxlsb import open_workbook as open_xlsb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df8463a-f127-471c-ae05-af2efde62cf0",
   "metadata": {},
   "source": [
    "# gpt-3.5, 4o-mini and 4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90f01e0d-be0c-4c2c-aaa1-94d932bde51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions for GPT are capped at 1000 characters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Upperbound on the length of questions for GPT\n",
    "#if 'question_characters_bound' not in st.session_state:\n",
    "#    st.session_state['question_characters_bound'] = 1000\n",
    "\n",
    "question_characters_bound = 1000\n",
    "\n",
    "print(f\"Questions for GPT are capped at {question_characters_bound} characters.\\n\")\n",
    "\n",
    "#Upperbound on number of judgments to scrape\n",
    "\n",
    "#Default judgment counter bound\n",
    "\n",
    "default_judgment_counter_bound = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74be555a-f376-4d43-9bb0-0e12ef726522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function to split a string into a list by line\n",
    "def split_by_line(x):\n",
    "    y = x.split('\\n')\n",
    "    for i in y:\n",
    "        if len(i) == 0:\n",
    "            y.remove(i)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b56a336-77a9-40f8-bc0c-af9b1ac77100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function to split a list into a dictionary for list items longer than 10 characters\n",
    "#Apply split_by_line() before the following function\n",
    "def GPT_label_dict(x_list):\n",
    "    GPT_dict = {}\n",
    "    for i in x_list:\n",
    "        if len(i) > 10:\n",
    "            GPT_index = x_list.index(i) + 1\n",
    "            i_label = 'GPT question ' + f'{GPT_index}'\n",
    "            GPT_dict.update({i_label: i})\n",
    "    return GPT_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "250b2292-f622-426d-a1ea-66bb29fb7a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check validity of API key\n",
    "\n",
    "def is_api_key_valid(key_to_check):\n",
    "    openai.api_key = key_to_check\n",
    "    \n",
    "    try:\n",
    "        completion = openai.chat.completions.create(\n",
    "            #model=\"gpt-3.5-turbo-0125\",\n",
    "            model = 'gpt-4o-mini', \n",
    "            messages=[{\"role\": \"user\", \"content\": '1+1='}], \n",
    "            max_tokens = 1\n",
    "        )\n",
    "    except:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d514f02e-4163-4afa-94da-6f01e996b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define input and output costs, token caps and maximum characters\n",
    "#each token is about 4 characters\n",
    "\n",
    "def gpt_input_cost(gpt_model):\n",
    "    if gpt_model == \"gpt-3.5-turbo-0125\":\n",
    "        gpt_input_cost = 1/1000000*0.5\n",
    "        \n",
    "    if gpt_model == \"gpt-4o\":\n",
    "        gpt_input_cost = 1/1000000*5\n",
    "\n",
    "    if gpt_model == \"gpt-4o-mini\":\n",
    "        gpt_input_cost = 1/1000000*0.15\n",
    "    return gpt_input_cost\n",
    "\n",
    "def gpt_output_cost(gpt_model):\n",
    "    if gpt_model == \"gpt-3.5-turbo-0125\":\n",
    "        gpt_output_cost = 1/1000000*1.5\n",
    "        \n",
    "    if gpt_model == \"gpt-4o\":\n",
    "        gpt_output_cost = 1/1000000*15\n",
    "\n",
    "    if gpt_model == \"gpt-4o-mini\":\n",
    "        gpt_output_cost = 1/1000000*0.6\n",
    "    \n",
    "    return gpt_output_cost\n",
    "\n",
    "#As of 2024-06-07, questions are capped at about 1000 characters ~ 250 tokens, role_content/system_instruction is about 115 tokens, json_direction is about 11 tokens, answers_json is about 8 tokens plus 30 tokens per question \n",
    "\n",
    "def tokens_cap(gpt_model):\n",
    "    #This is the global cap for each model, which will be shown to users\n",
    "    #Leaving 1000 tokens to spare\n",
    "    \n",
    "    if gpt_model == \"gpt-3.5-turbo-0125\":\n",
    "        \n",
    "        tokens_cap = int(16385 - 3000) #For GPT-3.5-turbo, token limit covering BOTH input and output is 16385,  while the output limit is 4096.\n",
    "    \n",
    "    if gpt_model == \"gpt-4o\":\n",
    "        tokens_cap = int(128000 - 3000) #For gpt-4o, token limit covering both BOTH and output is 128000, while the output limit is 4096.\n",
    "\n",
    "    if gpt_model == \"gpt-4o-mini\":\n",
    "        tokens_cap = int(128000 - 3000) #For gpt-4o-mini, token limit covering both BOTH and output is 128000, while the output limit is 4096.\n",
    "\n",
    "    return tokens_cap\n",
    "\n",
    "def max_output(gpt_model, messages_for_GPT):\n",
    "\n",
    "    if gpt_model == \"gpt-3.5-turbo-0125\":\n",
    "        \n",
    "        max_output_tokens = int(16385 - num_tokens_from_string(str(messages_for_GPT), \"cl100k_base\")) #For GPT-3.5-turbo, token limit covering BOTH input and output is 16385,  while the output limit is 4096.\n",
    "    \n",
    "    if gpt_model == \"gpt-4o\":\n",
    "        \n",
    "        max_output_tokens = int(128000 - num_tokens_from_string(str(messages_for_GPT), \"cl100k_base\")) #For gpt-4o, token limit covering both BOTH and output is 128000, while the output limit is 4096.\n",
    "    \n",
    "    if gpt_model == \"gpt-4o-mini\":\n",
    "        \n",
    "        max_output_tokens = int(128000 - num_tokens_from_string(str(messages_for_GPT), \"cl100k_base\")) #For gpt-4o-mini, token limit covering both BOTH and output is 128000, while the output limit is 4096.\n",
    "\n",
    "    return min(4096, max_output_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "891018f7-b3c1-4af0-a666-b12a487d6896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokens estimate preliminaries\n",
    "\n",
    "#encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "#encoding = tiktoken.encoding_for_model(gpt_model)\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    #Tokens estimate function\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    \n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e331f47e-cc3b-4762-a743-bb82d766cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judgment_prompt_json(judgment_json, gpt_model):\n",
    "\n",
    "    if isinstance(judgment_json[\"judgment\"], list):\n",
    "        judgment_to_string = '\\n'.join(judgment_json[\"judgment\"])\n",
    "        \n",
    "    elif isinstance(judgment_json[\"judgment\"], str):\n",
    "        judgment_to_string = judgment_json[\"judgment\"]\n",
    "        \n",
    "    else:\n",
    "        judgment_to_string = str(judgment_json[\"judgment\"])\n",
    "        \n",
    "    judgment_content = f'Based on the metadata and judgment in the following JSON: \"\"\" {json.dumps(judgment_json)} \"\"\"'\n",
    "\n",
    "    judgment_content_tokens = num_tokens_from_string(judgment_content, \"cl100k_base\")\n",
    "    \n",
    "    if judgment_content_tokens <= tokens_cap(gpt_model):\n",
    "        \n",
    "        return judgment_content\n",
    "\n",
    "    else:\n",
    "        \n",
    "        meta_data_len = judgment_content_tokens - num_tokens_from_string(judgment_to_string, \"cl100k_base\")\n",
    "        \n",
    "        judgment_chars_capped = int((tokens_cap(gpt_model) - meta_data_len)*4)\n",
    "        \n",
    "        judgment_string_trimmed = judgment_to_string[ :int(judgment_chars_capped/2)] + judgment_to_string[-int(judgment_chars_capped/2): ]\n",
    "\n",
    "        judgment_json[\"judgment\"] = judgment_string_trimmed     \n",
    "        \n",
    "        judgment_content_capped = f'Based on the metadata and judgment in the following JSON:  \"\"\" {json.dumps(judgment_json)} \"\"\"'\n",
    "        \n",
    "        return judgment_content_capped\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bae612f3-eabf-4dfe-abbb-2d97adf5558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For modern judgments, define system role content for GPT\n",
    "role_content = 'You are a legal research assistant helping an academic researcher to answer questions about a public judgment. You will be provided with the judgment and metadata in JSON form. Please answer questions based only on information contained in the judgment and metadata. Where your answer comes from specific paragraphs or sections, provide the paragraph numbers or section names as part of your answer. If you cannot answer the questions based on the judgment or metadata, do not make up information, but instead write \"answer not found\". '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b06e2156-74f5-41df-94c0-a539ad91a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT answer function for answers in json form, YES TOKENS\n",
    "#IN USE\n",
    "\n",
    "def GPT_json(questions_json, judgment_json, gpt_model, system_instruction):\n",
    "    #'question_json' variable is a json of questions to GPT\n",
    "    #'jugdment' variable is a judgment_json   \n",
    "\n",
    "    judgment_for_GPT = [{\"role\": \"user\", \"content\": judgment_prompt_json(judgment_json, gpt_model)}]\n",
    "\n",
    "    json_direction = [{\"role\": \"user\", \"content\": 'You will be given questions to answer in JSON form.'}]\n",
    "\n",
    "    #Create answer format\n",
    "    \n",
    "    q_keys = [*questions_json]\n",
    "    \n",
    "    answers_json = {}\n",
    "    \n",
    "    for q_index in q_keys:\n",
    "        answers_json.update({q_index: 'Your answer to the question with index ' + q_index + '. The paragraph or page numbers in the judgment, or sections of the metadata from which you obtained your answer. '})\n",
    "    \n",
    "    #Create questions, which include the answer format\n",
    "    \n",
    "    question_for_GPT = [{\"role\": \"user\", \"content\": json.dumps(questions_json) + ' Give responses in the following JSON form: ' + json.dumps(answers_json)}]\n",
    "    \n",
    "    #Create messages in one prompt for GPT\n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction}]\n",
    "    messages_for_GPT = intro_for_GPT + judgment_for_GPT + json_direction + question_for_GPT\n",
    "    \n",
    "#   return messages_for_GPT\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    try:\n",
    "        #completion = client.chat.completions.create(\n",
    "        completion = openai.chat.completions.create(\n",
    "            model = gpt_model,\n",
    "            messages = messages_for_GPT, \n",
    "            response_format = {\"type\": \"json_object\"}, \n",
    "            max_tokens = max_output(gpt_model, messages_for_GPT), \n",
    "            temperature = 0.2, \n",
    "            top_p = 0.2\n",
    "        )\n",
    "        \n",
    "#        return completion.choices[0].message.content #This gives answers as a string containing a dictionary\n",
    "        \n",
    "        #To obtain a json directly, use below\n",
    "        answers_dict = json.loads(completion.choices[0].message.content)\n",
    "        \n",
    "        #Obtain tokens\n",
    "        output_tokens = completion.usage.completion_tokens\n",
    "        \n",
    "        prompt_tokens = completion.usage.prompt_tokens\n",
    "        \n",
    "        return [answers_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "    except Exception as error:\n",
    "        \n",
    "        for q_index in q_keys:\n",
    "            answers_json[q_index] = error\n",
    "        \n",
    "        return [answers_json, 0, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71837ede-4057-4eae-abd6-b386a0b701d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT function for each respondent's dataframe, index by judgment then question, with input and output tokens given by GPT itself\n",
    "#IN USE\n",
    "\n",
    "#The following function DOES NOT check for existence of questions for GPT\n",
    "    # To so check, active line marked as #*\n",
    "\n",
    "def engage_GPT_json(questions_json, df_individual, GPT_activation, gpt_model, system_instruction):\n",
    "    # Variable questions_json refers to the json of questions\n",
    "    # Variable df_individual refers to each respondent's df\n",
    "    # Variable activation refers to status of GPT activation (real or test)\n",
    "    # The output is a new JSON for the relevant respondent with new columns re:\n",
    "        # f\"Judgment length in tokens (up to {tokens_cap(gpt_model)} given to GPT)\"\n",
    "        # 'GPT cost estimate (USD excl GST)'\n",
    "        # 'GPT time estimate (seconds)'\n",
    "        # GPT questions/answers\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    question_keys = [*questions_json]\n",
    "    \n",
    "    for judgment_index in df_individual.index:\n",
    "        \n",
    "        judgment_json = df_individual.to_dict('index')[judgment_index]\n",
    "        \n",
    "        #Calculate and append number of tokens of judgment, regardless of whether given to GPT\n",
    "        judgment_tokens = num_tokens_from_string(str(judgment_json), \"cl100k_base\")\n",
    "        df_individual.loc[judgment_index, f\"Judgment length in tokens (up to {tokens_cap(gpt_model)} given to GPT)\"] = judgment_tokens       \n",
    "\n",
    "        #Indicate whether judgment truncated\n",
    "        \n",
    "        df_individual.loc[judgment_index, \"Judgment truncated (if given to GPT)?\"] = ''       \n",
    "        \n",
    "        if judgment_tokens <= tokens_cap(gpt_model):\n",
    "            \n",
    "            df_individual.loc[judgment_index, \"Judgment truncated (if given to GPT)?\"] = 'No'\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            df_individual.loc[judgment_index, \"Judgment truncated (if given to GPT)?\"] = 'Yes'\n",
    "\n",
    "        #Create columns for respondent's GPT cost, time\n",
    "        df_individual.loc[judgment_index, 'GPT cost estimate (USD excl GST)'] = ''\n",
    "        df_individual.loc[judgment_index, 'GPT time estimate (seconds)'] = ''\n",
    "                \n",
    "        #Calculate GPT start time\n",
    "\n",
    "        GPT_start_time = datetime.now()\n",
    "\n",
    "        #Depending on activation status, apply GPT_json function to each judgment, gives answers as a string containing a dictionary\n",
    "\n",
    "        if int(GPT_activation) > 0:\n",
    "            GPT_output_list = GPT_json(questions_json, judgment_json, gpt_model, system_instruction) #Gives [answers as a JSON, output tokens, input tokens]\n",
    "            answers_dict = GPT_output_list[0]\n",
    "\n",
    "            #Calculate and append GPT finish time and time difference to individual df\n",
    "            GPT_finish_time = datetime.now()\n",
    "            \n",
    "            GPT_time_difference = GPT_finish_time - GPT_start_time\n",
    "    \n",
    "            df_individual.loc[judgment_index, 'GPT time estimate (seconds)'] = GPT_time_difference.total_seconds()        \n",
    "\n",
    "        else:\n",
    "            answers_dict = {}    \n",
    "            for q_index in question_keys:\n",
    "                #Increases judgment index by 2 to ensure consistency with Excel spreadsheet\n",
    "                answer = 'Placeholder answer for ' + ' judgment ' + str(int(judgment_index) + 2) + ' ' + str(q_index)\n",
    "                answers_dict.update({q_index: answer})\n",
    "            \n",
    "            #Own calculation of GPT costs for Placeholder answer fors\n",
    "\n",
    "            #Calculate capped judgment tokens\n",
    "\n",
    "            judgment_capped_tokens = num_tokens_from_string(judgment_prompt_json(judgment_json, gpt_model), \"cl100k_base\")\n",
    "\n",
    "            #Calculate questions tokens and cost\n",
    "\n",
    "            questions_tokens = num_tokens_from_string(json.dumps(questions_json), \"cl100k_base\")\n",
    "\n",
    "            #Calculate other instructions' tokens\n",
    "\n",
    "            other_instructions = system_instruction + 'you will be given questions to answer in JSON form.' + ' Give responses in the following JSON form: '\n",
    "\n",
    "            other_tokens = num_tokens_from_string(other_instructions, \"cl100k_base\") + len(question_keys)*num_tokens_from_string(\"GPT question x:  Your answer to the question with index GPT question x. The paragraph or page numbers in the judgment, or sections of the metadata from which you obtained your answer. \", \"cl100k_base\")\n",
    "\n",
    "            #Calculate number of tokens of answers\n",
    "            answers_tokens = num_tokens_from_string(str(answers_dict), \"cl100k_base\")\n",
    "\n",
    "            input_tokens = judgment_capped_tokens + questions_tokens + other_tokens\n",
    "            \n",
    "            GPT_output_list = [answers_dict, answers_tokens, input_tokens]\n",
    "\n",
    "    \t#Create GPT question headings, append answers to individual spreadsheets, and remove template/erroneous answers\n",
    "\n",
    "        for question_index in question_keys:\n",
    "            question_heading = question_index + ': ' + questions_json[question_index]\n",
    "            df_individual.loc[judgment_index, question_heading] = answers_dict[question_index]\n",
    "            \n",
    "            if 'Your answer to the question with index' in str(answers_dict[question_index]):\n",
    "                df_individual.loc[judgment_index, question_heading] = 'Error for ' + ' judgment ' + str(int(judgment_index) + 2) + ' ' + str(question_index) + ' Please try again.'\n",
    "\n",
    "        #Calculate GPT costs\n",
    "\n",
    "        GPT_cost = GPT_output_list[1]*gpt_output_cost(gpt_model) + GPT_output_list[2]*gpt_input_cost(gpt_model)\n",
    "\n",
    "        #Calculate and append GPT cost to individual df\n",
    "        df_individual.loc[judgment_index, 'GPT cost estimate (USD excl GST)'] = GPT_cost\n",
    "    \n",
    "    return df_individual\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e8b51-2d15-4be5-9d0a-f37dd3d25dd1",
   "metadata": {},
   "source": [
    "# gpt-4o vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02a9b3f7-84a5-4852-9515-b0a3ee0a7178",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Tokens counter\n",
    "\n",
    "def get_image_dims(image):\n",
    "    if re.match(r\"data:image\\/\\w+;base64\", image):\n",
    "        image = re.sub(r\"data:image\\/\\w+;base64,\", \"\", image)\n",
    "        image = Image.open(BytesIO(base64.b64decode(image)))\n",
    "        return image.size\n",
    "    else:\n",
    "        raise ValueError(\"Image must be a base64 string.\")\n",
    "\n",
    "def calculate_image_token_cost(image, detail=\"auto\"):\n",
    "    # Constants\n",
    "    LOW_DETAIL_COST = 85\n",
    "    HIGH_DETAIL_COST_PER_TILE = 170\n",
    "    ADDITIONAL_COST = 85\n",
    "\n",
    "    if detail == \"auto\":\n",
    "        # assume high detail for now\n",
    "        detail = \"high\"\n",
    "\n",
    "    if detail == \"low\":\n",
    "        # Low detail images have a fixed cost\n",
    "        return LOW_DETAIL_COST\n",
    "    elif detail == \"high\":\n",
    "        # Calculate token cost for high detail images\n",
    "        width, height = get_image_dims(image)\n",
    "        # Check if resizing is needed to fit within a 2048 x 2048 square\n",
    "        if max(width, height) > 2048:\n",
    "            # Resize the image to fit within a 2048 x 2048 square\n",
    "            ratio = 2048 / max(width, height)\n",
    "            width = int(width * ratio)\n",
    "            height = int(height * ratio)\n",
    "        # Further scale down to 768px on the shortest side\n",
    "        if min(width, height) > 768:\n",
    "            ratio = 768 / min(width, height)\n",
    "            width = int(width * ratio)\n",
    "            height = int(height * ratio)\n",
    "        # Calculate the number of 512px squares\n",
    "        num_squares = math.ceil(width / 512) * math.ceil(height / 512)\n",
    "        # Calculate the total token cost\n",
    "        total_cost = num_squares * HIGH_DETAIL_COST_PER_TILE + ADDITIONAL_COST\n",
    "        return total_cost\n",
    "    else:\n",
    "        # Invalid detail_option\n",
    "        raise ValueError(\"Invalid value for detail parameter. Use 'low' or 'high'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
