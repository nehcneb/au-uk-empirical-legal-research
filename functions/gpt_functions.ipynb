{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "519624fc-7c7a-4e54-92f6-4963a6cdd9d3",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da8ba5e8-287e-4b18-8a36-2e7f9896f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminary modules\n",
    "import base64 \n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import pause\n",
    "import os\n",
    "import io\n",
    "from io import BytesIO\n",
    "from dateutil import parser\n",
    "from dateutil.relativedelta import *\n",
    "from datetime import timedelta\n",
    "from PIL import Image\n",
    "import math\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import copy\n",
    "\n",
    "#OpenAI\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "#Streamlit\n",
    "import streamlit as st\n",
    "#from streamlit_gsheets import GSheetsConnection\n",
    "#from streamlit.components.v1 import html\n",
    "import streamlit_ext as ste\n",
    "\n",
    "#PandasAI\n",
    "#from dotenv import load_dotenv\n",
    "from pandasai import SmartDataframe\n",
    "from pandasai import Agent\n",
    "#from pandasai.llm import BambooLLM\n",
    "from pandasai.llm.openai import OpenAI\n",
    "import pandasai as pai\n",
    "from pandasai.responses.streamlit_response import StreamlitResponse\n",
    "from pandasai.helpers.openai_info import get_openai_callback as pandasai_get_openai_callback\n",
    "\n",
    "#Excel\n",
    "import openpyxl\n",
    "from pyxlsb import open_workbook as open_xlsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060e311a-e871-4e97-a59c-909c43fc3820",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_questions_answers, default_judgment_counter_bound\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'functions'"
     ]
    }
   ],
   "source": [
    "from functions.common_functions import check_questions_answers, default_judgment_counter_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df8463a-f127-471c-ae05-af2efde62cf0",
   "metadata": {},
   "source": [
    "# gpt-3.5, 4o-mini and 4o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d2a4e-13ec-46b6-975a-1d951316f84f",
   "metadata": {},
   "source": [
    "## Common functions and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f01e0d-be0c-4c2c-aaa1-94d932bde51a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions for GPT are capped at 2000 characters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Upperbound on the length of questions for GPT\n",
    "\n",
    "question_characters_bound = 2000\n",
    "\n",
    "print(f\"Questions for GPT are capped at {question_characters_bound} characters.\\n\")\n",
    "\n",
    "#Upperbound on number of judgments to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74be555a-f376-4d43-9bb0-0e12ef726522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function to split a string into a list by line\n",
    "def split_by_line(x):\n",
    "    y = x.split('\\n')\n",
    "    for i in y:\n",
    "        if len(i) == 0:\n",
    "            y.remove(i)\n",
    "    return y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b56a336-77a9-40f8-bc0c-af9b1ac77100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function to split a list into a dictionary for list items longer than 10 characters\n",
    "#Apply split_by_line() before the following function\n",
    "def GPT_label_dict(x_list):\n",
    "    GPT_dict = {}\n",
    "    for i in x_list:\n",
    "        if len(i) > 10:\n",
    "            GPT_index = x_list.index(i) + 1\n",
    "            i_label = 'GPT question ' + f'{GPT_index}'\n",
    "            GPT_dict.update({i_label: i})\n",
    "    return GPT_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250b2292-f622-426d-a1ea-66bb29fb7a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check validity of API key\n",
    "\n",
    "@st.cache_data\n",
    "def is_api_key_valid(key_to_check):\n",
    "    openai.api_key = key_to_check\n",
    "    \n",
    "    try:\n",
    "        completion = openai.chat.completions.create(\n",
    "            #model=\"gpt-3.5-turbo-0125\",\n",
    "            model = 'gpt-4o-mini', \n",
    "            messages=[{\"role\": \"user\", \"content\": 'Hi'}], \n",
    "            max_tokens = 1\n",
    "        )\n",
    "    except:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d514f02e-4163-4afa-94da-6f01e996b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define input and output costs, token caps and maximum characters\n",
    "#each token is about 4 characters\n",
    "\n",
    "def gpt_input_cost(gpt_model):\n",
    "    if gpt_model == \"gpt-3.5-turbo-0125\":\n",
    "        gpt_input_cost = 1/1000000*0.5\n",
    "        \n",
    "    if gpt_model == \"gpt-4o-2024-05-13\": #As of 20240910, gpt-4o points towards gpt-4o-2024-05-13\n",
    "        gpt_input_cost = 1/1000000*5\n",
    "\n",
    "    if gpt_model == \"gpt-4o-2024-08-06\": #From 20241002, gpt-4o points towards gpt-4o-2024-08-06\n",
    "        gpt_input_cost = 1/1000000*2.5\n",
    "\n",
    "    if gpt_model == \"gpt-4o\":\n",
    "        gpt_input_cost = 1/1000000*2.5\n",
    "        \n",
    "    if gpt_model == \"gpt-4o-mini\":\n",
    "        gpt_input_cost = 1/1000000*0.15\n",
    "        \n",
    "    return gpt_input_cost\n",
    "\n",
    "def gpt_output_cost(gpt_model):\n",
    "    if gpt_model == \"gpt-3.5-turbo-0125\":\n",
    "        gpt_output_cost = 1/1000000*1.5\n",
    "        \n",
    "    if gpt_model == \"gpt-4o-2024-05-13\": #As of 20240910, gpt-4o points towards gpt-4o-2024-05-13\n",
    "        gpt_output_cost = 1/1000000*15\n",
    "\n",
    "    if gpt_model == \"gpt-4o-2024-08-06\": #From 20241002, gpt-4o points towards gpt-4o-2024-08-06\n",
    "        gpt_output_cost = 1/1000000*10\n",
    "\n",
    "    if gpt_model == \"gpt-4o\":\n",
    "        gpt_output_cost = 1/1000000*10\n",
    "    \n",
    "    if gpt_model == \"gpt-4o-mini\":\n",
    "        gpt_output_cost = 1/1000000*0.6\n",
    "    \n",
    "    return gpt_output_cost\n",
    "    \n",
    "#As of 2024-06-07, questions are capped at about 1000 characters ~ 250 tokens, role_content/system_instruction is about 115 tokens, json_direction is about 11 tokens, answers_json is about 8 tokens plus 30 tokens per question \n",
    "\n",
    "def tokens_cap(gpt_model):\n",
    "    #This is the global cap for each model, which will be shown to users\n",
    "    #Leaving 1000 tokens to spare\n",
    "    \n",
    "    if gpt_model == \"gpt-3.5-turbo-0125\":\n",
    "        \n",
    "        tokens_cap = int(16385 - 3000) #For GPT-3.5-turbo, token limit covering BOTH input and output is 16385,  while the output limit is 4096.\n",
    "    \n",
    "    if gpt_model == \"gpt-4o-2024-05-13\": #As of 20240910, gpt-4o points towards gpt-4o-2024-05-13\n",
    "        tokens_cap = int(128000 - 3000) #For gpt-4o, token limit covering both BOTH and output is 128000, while the output limit is 4096.\n",
    "\n",
    "    if gpt_model == \"gpt-4o-2024-08-06\": #From 20241002, gpt-4o points towards gpt-4o-2024-08-06\n",
    "        \n",
    "        tokens_cap = int(128000 - 3000) #For gpt-4o, token limit covering both BOTH and output is 128000, while the output limit is 16384.\n",
    "\n",
    "    if gpt_model == \"gpt-4o\":\n",
    "        \n",
    "        tokens_cap = int(128000 - 3000) #For gpt-4o, token limit covering both BOTH and output is 128000, while the output limit is 16384.\n",
    "    \n",
    "    if gpt_model == \"gpt-4o-mini\":\n",
    "        tokens_cap = int(128000 - 3000) #For gpt-4o-mini, token limit covering both BOTH and output is 128000, while the output limit is 16384.\n",
    "\n",
    "    return tokens_cap\n",
    "\n",
    "def max_output(gpt_model, messages_for_GPT):\n",
    "\n",
    "    if gpt_model == \"gpt-3.5-turbo-0125\":\n",
    "        \n",
    "        max_output_tokens = int(16385 - num_tokens_from_string(str(messages_for_GPT), \"cl100k_base\")) #For GPT-3.5-turbo, token limit covering BOTH input and output is 16385,  while the output limit is 4096.\n",
    "    \n",
    "    if gpt_model == \"gpt-4o-2024-05-13\": #As of 20240910, gpt-4o points towards gpt-4o-2024-05-13\n",
    "        \n",
    "        max_output_tokens = int(128000 - num_tokens_from_string(str(messages_for_GPT), \"cl100k_base\")) #For gpt-4o, token limit covering both BOTH and output is 128000, while the output limit is 4096.\n",
    "\n",
    "    if gpt_model == \"gpt-4o-2024-08-06\": #From 20241002, gpt-4o points towards gpt-4o-2024-08-06\n",
    "        \n",
    "        max_output_tokens = int(128000 - num_tokens_from_string(str(messages_for_GPT), \"cl100k_base\")) #For gpt-4o, token limit covering both BOTH and output is 128000, while the output limit is 16384.\n",
    "\n",
    "    if gpt_model == \"gpt-4o\":\n",
    "        \n",
    "        max_output_tokens = int(128000 - num_tokens_from_string(str(messages_for_GPT), \"cl100k_base\")) #For gpt-4o, token limit covering both BOTH and output is 128000, while the output limit is 16384.s\n",
    "    \n",
    "    if gpt_model == \"gpt-4o-mini\":\n",
    "        \n",
    "        max_output_tokens = int(128000 - num_tokens_from_string(str(messages_for_GPT), \"cl100k_base\")) #For gpt-4o-mini, token limit covering both BOTH and output is 128000, while the output limit is 16384.\n",
    "\n",
    "    return min(4096, abs(max_output_tokens))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a8315-bb1c-4288-a013-472b3864b72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_msg = f'By default, this app will collect (ie scrape) up to {default_judgment_counter_bound} judgments, and process up to approximately {round(tokens_cap(\"gpt-4o-mini\")*3/4)} words from each judgment.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "891018f7-b3c1-4af0-a666-b12a487d6896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokens estimate preliminaries\n",
    "\n",
    "#encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "#encoding = tiktoken.encoding_for_model(gpt_model)\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    #Tokens estimate function\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    \n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e331f47e-cc3b-4762-a743-bb82d766cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judgment_prompt_json(judgment_json, gpt_model):\n",
    "\n",
    "    #Remove hyperlink\n",
    "    for key in judgment_json.keys():\n",
    "        if 'hyperlink' in key.lower():\n",
    "            judgment_json[key] = ''\n",
    "            break\n",
    "    \n",
    "    #Turn judgment to string\n",
    "    if isinstance(judgment_json[\"judgment\"], list):\n",
    "        judgment_to_string = '\\n'.join(judgment_json[\"judgment\"])\n",
    "        \n",
    "    elif isinstance(judgment_json[\"judgment\"], str):\n",
    "        judgment_to_string = judgment_json[\"judgment\"]\n",
    "        \n",
    "    else:\n",
    "        judgment_to_string = str(judgment_json[\"judgment\"])\n",
    "\n",
    "    #Truncate judgment if needed\n",
    "    judgment_content = f'Based on the metadata and judgment in the following JSON: \"\"\" {json.dumps(judgment_json, default=str)} \"\"\"'\n",
    "\n",
    "    judgment_content_tokens = num_tokens_from_string(judgment_content, \"cl100k_base\")\n",
    "    \n",
    "    if judgment_content_tokens <= tokens_cap(gpt_model):\n",
    "        \n",
    "        return judgment_content\n",
    "\n",
    "    else:\n",
    "        \n",
    "        meta_data_len = judgment_content_tokens - num_tokens_from_string(judgment_to_string, \"cl100k_base\")\n",
    "\n",
    "        intro_len = num_tokens_from_string('Based on the metadata and judgment in the following JSON: \"\"\"  \"\"\"', \"cl100k_base\")\n",
    "        \n",
    "        judgment_chars_capped = int(round((tokens_cap(gpt_model) - meta_data_len - intro_len)*4))\n",
    "        \n",
    "        judgment_string_trimmed = judgment_to_string[ :int(judgment_chars_capped/2)] + judgment_to_string[-int(judgment_chars_capped/2): ]\n",
    "\n",
    "        judgment_json[\"judgment\"] = judgment_string_trimmed     \n",
    "        \n",
    "        judgment_content_capped = f'Based on the metadata and judgment in the following JSON:  \"\"\" {json.dumps(judgment_json, default=str)} \"\"\"'\n",
    "        \n",
    "        return judgment_content_capped\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2252c08-65de-4b69-b132-64d5c72f411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check questions for potential privacy infringement\n",
    "\n",
    "questions_check_system_instruction = \"\"\"\n",
    "You are a compliance officer helping a human ethics committee to ensure that no personally identifiable information will be exposed. \n",
    "You will be given questions to check in JSON form. Please provide labels for these questions based only on information contained in the JSON.\n",
    "Where a question seeks information about a person's birth or address, you label \"1\". If a question does not seek such information, you label \"0\". If you are not sure, label \"unclear\".\n",
    "For example, the question \"What's the plaintiff's date of birth?\" should be labelled \"1\".\n",
    "For example, the question \"What's the defendant's address?\" should be labelled \"1\".\n",
    "For example, the question \"What's the victim's date of death?\" should be labelled \"0\".\n",
    "For example, the question \"What's the judge's name?\" should be labelled \"0\".\n",
    "For example, the question \"What's the defendant's age?\" should be labelled \"0\".\n",
    "\"\"\"\n",
    "\n",
    "#More general below\n",
    "#questions_check_system_instruction = \"\"\"\n",
    "#You are a compliance officer helping a human ethics committee to ensure that no personally identifiable information will be exposed. \n",
    "#You will be given questions to check in JSON form. \n",
    "#Based only on information contained in the JSON, please check each question for whether it seeks a person's birth, address, or other personally identifiable information. \n",
    "#Where a question indeed seeks personally identifiable information, you label \"1\". \n",
    "#Where a question does not seek personally identifiable information, you label \"0\". \n",
    "#If you are not sure, label \"unclear\".\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2735d310-08d5-4a3c-af07-7864eaa2e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check questions for potential privacy infringement\n",
    "\n",
    "@st.cache_data\n",
    "def GPT_questions_check(questions_json, gpt_model, questions_check_system_instruction):\n",
    "    #'question_json' variable is a json of questions to GPT\n",
    "    #'jugdment' variable is a judgment_json   \n",
    "\n",
    "    #judgment_for_GPT = [{\"role\": \"user\", \"content\": judgment_prompt_json(judgment_json, gpt_model)}]\n",
    "\n",
    "    json_direction = [{\"role\": \"user\", \"content\": 'Label the following questions in JSON form.'}]\n",
    "\n",
    "    #Create answer format\n",
    "    \n",
    "    q_keys = [*questions_json]\n",
    "    \n",
    "    labels_json = {}\n",
    "    \n",
    "    for q_index in q_keys:\n",
    "        labels_json.update({q_index: 'Your label for the question with index ' + q_index})\n",
    "    \n",
    "    #Create questions, which include the answer format\n",
    "    \n",
    "    question_to_check = [{\"role\": \"user\", \"content\": json.dumps(questions_json, default = str) + ' Return labels in the following JSON form: ' + json.dumps(labels_json, default = str)}]\n",
    "    \n",
    "    #Create messages in one prompt for GPT\n",
    "    \n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": questions_check_system_instruction}]\n",
    "    #messages_for_GPT = intro_for_GPT + judgment_for_GPT + json_direction + question_to_check\n",
    "    messages_for_GPT = intro_for_GPT + json_direction + question_to_check\n",
    "    \n",
    "#   return messages_for_GPT\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    try:\n",
    "        #completion = client.chat.completions.create(\n",
    "        completion = openai.chat.completions.create(\n",
    "            model = gpt_model,\n",
    "            messages = messages_for_GPT, \n",
    "            response_format = {\"type\": \"json_object\"}, \n",
    "            max_tokens = max_output(gpt_model, messages_for_GPT), \n",
    "            temperature = 0.1, \n",
    "            #top_p = 0.1\n",
    "        )\n",
    "        \n",
    "#        return completion.choices[0].message.content #This gives answers as a string containing a dictionary\n",
    "        \n",
    "        #To obtain a json directly, use below\n",
    "        labels_dict = json.loads(completion.choices[0].message.content)\n",
    "        \n",
    "        #Obtain tokens\n",
    "        output_tokens = completion.usage.completion_tokens\n",
    "        \n",
    "        prompt_tokens = completion.usage.prompt_tokens\n",
    "        \n",
    "        return [labels_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "    except Exception as error:\n",
    "        \n",
    "        for q_index in q_keys:\n",
    "            labels_json[q_index] = error\n",
    "        \n",
    "        return [labels_json, 0, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba47e604-6133-4a1f-8f30-844246ee5355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to replace unchecked questions with checked questions\n",
    "def checked_questions_json(questions_json, gpt_labels_output):\n",
    "    \n",
    "    checked_questions_json = questions_json\n",
    "\n",
    "    for q_key in gpt_labels_output[0]:\n",
    "        \n",
    "        if str(gpt_labels_output[0][q_key]) == '1':\n",
    "            \n",
    "            checked_questions_json[q_key] = 'Say \"n/a\" only.'\n",
    "    \n",
    "    return checked_questions_json\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "768bc5f3-d1ef-4cd8-a353-86a4e6822ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check questions for potential privacy infringement\n",
    "\n",
    "answers_check_system_instruction = \"\"\"\n",
    "You are a compliance officer helping an academic researcher to redact information about birth and address. \n",
    "You will be given text to check in JSON form. Please check the text based only on information contained in the JSON. \n",
    "Where any part of the text identifies birth or an address, you replace that part with \"[redacted]\". \n",
    "You then return the remainder of the text unredacted.\n",
    "You redact birth and address only. Do not redact anything else, such as names, date of death, age.\n",
    "For example, if the text given to you is \"John Smith, born 1 January 1950, died on 20 December 2008 at 1 Main St Blackacre aged 58.\", you return \"John Smith, born [redacted], died on 20 December 2008 at [redacted] aged 58.\".\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeaf1a7f-9229-4843-b341-1f29fab94b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check answers_to_check_json for potential privacy infringement\n",
    "\n",
    "@st.cache_data\n",
    "def GPT_answers_check(answers_to_check_json, gpt_model, answers_check_system_instruction):\n",
    "    #'question_json' variable is a json of answers_to_check_json to GPT\n",
    "    #'jugdment' variable is a judgment_json   \n",
    "\n",
    "    #judgment_for_GPT = [{\"role\": \"user\", \"content\": judgment_prompt_json(judgment_json, gpt_model)}]\n",
    "\n",
    "    json_direction = [{\"role\": \"user\", \"content\": 'Check the following text in JSON form.'}]\n",
    "\n",
    "    #Create answer format\n",
    "\n",
    "    answers_to_check_list = [answers_to_check_json]\n",
    "\n",
    "    redacted_answers_json = {}\n",
    "    \n",
    "    if isinstance(answers_to_check_json, list):\n",
    "        answers_to_check_list = answers_to_check_json\n",
    "\n",
    "    for answers_to_check_json in answers_to_check_list:\n",
    "        \n",
    "        q_keys = [*answers_to_check_json]\n",
    "        \n",
    "        for q_index in q_keys:\n",
    "            \n",
    "            #redacted_answers_json.update({q_index: 'Your answer for the question with index ' + q_index})\n",
    "            redacted_answers_json.update({q_index: 'Your response.'})\n",
    "\n",
    "    #Create answers_to_check_json, which include the answer format\n",
    "    \n",
    "    question_to_check = [{\"role\": \"user\", \"content\": json.dumps(answers_to_check_json, default = str) + ' Respond in the following JSON form: ' + json.dumps(redacted_answers_json, default = str)}]\n",
    "    \n",
    "    #Create messages in one prompt for GPT\n",
    "    \n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": answers_check_system_instruction}]\n",
    "    #messages_for_GPT = intro_for_GPT + judgment_for_GPT + json_direction + question_to_check\n",
    "    messages_for_GPT = intro_for_GPT + json_direction + question_to_check\n",
    "    \n",
    "#   return messages_for_GPT\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    try:\n",
    "        #completion = client.chat.completions.create(\n",
    "        completion = openai.chat.completions.create(\n",
    "            model = gpt_model,\n",
    "            messages = messages_for_GPT, \n",
    "            response_format = {\"type\": \"json_object\"}, \n",
    "            max_tokens = max_output(gpt_model, messages_for_GPT), \n",
    "            temperature = 0.1, \n",
    "            #top_p = 0.1\n",
    "        )\n",
    "        \n",
    "#        return completion.choices[0].message.content #This gives answers as a string containing a dictionary\n",
    "        \n",
    "        #To obtain a json directly, use below\n",
    "        answers_dict = json.loads(completion.choices[0].message.content)\n",
    "        \n",
    "        #Obtain tokens\n",
    "        output_tokens = completion.usage.completion_tokens\n",
    "        \n",
    "        prompt_tokens = completion.usage.prompt_tokens\n",
    "        \n",
    "        return [answers_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "    except Exception as error:\n",
    "        \n",
    "        for q_index in q_keys:\n",
    "            redacted_answers_json[q_index] = error\n",
    "        \n",
    "        return [redacted_answers_json, 0, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bae612f3-eabf-4dfe-abbb-2d97adf5558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For modern judgments, define system role content for GPT\n",
    "role_content = \"You are a legal research assistant helping an academic researcher to answer questions about a public judgment. You will be provided with the judgment and metadata in JSON form. Please answer questions based only on information contained in the judgment and metadata. Where your answer comes from specific paragraphs, pages or sections of the judgment or metadata, include a reference to those paragraphs, pages or sections. If you cannot answer the questions based on the judgment or metadata, do not make up information, but instead write 'answer not found'. \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560df91d-1cce-4fb8-b39f-b7ec11b77c0f",
   "metadata": {},
   "source": [
    "## GPT instant response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b06e2156-74f5-41df-94c0-a539ad91a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT answer function for answers in json form, YES TOKENS\n",
    "#IN USE\n",
    "\n",
    "@st.cache_data\n",
    "def GPT_json(questions_json, judgment_json, gpt_model, system_instruction):\n",
    "    #'question_json' variable is a json of questions to GPT\n",
    "    #'jugdment' variable is a judgment_json   \n",
    "\n",
    "    judgment_for_GPT = [{\"role\": \"user\", \"content\": judgment_prompt_json(judgment_json, gpt_model)}]\n",
    "\n",
    "    json_direction = [{\"role\": \"user\", \"content\": 'You will be given questions to answer in JSON form.'}]\n",
    "\n",
    "    #Create answer format\n",
    "    \n",
    "    q_keys = [*questions_json]\n",
    "    \n",
    "    answers_json = {}\n",
    "    \n",
    "    for q_index in q_keys:\n",
    "        #answers_json.update({questions_json[q_index]: f'Your answer to this question. (The paragraphs, pages or sections from which you obtained your answer)'})\n",
    "        answers_json.update({questions_json[q_index]: f'Your answer. (The paragraphs, pages or sections from which you obtained your answer)'})\n",
    "\n",
    "    #Create questions, which include the answer format\n",
    "    \n",
    "    question_for_GPT = [{\"role\": \"user\", \"content\": json.dumps(questions_json, default = str) + ' Give responses in the following JSON form: ' + json.dumps(answers_json, default = str)}]\n",
    "    \n",
    "    #Create messages in one prompt for GPT\n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction}]\n",
    "    messages_for_GPT = intro_for_GPT + judgment_for_GPT + json_direction + question_for_GPT\n",
    "    \n",
    "#   return messages_for_GPT\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    try:\n",
    "        #completion = client.chat.completions.create(\n",
    "        completion = openai.chat.completions.create(\n",
    "            model = gpt_model,\n",
    "            messages = messages_for_GPT, \n",
    "            response_format = {\"type\": \"json_object\"}, \n",
    "            max_tokens = max_output(gpt_model, messages_for_GPT), \n",
    "            temperature = 0.1, \n",
    "            #top_p = 0.1\n",
    "        )\n",
    "        \n",
    "#        return completion.choices[0].message.content #This gives answers as a string containing a dictionary\n",
    "        \n",
    "        #To obtain a json directly, use below\n",
    "        answers_dict = json.loads(completion.choices[0].message.content)\n",
    "        \n",
    "        #Obtain tokens\n",
    "        output_tokens = completion.usage.completion_tokens\n",
    "        \n",
    "        prompt_tokens = completion.usage.prompt_tokens\n",
    "        \n",
    "        #return [answers_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "        #Check answers\n",
    "\n",
    "        if check_questions_answers() > 0:\n",
    "            \n",
    "            try:\n",
    "                redacted_output = GPT_answers_check(answers_dict, gpt_model, answers_check_system_instruction)\n",
    "        \n",
    "                redacted_answers_dict = redacted_output[0]\n",
    "        \n",
    "                redacted_answers_output_tokens = redacted_output[1]\n",
    "        \n",
    "                redacted_answers_prompt_tokens = redacted_output[2]\n",
    "        \n",
    "                return [redacted_answers_dict, output_tokens + redacted_answers_output_tokens, prompt_tokens + redacted_answers_prompt_tokens]\n",
    "\n",
    "                print('Answers checked.')\n",
    "                \n",
    "            except Exception as e:\n",
    "    \n",
    "                print('Answers check failed.')\n",
    "    \n",
    "                print(e)\n",
    "    \n",
    "                return [answers_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "        else:\n",
    "\n",
    "            print('Answers not checked.')\n",
    "            \n",
    "            return [answers_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "    except Exception as error:\n",
    "        \n",
    "        print('GPT failed to produce answers.')\n",
    "        \n",
    "        for q_index in q_keys:\n",
    "            \n",
    "            answers_json[q_index] = error\n",
    "        \n",
    "        return [answers_json, 0, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71837ede-4057-4eae-abd6-b386a0b701d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT function for each respondent's dataframe, index by judgment then question, with input and output tokens given by GPT itself\n",
    "#IN USE\n",
    "\n",
    "#The following function DOES NOT check for existence of questions for GPT\n",
    "    # To so check, active line marked as #*\n",
    "\n",
    "@st.cache_data\n",
    "def engage_GPT_json(questions_json, df_individual, GPT_activation, gpt_model, system_instruction):\n",
    "    # Variable questions_json refers to the json of questions\n",
    "    # Variable df_individual refers to each respondent's df\n",
    "    # Variable activation refers to status of GPT activation (real or test)\n",
    "    # The output is a new JSON for the relevant respondent with new columns re:\n",
    "        # f\"Judgment length in tokens (up to {tokens_cap(gpt_model)} given to GPT)\"\n",
    "        # 'GPT cost estimate (USD excl GST)'\n",
    "        # 'GPT time estimate (seconds)'\n",
    "        # GPT questions/answers\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "\n",
    "    #Make a copy of questions for making headings later\n",
    "    unchecked_questions_json = questions_json.copy()\n",
    "\n",
    "    #Check questions for privacy violation\n",
    "\n",
    "    if check_questions_answers() > 0:\n",
    "    \n",
    "        try:\n",
    "    \n",
    "            labels_output = GPT_questions_check(questions_json, gpt_model, questions_check_system_instruction)\n",
    "    \n",
    "            labels_output_tokens = labels_output[1]\n",
    "    \n",
    "            labels_prompt_tokens = labels_output[2]\n",
    "        \n",
    "            questions_json = checked_questions_json(questions_json, labels_output)\n",
    "\n",
    "            print('Questions checked.')\n",
    "    \n",
    "        except Exception as e:\n",
    "            \n",
    "            print('Questions check failed.')\n",
    "            \n",
    "            print(e)\n",
    "    \n",
    "            labels_output_tokens = 0\n",
    "            \n",
    "            labels_prompt_tokens = 0\n",
    "\n",
    "    else:\n",
    "\n",
    "        print('Questions not checked.')\n",
    "        \n",
    "        labels_output_tokens = 0\n",
    "        \n",
    "        labels_prompt_tokens = 0\n",
    "\n",
    "    #Process questions\n",
    "    \n",
    "    for judgment_index in df_individual.index:\n",
    "        \n",
    "        judgment_json = df_individual.to_dict('index')[judgment_index]\n",
    "        \n",
    "        #Calculate and append number of tokens of judgment, regardless of whether given to GPT\n",
    "        judgment_tokens = num_tokens_from_string(str(judgment_json), \"cl100k_base\")\n",
    "        df_individual.loc[judgment_index, f\"Judgment length in tokens (up to {tokens_cap(gpt_model)} given to GPT)\"] = judgment_tokens       \n",
    "\n",
    "        #Indicate whether judgment truncated\n",
    "        \n",
    "        df_individual.loc[judgment_index, \"Judgment truncated (if given to GPT)?\"] = ''       \n",
    "        \n",
    "        if judgment_tokens <= tokens_cap(gpt_model):\n",
    "            \n",
    "            df_individual.loc[judgment_index, \"Judgment truncated (if given to GPT)?\"] = 'No'\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            df_individual.loc[judgment_index, \"Judgment truncated (if given to GPT)?\"] = 'Yes'\n",
    "\n",
    "        #Create columns for respondent's GPT cost, time\n",
    "        df_individual.loc[judgment_index, 'GPT cost estimate (USD excl GST)'] = ''\n",
    "        df_individual.loc[judgment_index, 'GPT time estimate (seconds)'] = ''\n",
    "                \n",
    "        #Calculate GPT start time\n",
    "\n",
    "        GPT_start_time = datetime.now()\n",
    "\n",
    "        #Depending on activation status, apply GPT_json function to each judgment, gives answers as a string containing a dictionary\n",
    "\n",
    "        if int(GPT_activation) > 0:\n",
    "            GPT_output_list = GPT_json(questions_json, judgment_json, gpt_model, system_instruction) #Gives [answers as a JSON, output tokens, input tokens]\n",
    "            answers_dict = GPT_output_list[0]\n",
    "\n",
    "            #Calculate and append GPT finish time and time difference to individual df\n",
    "            GPT_finish_time = datetime.now()\n",
    "            \n",
    "            GPT_time_difference = GPT_finish_time - GPT_start_time\n",
    "    \n",
    "            df_individual.loc[judgment_index, 'GPT time estimate (seconds)'] = GPT_time_difference.total_seconds()        \n",
    "\n",
    "        else:\n",
    "            answers_dict = {}\n",
    "            \n",
    "            question_keys = [*questions_json]\n",
    "            \n",
    "            for q_index in question_keys:\n",
    "                #Increases judgment index by 2 to ensure consistency with Excel spreadsheet\n",
    "                answer = 'Placeholder answer for ' + ' judgment ' + str(int(judgment_index) + 2) + ' ' + str(q_index)\n",
    "                answers_dict.update({q_index: answer})\n",
    "            \n",
    "            #Own calculation of GPT costs for Placeholder answer fors\n",
    "\n",
    "            #Calculate capped judgment tokens\n",
    "\n",
    "            judgment_capped_tokens = num_tokens_from_string(judgment_prompt_json(judgment_json, gpt_model), \"cl100k_base\")\n",
    "\n",
    "            #Calculate questions tokens and cost\n",
    "\n",
    "            questions_tokens = num_tokens_from_string(json.dumps(questions_json, default = str), \"cl100k_base\")\n",
    "\n",
    "            #Calculate other instructions' tokens\n",
    "\n",
    "            other_instructions = system_instruction + 'you will be given questions to answer in JSON form.' + ' Give responses in the following JSON form: '\n",
    "\n",
    "            other_tokens = num_tokens_from_string(other_instructions, \"cl100k_base\") + len(question_keys)*num_tokens_from_string(\"GPT question x:  Your answer to the question with index GPT question x. The paragraph or page numbers in the judgment, or sections of the metadata from which you obtained your answer. \", \"cl100k_base\")\n",
    "\n",
    "            #Calculate number of tokens of answers\n",
    "            answers_tokens = num_tokens_from_string(str(answers_dict), \"cl100k_base\")\n",
    "\n",
    "            input_tokens = judgment_capped_tokens + questions_tokens + other_tokens\n",
    "            \n",
    "            GPT_output_list = [answers_dict, answers_tokens, input_tokens]\n",
    "\n",
    "    \t#Create GPT question headings, append answers to individual spreadsheets, and remove template answers\n",
    "\n",
    "        #answers_list = [answers_dict]\n",
    "\n",
    "        #if isinstance(answers_dict, list):\n",
    "            #answers_list = answers_dict\n",
    "        \n",
    "        #for answers_dict in answers_list:\n",
    "\n",
    "        q_counter = 1\n",
    "        \n",
    "        for answer_index in answers_dict.keys():\n",
    "\n",
    "            #Check any question override\n",
    "            if 'Say \"n/a\" only' in str(answer_index):\n",
    "                answer_header = f'GPT question {q_counter}: ' + 'Not answered due to potential privacy violation'\n",
    "            else:\n",
    "                answer_header = f'GPT question {q_counter}: ' + answer_index\n",
    "\n",
    "            #Check any errors\n",
    "            answer_string = str(answers_dict[answer_index]).lower()\n",
    "            \n",
    "            if ((answer_string.startswith('your answer.')) or (answer_string.startswith('your response.'))):\n",
    "                \n",
    "                answers_dict[answer_index] = 'Error. Please try a different question or GPT model.'\n",
    "\n",
    "            #Append answer to spreadsheet\n",
    "            try:\n",
    "            \n",
    "                df_individual.loc[judgment_index, answer_header] = answers_dict[answer_index]\n",
    "\n",
    "            except:\n",
    "\n",
    "                df_individual.loc[judgment_index, answer_header] = str(answers_dict[answer_index])\n",
    "\n",
    "            q_counter += 1\n",
    "            \n",
    "        #Calculate GPT costs\n",
    "\n",
    "        #If no check for questions\n",
    "        #GPT_cost = GPT_output_list[1]*gpt_output_cost(gpt_model) + GPT_output_list[2]*gpt_input_cost(gpt_model)\n",
    "\n",
    "        #If check for questions\n",
    "        GPT_cost = (GPT_output_list[1] + labels_output_tokens/len(df_individual))*gpt_output_cost(gpt_model) + (GPT_output_list[2] + labels_prompt_tokens/len(df_individual))*gpt_input_cost(gpt_model)\n",
    "\n",
    "        #Calculate and append GPT cost to individual df\n",
    "        df_individual.loc[judgment_index, 'GPT cost estimate (USD excl GST)'] = GPT_cost\n",
    "    \n",
    "    return df_individual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf9623-73d7-41a4-9ac1-54a8256696bf",
   "metadata": {},
   "source": [
    "## Batch mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d62fa1-6e97-4cc4-9eac-ad9609c8b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If own account\n",
    "\n",
    "#Cutoff for requiring activate batch mode\n",
    "\n",
    "judgment_batch_cutoff = 25\n",
    "\n",
    "#max number of judgments under any mode\n",
    "judgment_batch_max = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c8f34e5-ec14-4b6e-81c2-625c76779d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create custom id for one judgment_json file\n",
    "\n",
    "#custom_id should be mnc plus time now\n",
    "\n",
    "def gpt_get_custom_id(judgment_json):\n",
    "    \n",
    "    #Returns time now by default\n",
    "    time_now = str(datetime.now()).replace(' ', '_').replace(':', '_').replace('.', '_')\n",
    "\n",
    "    mnc = ''\n",
    "\n",
    "    if 'Medium neutral citation' in judgment_json.keys():\n",
    "        mnc = judgment_json['Medium neutral citation'].replace(' ', '_')\n",
    "    \n",
    "    elif 'mnc' in judgment_json.keys():\n",
    "\n",
    "        mnc = judgment_json['mnc'].replace(' ', '_')\n",
    "\n",
    "    else:\n",
    "        mnc = 'unknown_mnc'\n",
    "\n",
    "    case_name = ''\n",
    "\n",
    "    if 'Case name' in judgment_json.keys():\n",
    "        case_name = judgment_json['Case name'].replace(' ', '_')\n",
    "    \n",
    "    elif 'title' in judgment_json.keys():\n",
    "\n",
    "        case_name = judgment_json['title'].replace(' ', '_')\n",
    "\n",
    "    else:\n",
    "        case_name = 'unknown_case_name'\n",
    "    \n",
    "    custom_id = f\"{time_now}_{case_name}_{mnc}\"\n",
    "\n",
    "    return custom_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55f1ffa3-f79b-49ec-9704-c55b95e357ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function for creating custom id and one line of jsonl file for batching\n",
    "#Returns a dictionary of custom id and one line\n",
    "\n",
    "def gpt_batch_input_id_line(questions_json, judgment_json, gpt_model, system_instruction):\n",
    "    #'question_json' variable is a json of questions to GPT\n",
    "    #'jugdment' variable is a judgment_json   \n",
    "\n",
    "    judgment_for_GPT = [{\"role\": \"user\", \"content\": judgment_prompt_json(judgment_json, gpt_model)}]\n",
    "\n",
    "    json_direction = [{\"role\": \"user\", \"content\": 'You will be given questions to answer in JSON form.'}]\n",
    "\n",
    "    #Create answer format\n",
    "    \n",
    "    q_keys = [*questions_json]\n",
    "    \n",
    "    answers_json = {}\n",
    "    \n",
    "    for q_index in q_keys:\n",
    "        answers_json.update({questions_json[q_index]: f'Your answer. (The paragraphs, pages or sections from which you obtained your answer)'})\n",
    "\n",
    "    #Create questions, which include the answer format\n",
    "    \n",
    "    question_for_GPT = [{\"role\": \"user\", \"content\": json.dumps(questions_json, default = str) + ' Give responses in the following JSON form: ' + json.dumps(answers_json, default = str)}]\n",
    "    \n",
    "    #Create messages in one prompt for GPT\n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction}]\n",
    "    messages_for_GPT = intro_for_GPT + judgment_for_GPT + json_direction + question_for_GPT\n",
    "\n",
    "    #Create one line in batch input\n",
    "    #Format for one line in batch input file is\n",
    "    #{\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-3.5-turbo-0125\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},{\"role\": \"user\", \"content\": \"Hello world!\"}],\"max_tokens\": 1000}}\n",
    "\n",
    "    body = {\"model\": gpt_model, \n",
    "            \"messages\": messages_for_GPT, \n",
    "            \"response_format\": {\"type\": \"json_object\"}, \n",
    "            \"max_tokens\": max_output(gpt_model, messages_for_GPT), \n",
    "            \"temperature\": 0.1, \n",
    "            #\"top_p\" = 0.1\n",
    "           }\n",
    "\n",
    "    custom_id = gpt_get_custom_id(judgment_json)\n",
    "    \n",
    "    oneline = {\"custom_id\": custom_id, \n",
    "              \"method\": \"POST\", \n",
    "              \"url\": \"/v1/chat/completions\", \n",
    "              \"body\": body\n",
    "             }\n",
    "\n",
    "    return {\"custom_id\": custom_id, \"oneline\": oneline}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13d194e1-4edf-4de4-9a32-7f521de5a3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 08:49:47.095 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "#Define function for creating jsonl file for batching together with df_individual with custom id inserted\n",
    "\n",
    "@st.cache_data\n",
    "def gpt_batch_input(questions_json, df_individual, GPT_activation, gpt_model, system_instruction):\n",
    "    # Variable questions_json refers to the json of questions\n",
    "    # Variable df_individual refers to each respondent's df\n",
    "    # Variable activation refers to status of GPT activation (real or test)\n",
    "    # The output is a new JSON for the relevant respondent with new columns re:\n",
    "        # f\"Judgment length in tokens (up to {tokens_cap(gpt_model)} given to GPT)\"\n",
    "        # 'GPT cost estimate (USD excl GST)'\n",
    "        # 'GPT time estimate (seconds)'\n",
    "        # GPT questions/answers\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "\n",
    "    #Make a copy of questions for making headings later\n",
    "    unchecked_questions_json = questions_json.copy()\n",
    "\n",
    "    #Check questions for privacy violation\n",
    "\n",
    "    if check_questions_answers() > 0:\n",
    "    \n",
    "        try:\n",
    "    \n",
    "            labels_output = GPT_questions_check(questions_json, gpt_model, questions_check_system_instruction)\n",
    "    \n",
    "            labels_output_tokens = labels_output[1]\n",
    "    \n",
    "            labels_prompt_tokens = labels_output[2]\n",
    "        \n",
    "            questions_json = checked_questions_json(questions_json, labels_output)\n",
    "\n",
    "            print('Questions checked.')\n",
    "    \n",
    "        except Exception as e:\n",
    "            \n",
    "            print('Questions check failed.')\n",
    "            \n",
    "            print(e)\n",
    "    \n",
    "            labels_output_tokens = 0\n",
    "            \n",
    "            labels_prompt_tokens = 0\n",
    "\n",
    "    else:\n",
    "\n",
    "        print('Questions not checked.')\n",
    "        \n",
    "        labels_output_tokens = 0\n",
    "        \n",
    "        labels_prompt_tokens = 0\n",
    "\n",
    "    #Create list for conversion to jsonl\n",
    "\n",
    "    batch_input_list = []\n",
    "    \n",
    "    #Process questions\n",
    "    \n",
    "    for judgment_index in df_individual.index:\n",
    "        \n",
    "        judgment_json = df_individual.to_dict('index')[judgment_index]\n",
    "        \n",
    "        #Calculate and append number of tokens of judgment, regardless of whether given to GPT\n",
    "        judgment_tokens = num_tokens_from_string(str(judgment_json), \"cl100k_base\")\n",
    "        df_individual.loc[judgment_index, f\"Judgment length in tokens (up to {tokens_cap(gpt_model)} given to GPT)\"] = judgment_tokens       \n",
    "\n",
    "        #Indicate whether judgment truncated\n",
    "        \n",
    "        df_individual.loc[judgment_index, \"Judgment truncated (if given to GPT)?\"] = ''       \n",
    "        \n",
    "        if judgment_tokens <= tokens_cap(gpt_model):\n",
    "            \n",
    "            df_individual.loc[judgment_index, \"Judgment truncated (if given to GPT)?\"] = 'No'\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            df_individual.loc[judgment_index, \"Judgment truncated (if given to GPT)?\"] = 'Yes'\n",
    "\n",
    "        #Create columns for respondent's GPT cost\n",
    "        df_individual.loc[judgment_index, 'GPT cost estimate (USD excl GST)'] = ''\n",
    "        #df_individual.loc[judgment_index, 'GPT time estimate (seconds)'] = ''\n",
    "                \n",
    "        #Calculate GPT start time\n",
    "\n",
    "        GPT_start_time = datetime.now()\n",
    "\n",
    "        #Depending on activation status, apply GPT_json function to each judgment, gives answers as a string containing a dictionary\n",
    "\n",
    "        if int(GPT_activation) > 0:\n",
    "\n",
    "            get_id_oneline = gpt_batch_input_id_line(questions_json, judgment_json, gpt_model, system_instruction)\n",
    "            \n",
    "            df_individual.loc[judgment_index, 'custom_id'] = get_id_oneline['custom_id']\n",
    "\n",
    "            batch_input_list.append(get_id_oneline['oneline'])\n",
    "\n",
    "            if 'judgment' in df_individual.columns:\n",
    "                \n",
    "                df_individual.loc[judgment_index, 'judgment'] = ''\n",
    "            \n",
    "            df_individual.loc[judgment_index, 'GPT submission time'] = str(GPT_start_time)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            print(f'Case {judgment_index}: GPT not activated.')\n",
    "\n",
    "    #Convert batch_input_list to jsonl\n",
    "    #The following steps are based on\n",
    "    #https://stackoverflow.com/questions/51775175/pandas-dataframe-to-jsonl-json-lines-conversion\n",
    "    #https://github.com/openai/openai-python/tree/main#file-uploads\n",
    "        #Replace 'client.' with 'openai.'\n",
    "        #Need to convert jsonl_for_batching to bytes mode, see https://www.datacamp.com/tutorial/string-to-bytes-conversion\n",
    "\n",
    "    df_jsonl = pd.DataFrame(batch_input_list)\n",
    "\n",
    "    jsonl_for_batching = df_jsonl.to_json(orient='records', lines=True)\n",
    "    \n",
    "    batch_input_file = openai.files.create(\n",
    "        file = jsonl_for_batching.encode(encoding=\"utf-8\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    batch_input_file_id = batch_input_file.id\n",
    "    \n",
    "    batch_record = openai.batches.create(\n",
    "        input_file_id=batch_input_file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\", \n",
    "            #metadata={\n",
    "      #\"name\":\n",
    "        #\"email\":\n",
    "    #}\n",
    "    )\n",
    "    \n",
    "    return {'batch_record': batch_record, 'df_individual': df_individual}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e8b51-2d15-4be5-9d0a-f37dd3d25dd1",
   "metadata": {},
   "source": [
    "## Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02a9b3f7-84a5-4852-9515-b0a3ee0a7178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokens counter\n",
    "\n",
    "def get_image_dims(image):\n",
    "    if re.match(r\"data:image\\/\\w+;base64\", image):\n",
    "        image = re.sub(r\"data:image\\/\\w+;base64,\", \"\", image)\n",
    "        image = Image.open(BytesIO(base64.b64decode(image)))\n",
    "        return image.size\n",
    "    else:\n",
    "        raise ValueError(\"Image must be a base64 string.\")\n",
    "\n",
    "def calculate_image_token_cost(image, detail=\"auto\"):\n",
    "    # Constants\n",
    "    LOW_DETAIL_COST = 85\n",
    "    HIGH_DETAIL_COST_PER_TILE = 170\n",
    "    ADDITIONAL_COST = 85\n",
    "\n",
    "    if detail == \"auto\":\n",
    "        # assume high detail for now\n",
    "        detail = \"high\"\n",
    "\n",
    "    if detail == \"low\":\n",
    "        # Low detail images have a fixed cost\n",
    "        return LOW_DETAIL_COST\n",
    "    elif detail == \"high\":\n",
    "        # Calculate token cost for high detail images\n",
    "        width, height = get_image_dims(image)\n",
    "        # Check if resizing is needed to fit within a 2048 x 2048 square\n",
    "        if max(width, height) > 2048:\n",
    "            # Resize the image to fit within a 2048 x 2048 square\n",
    "            ratio = 2048 / max(width, height)\n",
    "            width = int(width * ratio)\n",
    "            height = int(height * ratio)\n",
    "        # Further scale down to 768px on the shortest side\n",
    "        if min(width, height) > 768:\n",
    "            ratio = 768 / min(width, height)\n",
    "            width = int(width * ratio)\n",
    "            height = int(height * ratio)\n",
    "        # Calculate the number of 512px squares\n",
    "        num_squares = math.ceil(width / 512) * math.ceil(height / 512)\n",
    "        # Calculate the total token cost\n",
    "        total_cost = num_squares * HIGH_DETAIL_COST_PER_TILE + ADDITIONAL_COST\n",
    "        return total_cost\n",
    "    else:\n",
    "        # Invalid detail_option\n",
    "        raise ValueError(\"Invalid value for detail parameter. Use 'low' or 'high'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52570fba-249a-4255-8012-cfc4a5121826",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521ebf83-c7bc-4cc2-8956-f30111aedfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jurisdiction specific instruction and functions\n",
    "\n",
    "def gpt_run(jurisdiction_page, df_master):\n",
    "\n",
    "    if jurisdiction_page == 'pages/HCA.py':\n",
    "        \n",
    "        system_instruction = role_content\n",
    "        \n",
    "        from functions.hca_functions import hca_run, hca_collections, hca_search, hca_search_results_to_judgment_links, hca_pdf_judgment, hca_meta_labels_droppable, hca_meta_judgment_dict, hca_meta_judgment_dict_alt, hca_mnc_to_link_browse, hca_citation_to_link, hca_mnc_to_link, hca_load_data, hca_data_url, hca_df, hca_judgment_to_exclude, hca_search_results_to_judgment_links_filtered_df\n",
    "    \n",
    "        run = copy.copy(hca_run)\n",
    "\n",
    "    if jurisdiction_page == 'pages/NSW.py':\n",
    "        \n",
    "        system_instruction = role_content\n",
    "\n",
    "        from nswcaselaw.search import Search\n",
    "        \n",
    "        from functions.nsw_functions import nsw_run, nsw_meta_labels_droppable, nsw_courts, nsw_courts_positioning, nsw_default_courts, nsw_tribunals, nsw_tribunals_positioning, nsw_court_choice, nsw_tribunal_choice, nsw_date, nsw_link, nsw_short_judgment, nsw_tidying_up, nsw_tidying_up_prebatch, nsw_search_url\n",
    "    \n",
    "        run = copy.copy(nsw_run)\n",
    "    \n",
    "    if jurisdiction_page == 'pages/FCA.py':\n",
    "        \n",
    "        system_instruction = role_content\n",
    "        \n",
    "        from functions.fca_functions import fca_run, fca_courts, fca_courts_list, fca_search, fca_search_url, fca_search_results_to_judgment_links, fca_link_to_doc, fca_metalabels, fca_metalabels_droppable, fca_meta_judgment_dict, fca_pdf_name_mnc_list, fca_pdf_name\n",
    "    \n",
    "        run = copy.copy(fca_run)\n",
    "\n",
    "    if jurisdiction_page == 'pages/CA.py':\n",
    "        \n",
    "        system_instruction = role_content\n",
    "        \n",
    "        from functions.ca_functions import ca_run, all_ca_jurisdictions, ca_courts, bc_courts, ab_courts, sk_courts, mb_courts, on_courts, qc_courts, nb_courts, ns_courts, pe_courts, nl_courts, yk_courts, nt_courts, nu_courts, all_ca_jurisdiction_court_pairs, ca_court_tribunal_types, all_subjects, ca_search, ca_search_url, ca_search_results_to_judgment_links, ca_meta_labels_droppable, ca_meta_dict, ca_date, ca_meta_judgment_dict\n",
    "        \n",
    "        run = copy.copy(ca_run)\n",
    "\n",
    "    if jurisdiction_page == 'pages/UK.py':\n",
    "        \n",
    "        system_instruction = role_content\n",
    "        \n",
    "        from functions.uk_functions import uk_run, uk_courts_default_list, uk_courts, uk_courts_list, uk_court_choice, uk_link, uk_search, uk_search_results_to_judgment_links, uk_meta_labels_droppable, uk_meta_judgment_dict\n",
    "        \n",
    "        run = copy.copy(uk_run)\n",
    "\n",
    "    if jurisdiction_page == 'pages/AFCA.py':\n",
    "\n",
    "        system_instruction = role_content\n",
    "                \n",
    "        from functions.afca_functions import afca_run, afca_old_run, afca_new_run, product_line_options, product_category_options, product_name_options, issue_type_options, issue_options, afca_search, afca_meta_judgment_dict,  afca_meta_labels_droppable, afca_old_pdf_judgment, afca_old_element_meta, afca_old_search, afca_old_meta_labels_droppable, afca_meta_labels_droppable, streamlit_timezone\n",
    "                \n",
    "        if streamlit_timezone() == True:\n",
    "\n",
    "            st.warning('One or more Chrome window may be launched. It must be kept open.')\n",
    "\n",
    "        run = copy.copy(afca_run)\n",
    "\n",
    "    if jurisdiction_page == 'pages/ER.py':\n",
    "\n",
    "        from functions.er_functions import er_run, er_run_b64, er_methods_list, er_method_types, er_search, er_search_results_to_case_link_pairs, er_judgment_text, er_meta_judgment_dict, role_content_er, er_judgment_tokens_b64, er_meta_judgment_dict_b64, er_GPT_b64_json, er_engage_GPT_b64_json\n",
    "\n",
    "        #from gpt_functions import get_image_dims, calculate_image_token_cost\n",
    "\n",
    "        system_instruction = role_content_er\n",
    "\n",
    "        run = copy.copy(er_run)\n",
    "\n",
    "    if jurisdiction_page == 'pages/KR.py':\n",
    "\n",
    "        system_instruction = role_content\n",
    "                \n",
    "        from functions.kr_functions import kr_run, kr_methods_list, kr_method_types, kr_search, kr_search_results_to_case_link_pairs, kr_judgment_text, kr_meta_judgment_dict\n",
    "        \n",
    "        run = copy.copy(kr_run)\n",
    "\n",
    "    if jurisdiction_page == 'pages/SCTA.py':\n",
    "\n",
    "        system_instruction = role_content\n",
    "                \n",
    "        from functions.scta_functions import scta_run, scta_methods_list, scta_method_types, scta_search, scta_search_results_to_case_link_pairs, scta_judgment_text, scta_meta_judgment_dict\n",
    "        \n",
    "        run = copy.copy(scta_run)\n",
    "    \n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction}]\n",
    "\n",
    "    df_individual = run(df_master)\n",
    "\n",
    "    return df_individual\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e7d5ad-6bc1-4db9-af6f-69ff1674002d",
   "metadata": {},
   "source": [
    "# Batch run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf7e161-5cd7-4fcc-b6aa-a175b83b424b",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "#Jurisdiction specific instruction and functions\n",
    "\n",
    "def gpt_batch_input_submit(jurisdiction_page, df_master):\n",
    "\n",
    "    if jurisdiction_page == 'pages/HCA.py':\n",
    "        \n",
    "        system_instruction = role_content\n",
    "        \n",
    "        from functions.hca_functions import hca_batch, hca_collections, hca_search, hca_search_results_to_judgment_links, hca_pdf_judgment, hca_meta_labels_droppable, hca_meta_judgment_dict, hca_meta_judgment_dict_alt, hca_mnc_to_link_browse, hca_citation_to_link, hca_mnc_to_link, hca_load_data, hca_data_url, hca_df, hca_judgment_to_exclude, hca_search_results_to_judgment_links_filtered_df\n",
    "    \n",
    "        batch =  copy.copy(hca_batch)\n",
    "\n",
    "    if jurisdiction_page == 'pages/NSW.py':\n",
    "        \n",
    "        system_instruction = role_content\n",
    "\n",
    "        from nswcaselaw.search import Search\n",
    "\n",
    "        from functions.nsw_functions import nsw_batch, nsw_tidying_up_prebatch, nsw_meta_labels_droppable, nsw_courts, nsw_courts_positioning, nsw_default_courts, nsw_tribunals, nsw_tribunals_positioning, nsw_court_choice, nsw_tribunal_choice, nsw_date, nsw_link, nsw_short_judgment\n",
    "    \n",
    "        batch =  copy.copy(nsw_batch)\n",
    "    \n",
    "    if jurisdiction_page == 'pages/FCA.py':\n",
    "        \n",
    "        system_instruction = role_content\n",
    "        \n",
    "        from functions.fca_functions import fca_batch, fca_courts, fca_courts_list, fca_search, fca_search_url, fca_search_results_to_judgment_links, fca_link_to_doc, fca_metalabels, fca_metalabels_droppable, fca_meta_judgment_dict, fca_pdf_name_mnc_list, fca_pdf_name\n",
    "    \n",
    "        batch = copy.copy(fca_batch)\n",
    "\n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction}]\n",
    "\n",
    "    batch_record_df_individual = batch(df_master)\n",
    "    \n",
    "    return batch_record_df_individual"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
