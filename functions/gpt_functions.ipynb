{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "519624fc-7c7a-4e54-92f6-4963a6cdd9d3",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da8ba5e8-287e-4b18-8a36-2e7f9896f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminary modules\n",
    "import base64 \n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import pause\n",
    "import os\n",
    "import io\n",
    "from io import BytesIO\n",
    "from dateutil import parser\n",
    "from dateutil.relativedelta import *\n",
    "from datetime import timedelta\n",
    "from PIL import Image\n",
    "import math\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "from io import StringIO\n",
    "import copy\n",
    "#import time\n",
    "import traceback\n",
    "\n",
    "#OpenAI\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "#Streamlit\n",
    "import streamlit as st\n",
    "#from streamlit_gsheets import GSheetsConnection\n",
    "#from streamlit.components.v1 import html\n",
    "#import streamlit_ext as ste\n",
    "\n",
    "#aws\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "#PandasAI\n",
    "#from dotenv import load_dotenv\n",
    "from pandasai import SmartDataframe\n",
    "from pandasai import Agent\n",
    "#from pandasai.llm import BambooLLM\n",
    "from pandasai.llm.openai import OpenAI\n",
    "import pandasai as pai\n",
    "from pandasai.responses.streamlit_response import StreamlitResponse\n",
    "from pandasai.helpers.openai_info import get_openai_callback as pandasai_get_openai_callback\n",
    "\n",
    "#Excel\n",
    "import openpyxl\n",
    "from pyxlsb import open_workbook as open_xlsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060e311a-e871-4e97-a59c-909c43fc3820",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_questions_answers, default_judgment_counter_bound\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'functions'"
     ]
    }
   ],
   "source": [
    "from functions.common_functions import check_questions_answers, pop_judgment, default_judgment_counter_bound, truncation_note, search_error_note, spinner_text, send_notification_email"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df8463a-f127-471c-ae05-af2efde62cf0",
   "metadata": {},
   "source": [
    "# gpt-3.5, 4o-mini and 4o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d2a4e-13ec-46b6-975a-1d951316f84f",
   "metadata": {},
   "source": [
    "## Common functions and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f01e0d-be0c-4c2c-aaa1-94d932bde51a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions for GPT are capped at 2000 characters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Upperbound on the length of questions for GPT\n",
    "\n",
    "question_characters_bound = 2000\n",
    "\n",
    "#Upperbound on number of judgments to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74be555a-f376-4d43-9bb0-0e12ef726522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function to split a string into a list by line\n",
    "def split_by_line(x):\n",
    "    y = x.split('\\n')\n",
    "    for i in y:\n",
    "        if len(i) == 0:\n",
    "            y.remove(i)\n",
    "    return y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bdefac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function to converting a dict into a line-separated string\n",
    "def dict_to_string(questions_dict):\n",
    "    questions_list = [*questions_dict.values()]\n",
    "    questions_str = '\\n'.join(questions_list)\n",
    "\n",
    "    return questions_str\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b56a336-77a9-40f8-bc0c-af9b1ac77100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function to split a list into a dictionary for list items longer than 10 characters\n",
    "#Apply split_by_line() before the following function\n",
    "def GPT_label_dict(x_list):\n",
    "    GPT_dict = {}\n",
    "    for i in x_list:\n",
    "        if len(i) > 10:\n",
    "            GPT_index = x_list.index(i) + 1\n",
    "            i_label = 'GPT question ' + f'{GPT_index}'\n",
    "            GPT_dict.update({i_label: i})\n",
    "    return GPT_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250b2292-f622-426d-a1ea-66bb29fb7a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check validity of API key\n",
    "\n",
    "@st.cache_data(show_spinner = False)\n",
    "def is_api_key_valid(key_to_check):\n",
    "    openai.api_key = key_to_check\n",
    "    \n",
    "    try:\n",
    "        completion = openai.chat.completions.create(\n",
    "            #model=\"gpt-3.5-turbo-0125\",\n",
    "            model = 'gpt-4o-mini', \n",
    "            messages=[{\"role\": \"user\", \"content\": 'Hi'}], \n",
    "            max_tokens = 1\n",
    "        )\n",
    "    except:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d514f02e-4163-4afa-94da-6f01e996b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define input and output costs, token caps and maximum characters\n",
    "#each token is about 4 characters\n",
    "\n",
    "def tokens_cap(gpt_model):\n",
    "    #This is the global cap for each model, which will be shown to users\n",
    "    #Leaving 1000 tokens to spare\n",
    "    \n",
    "    if gpt_model == \"gpt-3.5-turbo-0125\":\n",
    "        \n",
    "        tokens_cap = int(16385 - 3000) #For GPT-3.5-turbo, token limit covering BOTH input and output is 16385,  while the output limit is 4096.\n",
    "    \n",
    "    elif \"gpt-4o-mini\" in gpt_model:\n",
    "        tokens_cap = int(128000 - 3000) #For gpt-4o-mini, token limit covering both BOTH and output is 128000, while the output limit is 16384.\n",
    "\n",
    "    else: #(\"gpt-4o\" in gpt_model) and ('mini' not in gpt_model):\n",
    "        tokens_cap = int(128000 - 3000) #For gpt-4o, token limit covering both BOTH and output is 128000, while the output limit is 16384.\n",
    "\n",
    "    return tokens_cap\n",
    "\n",
    "def gpt_input_cost(gpt_model):\n",
    "    if gpt_model == \"gpt-3.5-turbo-0125\":\n",
    "        gpt_input_cost = 1/1000000*0.5\n",
    "\n",
    "    elif \"gpt-4o-mini\" in gpt_model:\n",
    "        gpt_input_cost = 1/1000000*0.15\n",
    "    \n",
    "    else: #(\"gpt-4o\" in gpt_model) and ('mini' not in gpt_model):\n",
    "        gpt_input_cost = 1/1000000*2.5\n",
    "        \n",
    "    return gpt_input_cost\n",
    "\n",
    "def gpt_output_cost(gpt_model):\n",
    "    if gpt_model == \"gpt-3.5-turbo-0125\":\n",
    "        gpt_output_cost = 1/1000000*1.5\n",
    "        \n",
    "    elif \"gpt-4o-mini\" in gpt_model:\n",
    "        gpt_output_cost = 1/1000000*0.6\n",
    "\n",
    "    else: #(\"gpt-4o\" in gpt_model) and ('mini' not in gpt_model):\n",
    "        gpt_output_cost = 1/1000000*10\n",
    "\n",
    "    return gpt_output_cost\n",
    "    \n",
    "def max_output(gpt_model, messages_for_GPT):\n",
    "\n",
    "    if gpt_model == \"gpt-3.5-turbo-0125\":\n",
    "        \n",
    "        max_output_tokens = int(16385 - num_tokens_from_string(str(messages_for_GPT), \"cl100k_base\")) #For GPT-3.5-turbo, token limit covering BOTH input and output is 16385,  while the output limit is 4096.\n",
    "\n",
    "    elif gpt_model == \"gpt-4o-mini\":\n",
    "        \n",
    "        max_output_tokens = int(128000 - num_tokens_from_string(str(messages_for_GPT), \"cl100k_base\")) #For gpt-4o-mini, token limit covering both BOTH and output is 128000, while the output limit is 16384.\n",
    "\n",
    "    else: #(\"gpt-4o\" in gpt_model) and ('mini' not in gpt_model):\n",
    "        \n",
    "        max_output_tokens = int(128000 - num_tokens_from_string(str(messages_for_GPT), \"cl100k_base\")) #For gpt-4o, token limit covering both BOTH and output is 128000, while the output limit is 16384.s\n",
    "    \n",
    "    return min(4096, abs(max_output_tokens))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a8315-bb1c-4288-a013-472b3864b72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_msg = f'**Please enter your search terms.** By default, this app will collect (ie scrape) up to {default_judgment_counter_bound} cases, and process up to approximately {round(tokens_cap(\"gpt-4o-mini\")*3/4)} words from each case.'\n",
    "\n",
    "default_caption = f'Please reach out to Ben Chen at ben.chen@sydney.edu.au should you wish to cover more cases.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "891018f7-b3c1-4af0-a666-b12a487d6896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokens estimate preliminaries\n",
    "\n",
    "#encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "#encoding = tiktoken.encoding_for_model(gpt_model)\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    #Tokens estimate function\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    \n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e331f47e-cc3b-4762-a743-bb82d766cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judgment_prompt_json(judgment_json, gpt_model):\n",
    "\n",
    "    #Remove hyperlink\n",
    "    for key in judgment_json.keys():\n",
    "        if 'hyperlink' in key.lower():\n",
    "            judgment_json[key] = ''\n",
    "            break\n",
    "\n",
    "    #Determine whether 'judgment', opinions, 'recap_documents', or extracted_text contains text\n",
    "\n",
    "    text_key = ''\n",
    "\n",
    "    #Loop through non-b64 keys\n",
    "    for key in ['judgment', 'opinions', 'recap_documents', 'extracted_text']:\n",
    "        if key in judgment_json.keys():\n",
    "            text_key = key\n",
    "            break\n",
    "    \n",
    "    #st.write(f\"text_key is {text_key}\")\n",
    "    \n",
    "    #Just use original judgment_json if no long text\n",
    "    if text_key not in judgment_json.keys():\n",
    "        return judgment_json\n",
    "\n",
    "    else:        \n",
    "        #Turn judgment, opinions, 'recap_documents', or extracted_text to string\n",
    "        if isinstance(judgment_json[text_key], list):\n",
    "            try:\n",
    "                judgment_to_string = '\\n'.join(judgment_json[text_key])\n",
    "    \n",
    "            except:\n",
    "                judgment_to_string = str(judgment_json[text_key])\n",
    "            \n",
    "        elif isinstance(judgment_json[text_key], str):\n",
    "            judgment_to_string = judgment_json[text_key]\n",
    "            \n",
    "        else:\n",
    "            judgment_to_string = str(judgment_json[text_key])\n",
    "    \n",
    "        #Truncate judgment, opinions, 'recap_documents', or extracted_text if needed\n",
    "        judgment_content = f'Based on the metadata and {text_key} in the following JSON: \"\"\" {json.dumps(judgment_json, default=str)} \"\"\"'\n",
    "    \n",
    "        judgment_content_tokens = num_tokens_from_string(judgment_content, \"cl100k_base\")\n",
    "        \n",
    "        if judgment_content_tokens <= tokens_cap(gpt_model):\n",
    "            \n",
    "            return judgment_content\n",
    "    \n",
    "        else:\n",
    "            \n",
    "            meta_data_len = judgment_content_tokens - num_tokens_from_string(judgment_to_string, \"cl100k_base\")\n",
    "    \n",
    "            intro_len = num_tokens_from_string(f'Based on the metadata and {text_key} in the following JSON: \"\"\"  \"\"\"', \"cl100k_base\")\n",
    "            \n",
    "            judgment_chars_capped = int(round((tokens_cap(gpt_model) - meta_data_len - intro_len)*4))\n",
    "            \n",
    "            judgment_string_trimmed = judgment_to_string[ :int(judgment_chars_capped/2)] + judgment_to_string[-int(judgment_chars_capped/2): ]\n",
    "    \n",
    "            judgment_json[text_key] = judgment_string_trimmed     \n",
    "            \n",
    "            judgment_content_capped = f'Based on the metadata and {text_key} in the following JSON:  \"\"\" {json.dumps(judgment_json, default=str)} \"\"\"'\n",
    "            \n",
    "            return judgment_content_capped\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2252c08-65de-4b69-b132-64d5c72f411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check questions for potential privacy infringement\n",
    "\n",
    "questions_check_system_instruction = \"\"\"\n",
    "You are a compliance officer helping a human ethics committee to ensure that no personally identifiable information will be exposed. \n",
    "You will be given questions to check in JSON form. Please provide labels for these questions based only on information contained in the JSON.\n",
    "Where a question seeks information about a person's birth or address, you label \"1\". If a question does not seek such information, you label \"0\". If you are not sure, label \"unclear\".\n",
    "For example, the question \"What's the plaintiff's date of birth?\" should be labelled \"1\".\n",
    "For example, the question \"What's the defendant's address?\" should be labelled \"1\".\n",
    "For example, the question \"What's the victim's date of death?\" should be labelled \"0\".\n",
    "For example, the question \"What's the judge's name?\" should be labelled \"0\".\n",
    "For example, the question \"What's the defendant's age?\" should be labelled \"0\".\n",
    "\"\"\"\n",
    "\n",
    "#More general below\n",
    "#questions_check_system_instruction = \"\"\"\n",
    "#You are a compliance officer helping a human ethics committee to ensure that no personally identifiable information will be exposed. \n",
    "#You will be given questions to check in JSON form. \n",
    "#Based only on information contained in the JSON, please check each question for whether it seeks a person's birth, address, or other personally identifiable information. \n",
    "#Where a question indeed seeks personally identifiable information, you label \"1\". \n",
    "#Where a question does not seek personally identifiable information, you label \"0\". \n",
    "#If you are not sure, label \"unclear\".\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2735d310-08d5-4a3c-af07-7864eaa2e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check questions for potential privacy infringement\n",
    "#For instant mode\n",
    "\n",
    "#Don't add @st.cache_data\n",
    "\n",
    "def GPT_questions_label(_questions_json, gpt_model, questions_check_system_instruction):\n",
    "    #'question_json' variable is a json of questions to GPT\n",
    "    #'jugdment' variable is a judgment_json   \n",
    "    #Returns a json of checked questions\n",
    "\n",
    "    json_direction = [{\"role\": \"user\", \"content\": 'Label the following questions in JSON form.'}]\n",
    "\n",
    "    #Create answer format\n",
    "    \n",
    "    q_keys = [*_questions_json]\n",
    "    \n",
    "    labels_json = {}\n",
    "    \n",
    "    for q_index in q_keys:\n",
    "        labels_json.update({q_index: 'Your label for the question with index ' + q_index})\n",
    "    \n",
    "    #Create questions, which include the answer format\n",
    "    \n",
    "    question_to_check = [{\"role\": \"user\", \"content\": json.dumps(_questions_json, default = str) + ' Return labels in the following JSON form: ' + json.dumps(labels_json, default = str)}]\n",
    "    \n",
    "    #Create messages in one prompt for GPT\n",
    "    \n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": questions_check_system_instruction}]\n",
    "    #messages_for_GPT = intro_for_GPT + judgment_for_GPT + json_direction + question_to_check\n",
    "    messages_for_GPT = intro_for_GPT + json_direction + question_to_check\n",
    "    \n",
    "#   return messages_for_GPT\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    try:\n",
    "        #completion = client.chat.completions.create(\n",
    "        completion = openai.chat.completions.create(\n",
    "            model = gpt_model,\n",
    "            messages = messages_for_GPT, \n",
    "            response_format = {\"type\": \"json_object\"}, \n",
    "            max_tokens = max_output(gpt_model, messages_for_GPT), \n",
    "            temperature = 0.1, \n",
    "            #top_p = 0.1\n",
    "        )\n",
    "        \n",
    "#        return completion.choices[0].message.content #This gives answers as a string containing a dictionary\n",
    "        \n",
    "        #To obtain a json directly, use below\n",
    "        labels_dict = json.loads(completion.choices[0].message.content)\n",
    "        \n",
    "        #Obtain tokens\n",
    "        output_tokens = completion.usage.completion_tokens\n",
    "        \n",
    "        prompt_tokens = completion.usage.prompt_tokens\n",
    "        \n",
    "        return [labels_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "    except Exception as error:\n",
    "        \n",
    "        for q_index in q_keys:\n",
    "            \n",
    "            labels_json.update({q_index: error})\n",
    "        \n",
    "        return [labels_json, 0, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6981ddf-3177-490a-81b2-21bf2e383855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display unanswered questions\n",
    "def unanswered_questions(unchecked_questions_json, checked_questions_json):\n",
    "\n",
    "    #Reset unanswered questions text for batch get email\n",
    "    st.session_state['unanswered_questions'] = ''\n",
    "\n",
    "    #Produce unanswered questions\n",
    "    unanswered_questions_list = []\n",
    "    \n",
    "    for question in unchecked_questions_json.values():\n",
    "        if question not in checked_questions_json.values():\n",
    "            unanswered_questions_list.append(question)\n",
    "\n",
    "    if len(unanswered_questions_list) > 0:\n",
    "                \n",
    "        if len(unanswered_questions_list) == 1:\n",
    "\n",
    "            witheld_text = 'To avoid exposing personally identifiable information, the following question was witheld:\\n\\n'\n",
    "        \n",
    "        if len(unanswered_questions_list) > 1: \n",
    "            \n",
    "            witheld_text = 'To avoid exposing personally identifiable information, the following questions were witheld:\\n\\n'\n",
    "\n",
    "        witheld_text += '\\n\\n'.join(unanswered_questions_list)\n",
    "    \n",
    "        #Display unanswered questions\n",
    "        st.warning(witheld_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba47e604-6133-4a1f-8f30-844246ee5355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to replace unchecked questions with checked questions\n",
    "def checked_questions_json(questions_json, gpt_labels_output):\n",
    "    \n",
    "    checked_questions_json = questions_json\n",
    "\n",
    "    for q_key in gpt_labels_output[0]:\n",
    "        \n",
    "        if str(gpt_labels_output[0][q_key]) == '1':\n",
    "            \n",
    "            checked_questions_json.pop(q_key)\n",
    "    \n",
    "    return checked_questions_json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c101822",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Check questions for potential privacy infringement\n",
    "#Don't add @st.cache_data\n",
    "\n",
    "def GPT_questions_check(_questions_json_or_string, gpt_model, questions_check_system_instruction):\n",
    "    #'questions_str' variable is a string of questions to GPT\n",
    "    #Returns both a string and a json of checked questions, together with costs, and displays any witheld questions\n",
    "    \n",
    "    #Create dict of questions for GPT\n",
    "\n",
    "    if isinstance(_questions_json_or_string, str):\n",
    "    \n",
    "        questions_list = split_by_line(_questions_json_or_string[0: question_characters_bound])\n",
    "        questions_json = GPT_label_dict(questions_list)\n",
    "\n",
    "    else:\n",
    "        questions_json = _questions_json_or_string\n",
    "\n",
    "    #Check questions for privacy violation\n",
    "    \n",
    "    try:\n",
    "\n",
    "        unchecked_questions_json = questions_json.copy()\n",
    "        \n",
    "        labels_output = GPT_questions_label(questions_json, gpt_model, questions_check_system_instruction)\n",
    "\n",
    "        questions_check_output_tokens = labels_output[1]\n",
    "\n",
    "        questions_check_input_tokens = labels_output[2]\n",
    "    \n",
    "        questions_json = checked_questions_json(questions_json, labels_output)\n",
    "\n",
    "        unanswered_questions(unchecked_questions_json, questions_json)\n",
    "\n",
    "        print('Questions checked.')\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print('Questions check failed.')\n",
    "        print(e)\n",
    "\n",
    "\n",
    "        #create placeholder input and output tokens\n",
    "        questions_check_output_tokens = 0\n",
    "        questions_check_input_tokens = 0\n",
    "        \n",
    "    \n",
    "    #Returns a stirng of questions\n",
    "    questions_string = dict_to_string(questions_json)\n",
    "\n",
    "    return {'questions_json': questions_json, \n",
    "            'questions_string': questions_string, \n",
    "            'questions_check_output_tokens': questions_check_output_tokens, \n",
    "            'questions_check_input_tokens': questions_check_input_tokens\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "768bc5f3-d1ef-4cd8-a353-86a4e6822ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check questions for potential privacy infringement\n",
    "\n",
    "answers_check_system_instruction = \"\"\"\n",
    "You are a compliance officer helping an academic researcher to redact information about birth and address. \n",
    "You will be given text to check in JSON form. Please check the text based only on information contained in the JSON. \n",
    "Where any part of the text identifies birth or an address, you replace that part with \"[redacted]\". \n",
    "You then return the remainder of the text unredacted.\n",
    "You redact birth and address only. Do not redact anything else, such as names, date of death, age.\n",
    "For example, if the text given to you is \"John Smith, born 1 January 1950, died on 20 December 2008 at 1 Main St Blackacre aged 58.\", you return \"John Smith, born [redacted], died on 20 December 2008 at [redacted] aged 58.\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeaf1a7f-9229-4843-b341-1f29fab94b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check _answers_to_check_json for potential privacy infringement\n",
    "#Don't add @st.cache_data\n",
    "\n",
    "def GPT_answers_check(_answers_to_check_json, gpt_model, answers_check_system_instruction):\n",
    "\n",
    "    #Check answers\n",
    "    \n",
    "    json_direction = [{\"role\": \"user\", \"content\": 'Check the following text in JSON form.'}]\n",
    "\n",
    "    #Create answer format\n",
    "\n",
    "    answers_to_check_list = [_answers_to_check_json]\n",
    "\n",
    "    redacted_answers_json = {}\n",
    "    \n",
    "    if isinstance(_answers_to_check_json, list):\n",
    "        \n",
    "        answers_to_check_list = _answers_to_check_json\n",
    "\n",
    "    for _answers_to_check_json in answers_to_check_list:\n",
    "        \n",
    "        q_keys = [*_answers_to_check_json]\n",
    "        \n",
    "        for q_index in q_keys:\n",
    "            \n",
    "            redacted_answers_json.update({q_index: 'Your response.'})\n",
    "\n",
    "    #Create _answers_to_check_json, which include the answer format\n",
    "    \n",
    "    question_to_check = [{\"role\": \"user\", \"content\": json.dumps(_answers_to_check_json, default = str) + ' \\n Respond in the following JSON form: ' + json.dumps(redacted_answers_json, default = str)}]\n",
    "    \n",
    "    #Create messages in one prompt for GPT\n",
    "    \n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": answers_check_system_instruction}]\n",
    "    messages_for_GPT = intro_for_GPT + json_direction + question_to_check\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    try:\n",
    "        #completion = client.chat.completions.create(\n",
    "        completion = openai.chat.completions.create(\n",
    "            model = gpt_model,\n",
    "            messages = messages_for_GPT, \n",
    "            response_format = {\"type\": \"json_object\"}, \n",
    "            max_tokens = max_output(gpt_model, messages_for_GPT), \n",
    "            temperature = 0.1, \n",
    "            #top_p = 0.1\n",
    "        )\n",
    "        \n",
    "#        return completion.choices[0].message.content #This gives answers as a string containing a dictionary\n",
    "        \n",
    "        #To obtain a json directly, use below\n",
    "        redacted_answers_dict = json.loads(completion.choices[0].message.content)\n",
    "        \n",
    "        #Obtain tokens\n",
    "        redacted_answers_output_tokens = completion.usage.completion_tokens\n",
    "        \n",
    "        redacted_answers_prompt_tokens = completion.usage.prompt_tokens\n",
    "\n",
    "        print('Answers checked.')\n",
    "\n",
    "    except Exception as error:\n",
    "        \n",
    "        print('Answers check failed.')\n",
    "\n",
    "        #Create placeholder GPT answers check output\n",
    "        redacted_answers_dict = {}\n",
    "        for q_index in q_keys:\n",
    "            redacted_answers_dict.update({q_index: error})\n",
    "\n",
    "        redacted_answers_output_tokens = 0\n",
    "\n",
    "        redacted_answers_prompt_tokens = 0\n",
    "        \n",
    "    return [redacted_answers_dict, redacted_answers_output_tokens, redacted_answers_prompt_tokens]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bae612f3-eabf-4dfe-abbb-2d97adf5558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For modern judgments, define system role content for GPT\n",
    "role_content = \"\"\"You are a legal research assistant helping an academic researcher to answer questions about a public judgment and court record. You will be provided with the judgment, record and the associated metadata in JSON form. \n",
    "Please answer questions based only on information contained in the judgment, record and metadata. Where your answer comes from specific paragraphs, pages or sections of the judgment, record or metadata, include a reference to those paragraphs, pages or sections. \n",
    "If you cannot answer the questions based on the judgment, record or metadata, do not make up information, but instead write 'answer not found'. \n",
    "Respond in JSON form. In your response, produce as many keys as you need. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560df91d-1cce-4fb8-b39f-b7ec11b77c0f",
   "metadata": {},
   "source": [
    "## GPT instant response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b06e2156-74f5-41df-94c0-a539ad91a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT answer function for answers in json form, YES TOKENS\n",
    "#IN USE\n",
    "\n",
    "@st.cache_data(show_spinner = False)\n",
    "def GPT_json(questions_json, df_example, judgment_json, gpt_model, system_instruction):\n",
    "    #'question_json' variable is a json of questions to GPT\n",
    "    #'jugdment' variable is a judgment_json   \n",
    "\n",
    "    judgment_for_GPT = [{\"role\": \"user\", \"content\": judgment_prompt_json(judgment_json, gpt_model)}]\n",
    "\n",
    "    json_direction = [{\"role\": \"user\", \"content\": 'You will be given questions to answer in JSON form.'}]\n",
    "\n",
    "    #Create answer format\n",
    "    answers_json = {}\n",
    "    \n",
    "    if len(df_example.replace('\"', '')) > 0:\n",
    "\n",
    "        try:\n",
    "            \n",
    "            if isinstance(df_example, str):\n",
    "                \n",
    "                answers_json = json.loads(df_example)\n",
    "\n",
    "            if isinstance(df_example, dict):\n",
    "                \n",
    "                answers_json = df_example\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Example provided but can't produce json to send to GPT.\")\n",
    "            print(e)\n",
    "    \n",
    "    q_keys = [*questions_json]\n",
    "    \n",
    "    if len(answers_json) == 0:\n",
    "\n",
    "        q_counter = 1\n",
    "        for q_index in q_keys:\n",
    "            answers_json.update({f'GPT question {q_counter}: {questions_json[q_index]}': f'Your answer. (The paragraphs, pages or sections from which you obtained your answer)'})\n",
    "            q_counter += 1\n",
    "    \n",
    "    #Create questions, which include the answer format\n",
    "    question_for_GPT = [{\"role\": \"user\", \"content\": json.dumps(questions_json, default = str) + ' \\n Respond in the following JSON form: ' + json.dumps(answers_json, default = str)}]\n",
    "    \n",
    "    #Create messages in one prompt for GPT\n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction}]\n",
    "    messages_for_GPT = intro_for_GPT + judgment_for_GPT + json_direction + question_for_GPT\n",
    "    \n",
    "#   return messages_for_GPT\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        #completion = client.chat.completions.create(\n",
    "        completion = openai.chat.completions.create(\n",
    "            model = gpt_model,\n",
    "            messages = messages_for_GPT, \n",
    "            response_format = {\"type\": \"json_object\"}, \n",
    "            max_tokens = max_output(gpt_model, messages_for_GPT), \n",
    "            temperature = 0.1, \n",
    "            #top_p = 0.1\n",
    "        )\n",
    "        \n",
    "#        return completion.choices[0].message.content #This gives answers as a string containing a dictionary\n",
    "        \n",
    "        #To obtain a json directly\n",
    "        answers_dict = json.loads(completion.choices[0].message.content)\n",
    "\n",
    "        #Obtain tokens\n",
    "        output_tokens = completion.usage.completion_tokens\n",
    "        \n",
    "        prompt_tokens = completion.usage.prompt_tokens\n",
    "        \n",
    "        return [answers_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "    except Exception as error:\n",
    "        \n",
    "        print('GPT failed to produce answers.')\n",
    "        \n",
    "        for q_index in q_keys:\n",
    "            \n",
    "            answers_json.update({q_index: error})\n",
    "        \n",
    "        return [answers_json, 0, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71837ede-4057-4eae-abd6-b386a0b701d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT function for each respondent's dataframe, index by judgment then question, with input and output tokens given by GPT itself\n",
    "#IN USE\n",
    "\n",
    "#The following function DOES NOT check for existence of questions for GPT\n",
    "    # To so check, active line marked as #*\n",
    "\n",
    "@st.cache_data(show_spinner = False)\n",
    "def engage_GPT_json(questions_json, df_example, df_individual, GPT_activation, gpt_model, system_instruction):\n",
    "    # Variable questions_json refers to the json of questions\n",
    "    # Variable df_individual refers to each respondent's df\n",
    "    # Variable activation refers to status of GPT activation (real or test)\n",
    "    # The output is a new JSON for the relevant respondent with new columns re:\n",
    "        # f\"File length in tokens (up to {tokens_cap(gpt_model)} given to GPT)\"\n",
    "        # 'GPT cost estimate (USD excl GST)'\n",
    "        # 'GPT time estimate (seconds)'\n",
    "        # GPT questions/answers\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "\n",
    "    #Check questions for privacy violation\n",
    "\n",
    "    if check_questions_answers() > 0:\n",
    "    \n",
    "        questions_checked_dict = GPT_questions_check(questions_json, gpt_model, questions_check_system_instruction)\n",
    "\n",
    "        questions_json = questions_checked_dict['questions_json']\n",
    "    \n",
    "        questions_check_output_tokens = questions_checked_dict['questions_check_output_tokens']\n",
    "    \n",
    "        questions_check_input_tokens = questions_checked_dict['questions_check_input_tokens']\n",
    "\n",
    "    else:\n",
    "\n",
    "        print('Questions not checked.')\n",
    "        \n",
    "        questions_check_output_tokens = 0\n",
    "        \n",
    "        questions_check_input_tokens = 0\n",
    "    \n",
    "    #Process questions\n",
    "\n",
    "    #GPT use counter\n",
    "    gpt_use_counter = 0\n",
    "    \n",
    "    for judgment_index in df_individual.index:\n",
    "        \n",
    "        judgment_json = df_individual.to_dict('index')[judgment_index]\n",
    "\n",
    "        #Check wither error in getting the full text\n",
    "        text_error = False\n",
    "        for text_key in ['judgment', 'opinions', 'recap_documents', 'extracted_text']:\n",
    "            if text_key in judgment_json.keys():\n",
    "\n",
    "                #Checking if judgment_json[text_key] is np.nan\n",
    "                if isinstance(judgment_json[text_key], float):\n",
    "                \n",
    "                    judgment_json[text_key] = ''\n",
    "                    \n",
    "                if len(judgment_json[text_key]) == 0:\n",
    "                    text_error = True\n",
    "                    df_individual.loc[judgment_index, 'Note'] = search_error_note\n",
    "                    print(f\"Case/file indexed {judgment_index} not sent to GPT given full text was not scrapped.\")\n",
    "                        \n",
    "                break\n",
    "        \n",
    "        #Calculate and append number of tokens of judgment, regardless of whether given to GPT\n",
    "        judgment_tokens = num_tokens_from_string(str(judgment_json), \"cl100k_base\")\n",
    "        df_individual.loc[judgment_index, f\"File length in tokens (up to {tokens_cap(gpt_model)} given to GPT)\"] = judgment_tokens       \n",
    "\n",
    "        #Indicate whether judgment truncated        \n",
    "        if judgment_tokens > tokens_cap(gpt_model):\n",
    "            df_individual.loc[judgment_index, 'Note'] = truncation_note\n",
    "\n",
    "        #Create columns for respondent's GPT cost, time\n",
    "        df_individual.loc[judgment_index, 'GPT cost estimate (USD excl GST)'] = ''\n",
    "        df_individual.loc[judgment_index, 'GPT time estimate (seconds)'] = ''\n",
    "                \n",
    "        #Calculate GPT start time\n",
    "\n",
    "        GPT_start_time = datetime.now()\n",
    "\n",
    "        #Depending on activation status, apply GPT_json function to each judgment, gives answers as a string containing a dictionary\n",
    "\n",
    "        if ((int(GPT_activation) > 0) and (text_error == False)):\n",
    "            GPT_output_list = GPT_json(questions_json, df_example, judgment_json, gpt_model, system_instruction) #Gives [answers as a JSON, output tokens, input tokens]\n",
    "            answers_dict = GPT_output_list[0]\n",
    "\n",
    "            #Check answers for potential policy violation\n",
    "            if check_questions_answers() > 0:\n",
    "            \n",
    "                GPT_answers_check_output_list = GPT_answers_check(answers_dict, gpt_model, answers_check_system_instruction)\n",
    "\n",
    "                #Get potentially redacted answers and costs\n",
    "                answers_dict = GPT_answers_check_output_list[0]\n",
    "                redacted_answers_output_tokens = GPT_answers_check_output_list[1]\n",
    "                redacted_answers_prompt_tokens = GPT_answers_check_output_list[2]\n",
    "\n",
    "            else:\n",
    "                print('Answers not checked.')\n",
    "                redacted_answers_output_tokens = 0\n",
    "                redacted_answers_prompt_tokens = 0\n",
    "\n",
    "            #Calculate GPT cost of answering questions\n",
    "            answers_output_tokens = GPT_output_list[1] + redacted_answers_output_tokens\n",
    "            answers_input_tokens  = GPT_output_list[2] + redacted_answers_output_tokens\n",
    "                    \n",
    "            #Calculate and append GPT finish time and time difference to individual df\n",
    "            GPT_finish_time = datetime.now()\n",
    "            \n",
    "            GPT_time_difference = GPT_finish_time - GPT_start_time\n",
    "    \n",
    "            df_individual.loc[judgment_index, 'GPT time estimate (seconds)'] = GPT_time_difference.total_seconds()    \n",
    "\n",
    "            #Display GPT use counter\n",
    "            gpt_use_counter += 1\n",
    "            print(f\"GPT proccessed {gpt_use_counter}/{len(df_individual)} cases.\")\n",
    "\n",
    "        else:\n",
    "            answers_dict = {}\n",
    "            \n",
    "            question_keys = [*questions_json]\n",
    "            \n",
    "            for q_index in question_keys:\n",
    "                #Increases judgment index by 2 to ensure consistency with Excel spreadsheet\n",
    "                answer = ''\n",
    "                answers_dict.update({questions_json[q_index]: answer})\n",
    "\n",
    "            #st.write(answers_dict)\n",
    "            \n",
    "            #Calculate capped judgment tokens\n",
    "\n",
    "            judgment_capped_tokens = num_tokens_from_string(judgment_prompt_json(judgment_json, gpt_model), \"cl100k_base\")\n",
    "\n",
    "            #Calculate questions tokens and cost\n",
    "\n",
    "            questions_tokens = num_tokens_from_string(json.dumps(questions_json, default = str), \"cl100k_base\")\n",
    "\n",
    "            #Calculate other instructions' tokens\n",
    "\n",
    "            other_instructions = system_instruction + 'you will be given questions to answer in JSON form.' + ' \\n Respond in the following JSON form: '\n",
    "\n",
    "            other_tokens = num_tokens_from_string(other_instructions, \"cl100k_base\") + len(question_keys)*num_tokens_from_string(\"GPT question x:  Your answer. (The paragraphs, pages or sections from which you obtained your answer)\", \"cl100k_base\")\n",
    "\n",
    "            #Calculate number of tokens of answers\n",
    "            answers_output_tokens = num_tokens_from_string(str(answers_dict), \"cl100k_base\")\n",
    "\n",
    "            answers_input_tokens = judgment_capped_tokens + questions_tokens + other_tokens\n",
    "            \n",
    "    \t#Create GPT question headings, append answers to individual spreadsheets, and remove template answers\n",
    "\n",
    "        #answers_list = [answers_dict]\n",
    "\n",
    "        #if isinstance(answers_dict, list):\n",
    "            #answers_list = answers_dict\n",
    "        \n",
    "        #for answers_dict in answers_list:        \n",
    "        for answer_index in answers_dict.keys():\n",
    "\n",
    "            #Check any errors\n",
    "            answer_string = str(answers_dict[answer_index]).lower()\n",
    "            \n",
    "            if ((answer_string.startswith('your answer.')) or (answer_string.startswith('your response.'))):\n",
    "                \n",
    "                answers_dict[answer_index] = 'Error. Please try a different question or GPT model.'\n",
    "\n",
    "            #Append answer to spreadsheet\n",
    "\n",
    "            answer_header = answer_index\n",
    "\n",
    "            try:\n",
    "            \n",
    "                df_individual.loc[judgment_index, answer_header] = answers_dict[answer_index]\n",
    "\n",
    "            except:\n",
    "\n",
    "                df_individual.loc[judgment_index, answer_header] = str(answers_dict[answer_index])\n",
    "        \n",
    "        #Calculate GPT costs\n",
    "\n",
    "        #If no check for questions\n",
    "        #GPT_cost = answers_output_tokens*gpt_output_cost(gpt_model) + answers_input_tokens*gpt_input_cost(gpt_model)\n",
    "\n",
    "        #If check for questions\n",
    "        GPT_cost = (answers_output_tokens + questions_check_output_tokens/len(df_individual))*gpt_output_cost(gpt_model) + (answers_input_tokens + questions_check_input_tokens/len(df_individual))*gpt_input_cost(gpt_model)\n",
    "\n",
    "        #Calculate and append GPT cost to individual df\n",
    "        df_individual.loc[judgment_index, 'GPT cost estimate (USD excl GST)'] = GPT_cost\n",
    "    \n",
    "    return df_individual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bff7e3-97ad-4099-bcf5-3b9572ae32d8",
   "metadata": {},
   "source": [
    "## Vision instant mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02a9b3f7-84a5-4852-9515-b0a3ee0a7178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokens counter\n",
    "\n",
    "def get_image_dims(image):\n",
    "    if re.match(r\"data:image\\/\\w+;base64\", image):\n",
    "        image = re.sub(r\"data:image\\/\\w+;base64,\", \"\", image)\n",
    "        image = Image.open(BytesIO(base64.b64decode(image)))\n",
    "        return image.size\n",
    "    else:\n",
    "        raise ValueError(\"Image must be a base64 string.\")\n",
    "\n",
    "def calculate_image_token_cost(image, detail=\"auto\"):\n",
    "    # Constants\n",
    "    LOW_DETAIL_COST = 85\n",
    "    HIGH_DETAIL_COST_PER_TILE = 170\n",
    "    ADDITIONAL_COST = 85\n",
    "\n",
    "    if detail == \"auto\":\n",
    "        # assume high detail for now\n",
    "        detail = \"high\"\n",
    "\n",
    "    if detail == \"low\":\n",
    "        # Low detail images have a fixed cost\n",
    "        return LOW_DETAIL_COST\n",
    "    elif detail == \"high\":\n",
    "        # Calculate token cost for high detail images\n",
    "        width, height = get_image_dims(image)\n",
    "        # Check if resizing is needed to fit within a 2048 x 2048 square\n",
    "        if max(width, height) > 2048:\n",
    "            # Resize the image to fit within a 2048 x 2048 square\n",
    "            ratio = 2048 / max(width, height)\n",
    "            width = int(width * ratio)\n",
    "            height = int(height * ratio)\n",
    "        # Further scale down to 768px on the shortest side\n",
    "        if min(width, height) > 768:\n",
    "            ratio = 768 / min(width, height)\n",
    "            width = int(width * ratio)\n",
    "            height = int(height * ratio)\n",
    "        # Calculate the number of 512px squares\n",
    "        num_squares = math.ceil(width / 512) * math.ceil(height / 512)\n",
    "        # Calculate the total token cost\n",
    "        total_cost = num_squares * HIGH_DETAIL_COST_PER_TILE + ADDITIONAL_COST\n",
    "        return total_cost\n",
    "    else:\n",
    "        # Invalid detail_option\n",
    "        raise ValueError(\"Invalid value for detail parameter. Use 'low' or 'high'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea52781-420e-46cb-854b-cf36a63ff3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT answer function for answers in json form, YES TOKENS\n",
    "#For gpt-4o vision\n",
    "\n",
    "@st.cache_data(show_spinner = False)\n",
    "def GPT_b64_json(questions_json, df_example, judgment_json, gpt_model, system_instruction):\n",
    "    #'question_json' variable is a json of questions to GPT\n",
    "\n",
    "    #file_for_GPT = [{\"role\": \"user\", \"content\": file_prompt(file_triple, gpt_model) + 'you will be given questions to answer in JSON form.'}]\n",
    "\n",
    "    #Add images to messages to GPT\n",
    "    image_content_value = [{\"type\": \"text\", \"text\": 'Based on the following images:'}]\n",
    "\n",
    "    for key in ['judgment_b64', 'b64_list']:\n",
    "        if key in judgment_json.keys():\n",
    "            for image_b64 in judgment_json[key]:\n",
    "                image_message_to_attach = {\"type\": \"image_url\", \"image_url\": {\"url\": image_b64,}}\n",
    "                image_content_value.append(image_message_to_attach)\n",
    "            break\n",
    "\n",
    "    image_content = [{\"role\": \"user\", \n",
    "                      \"content\": image_content_value\n",
    "                     }\n",
    "                  ]\n",
    "\n",
    "    metadata_content = [{\"role\": \"user\", \"content\": ''}]\n",
    "\n",
    "    metadata_json_raw = judgment_json\n",
    "\n",
    "    for key in ['tokens_raw', 'judgment_b64', 'b64_list']: #'Hyperlink to CommonLII'\n",
    "        if key in metadata_json_raw.keys():\n",
    "            metadata_json_raw.pop(key)\n",
    "        #except:\n",
    "            #print(f'Unable to remove {key} from metadata_json_raw')\n",
    "\n",
    "    metadata_json = metadata_json_raw\n",
    "\n",
    "    #print(f\"metadata_json == {metadata_json}\")\n",
    "    \n",
    "    #if 'judgment' not in metadata_json.keys():\n",
    "    metadata_content = [{\"role\": \"user\", \"content\": 'Based on the following metadata:' + str(metadata_json)}]\n",
    "\n",
    "    #print(f\"metadata_content == {metadata_content}\")\n",
    "\n",
    "    #Create json direction content\n",
    "\n",
    "    json_direction = [{\"role\": \"user\", \"content\": 'You will be given questions to answer in JSON form.'}]\n",
    "\n",
    "    file_for_GPT = image_content + metadata_content + json_direction\n",
    "    \n",
    "    #Create answer format\n",
    "    answers_json = {}\n",
    "\n",
    "    if len(df_example.replace('\"', '')) > 0:\n",
    "\n",
    "        try:\n",
    "            \n",
    "            if isinstance(df_example, str):\n",
    "                \n",
    "                answers_json = json.loads(df_example)\n",
    "\n",
    "            if isinstance(df_example, dict):\n",
    "                \n",
    "                answers_json = df_example\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Example provided but can't produce json to send to GPT.\")\n",
    "            print(e)\n",
    "    \n",
    "    q_keys = [*questions_json]\n",
    "    \n",
    "    if len(answers_json) == 0:\n",
    "        q_counter = 1\n",
    "        for q_index in q_keys:\n",
    "            answers_json.update({f'GPT question {q_counter}: {questions_json[q_index]}': f'Your answer. (The paragraphs, pages or sections from which you obtained your answer)'})\n",
    "            q_counter += 1\n",
    "\n",
    "    #Create questions, which include the answer format\n",
    "    \n",
    "    question_for_GPT = [{\"role\": \"user\", \"content\": json.dumps(questions_json) + ' \\n Respond in the following JSON form: ' + json.dumps(answers_json)}]\n",
    "    \n",
    "    #Create messages in one prompt for GPT\n",
    "    if 'Language choice' in metadata_json.keys():\n",
    "        language_content = f\"The file is written in {metadata_json['Language choice']}.\"\n",
    "    else:\n",
    "        language_content = ''\n",
    "\n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction + language_content}] \n",
    "\n",
    "    messages_for_GPT = intro_for_GPT + file_for_GPT + question_for_GPT\n",
    "    \n",
    "#   return messages_for_GPT\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    try:\n",
    "        #completion = client.chat.completions.create(\n",
    "        completion = openai.chat.completions.create(\n",
    "            model=gpt_model,\n",
    "            messages=messages_for_GPT, \n",
    "            response_format={\"type\": \"json_object\"}, \n",
    "            temperature = 0.2, \n",
    "            top_p = 0.2\n",
    "        )\n",
    "        \n",
    "#        return completion.choices[0].message.content #This gives answers as a string containing a dictionary\n",
    "                                \n",
    "        answers_dict = json.loads(completion.choices[0].message.content)\n",
    "        \n",
    "        #Obtain tokens\n",
    "        output_tokens = completion.usage.completion_tokens\n",
    "        \n",
    "        prompt_tokens = completion.usage.prompt_tokens\n",
    "        \n",
    "        #return [answers_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "        #Check answers\n",
    "\n",
    "        if check_questions_answers() > 0:\n",
    "            \n",
    "            try:\n",
    "                redacted_output = GPT_answers_check(answers_dict, gpt_model, answers_check_system_instruction)\n",
    "        \n",
    "                redacted_answers_dict = redacted_output[0]\n",
    "        \n",
    "                redacted_answers_output_tokens = redacted_output[1]\n",
    "        \n",
    "                redacted_answers_prompt_tokens = redacted_output[2]\n",
    "        \n",
    "                return [redacted_answers_dict, output_tokens + redacted_answers_output_tokens, prompt_tokens + redacted_answers_prompt_tokens]\n",
    "\n",
    "                print('Answers checked.')\n",
    "                \n",
    "            except Exception as e:\n",
    "    \n",
    "                print('Answers check failed.')\n",
    "    \n",
    "                print(e)\n",
    "    \n",
    "                return [answers_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "        else:\n",
    "\n",
    "            print('Answers not checked.')\n",
    "            \n",
    "            return [answers_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "    except Exception as error:\n",
    "        \n",
    "        print('GPT failed to produce answers.')\n",
    "        \n",
    "        for q_index in q_keys:\n",
    "            \n",
    "            answers_json.update({q_index: error})\n",
    "        \n",
    "        return [answers_json, 0, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27256618-a023-4426-974a-8abfc98edaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT function for each respondent's dataframe, index by judgment then question, with input and output tokens given by GPT itself\n",
    "#For gpt-4o vision\n",
    "\n",
    "#The following function DOES NOT check for existence of questions for GPT\n",
    "    # To so check, active line marked as #*\n",
    "\n",
    "@st.cache_data(show_spinner = False)\n",
    "def engage_GPT_b64_json(questions_json, df_example, df_individual, GPT_activation, gpt_model, system_instruction):\n",
    "    # Variable questions_json refers to the json of questions\n",
    "    # Variable df_individual refers to each respondent's df\n",
    "    # Variable activation refers to status of GPT activation (real or test)\n",
    "    # The output is a new JSON for the relevant respondent with new columns re:\n",
    "        # f\"Length of first 10 pages in tokens (up to {tokens_cap(gpt_model)} given to GPT)\"\n",
    "        # 'GPT cost estimate (USD excl GST)'\n",
    "        # 'GPT time estimate (seconds)'\n",
    "        # GPT questions/answers\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "\n",
    "    #Check questions for privacy violation\n",
    "    if check_questions_answers() > 0:\n",
    "    \n",
    "        questions_checked_dict = GPT_questions_check(questions_json, gpt_model, questions_check_system_instruction)\n",
    "\n",
    "        questions_json = questions_checked_dict['questions_json']\n",
    "    \n",
    "        questions_check_output_tokens = questions_checked_dict['questions_check_output_tokens']\n",
    "    \n",
    "        questions_check_input_tokens = questions_checked_dict['questions_check_input_tokens']\n",
    "\n",
    "    else:\n",
    "\n",
    "        print('Questions not checked.')\n",
    "        \n",
    "        questions_check_output_tokens = 0\n",
    "        \n",
    "        questions_check_input_tokens = 0\n",
    "\n",
    "    #Process questions\n",
    "\n",
    "    #GPT use counter\n",
    "    gpt_use_counter = 0\n",
    "    \n",
    "    for judgment_index in df_individual.index:\n",
    "        \n",
    "        judgment_json = df_individual.to_dict('index')[judgment_index]\n",
    "\n",
    "        #Check wither error in getting the full text\n",
    "        text_error = False\n",
    "        for key in ['judgment_b64', 'b64_list']:#, 'extracted_text']:\n",
    "            if key in judgment_json.keys():\n",
    "                if len(judgment_json[key]) == 0:\n",
    "                    text_error = True\n",
    "                    df_individual.loc[judgment_index, 'Note'] = search_error_note\n",
    "                    print(f\"Case/file indexed {judgment_index} not sent to GPT given full text was not scrapped.\")\n",
    "                break\n",
    "\n",
    "        #Calculate and append number of tokens of judgment, regardless of whether given to GPT\n",
    "        df_individual.loc[judgment_index, f\"Tokens (up to {tokens_cap(gpt_model)} given to GPT)\"] = judgment_json['tokens_raw']       \n",
    "\n",
    "        #Create columns for respondent's GPT cost, time\n",
    "        df_individual.loc[judgment_index, 'GPT cost estimate (USD excl GST)'] = ''\n",
    "        df_individual.loc[judgment_index, 'GPT time estimate (seconds)'] = ''\n",
    "                \n",
    "        #Calculate GPT start time\n",
    "\n",
    "        GPT_start_time = datetime.now()\n",
    "\n",
    "        #Depending on activation status, apply GPT_json function to each judgment, gives answers as a string containing a dictionary\n",
    "\n",
    "        if ((int(GPT_activation) > 0) and (text_error == False)):\n",
    "            GPT_output_list = GPT_b64_json(questions_json, df_example, judgment_json, gpt_model, system_instruction) #Gives [answers as a JSON, output tokens, input tokens]\n",
    "            answers_dict = GPT_output_list[0]\n",
    "\n",
    "            if check_questions_answers() > 0:\n",
    "            \n",
    "                GPT_answers_check_output_list = GPT_answers_check(answers_dict, gpt_model, answers_check_system_instruction)\n",
    "\n",
    "                #Get potentially redacted answers and costs\n",
    "                answers_dict = GPT_answers_check_output_list[0]\n",
    "                redacted_answers_output_tokens = GPT_answers_check_output_list[1]\n",
    "                redacted_answers_prompt_tokens = GPT_answers_check_output_list[2]\n",
    "\n",
    "            else:\n",
    "                print('Answers not checked.')\n",
    "                redacted_answers_output_tokens = 0\n",
    "                redacted_answers_prompt_tokens = 0\n",
    "\n",
    "            #Calculate GPT cost of answering questions\n",
    "            answers_output_tokens = GPT_output_list[1] + redacted_answers_output_tokens\n",
    "            answers_input_tokens  = GPT_output_list[2] + redacted_answers_output_tokens\n",
    "                    \n",
    "            #Calculate and append GPT finish time and time difference to individual df\n",
    "            GPT_finish_time = datetime.now()\n",
    "            \n",
    "            GPT_time_difference = GPT_finish_time - GPT_start_time\n",
    "    \n",
    "            df_individual.loc[judgment_index, 'GPT time estimate (seconds)'] = GPT_time_difference.total_seconds()    \n",
    "\n",
    "            #Display GPT use counter\n",
    "            gpt_use_counter += 1\n",
    "            print(f\"GPT proccessed {gpt_use_counter}/{len(df_individual)} cases.\")\n",
    "                    \n",
    "        else:\n",
    "            answers_dict = {}    \n",
    "            \n",
    "            question_keys = [*questions_json]\n",
    "\n",
    "            for q_index in question_keys:\n",
    "                #Increases judgment index by 2 to ensure consistency with Excel spreadsheet\n",
    "                answer = ''\n",
    "                answers_dict.update({questions_json[q_index]: answer})\n",
    "            \n",
    "            #Calculate capped judgment tokens\n",
    "\n",
    "            judgment_capped_tokens = min(judgment_json['tokens_raw'], tokens_cap(gpt_model))\n",
    "\n",
    "            #Calculate questions tokens and cost\n",
    "\n",
    "            questions_tokens = num_tokens_from_string(json.dumps(questions_json), \"cl100k_base\")\n",
    "\n",
    "            #Calculate metadata tokens\n",
    "\n",
    "            metadata_tokens = 0\n",
    "            \n",
    "            metadata_json_for_counting = judgment_json\n",
    "\n",
    "            for key in ['tokens_raw', 'judgment_b64', 'b64_list']: #['Hyperlink to CommonLII', 'judgment', 'tokens_raw']:\n",
    "                if key in metadata_json_for_counting.keys():\n",
    "                    metadata_json_for_counting.pop(key)\n",
    "                #except:\n",
    "                    #print(f'Unable to remove {key} from metadata_json_for_counting')        \n",
    "\n",
    "            #if 'judgment' not in metadata_json_for_counting.keys():\n",
    "            metadata_tokens = metadata_tokens + num_tokens_from_string(str(metadata_json_for_counting), \"cl100k_base\")\n",
    "\n",
    "            #Calculate other instructions' tokens\n",
    "\n",
    "            other_instructions = system_instruction + 'you will be given questions to answer in JSON form.' + ' \\n Respond in the following JSON form: '\n",
    "\n",
    "            other_tokens = num_tokens_from_string(other_instructions, \"cl100k_base\") + len(question_keys)*num_tokens_from_string(\"GPT question x:  Your answer to the question with index GPT question x. State specific page numbers or sections of the judgment.\", \"cl100k_base\")\n",
    "\n",
    "            #Calculate number of tokens of answers\n",
    "            answers_output_tokens = num_tokens_from_string(str(answers_dict), \"cl100k_base\")\n",
    "\n",
    "            answers_input_tokens = judgment_capped_tokens + questions_tokens + metadata_tokens + other_tokens\n",
    "            \n",
    "        #Create GPT question headings and append answers to individual spreadsheets\n",
    "        for answer_index in answers_dict.keys():\n",
    "\n",
    "            #Check any errors\n",
    "            answer_string = str(answers_dict[answer_index]).lower()\n",
    "            \n",
    "            if ((answer_string.startswith('your answer.')) or (answer_string.startswith('your response.'))):\n",
    "                \n",
    "                answers_dict[answer_index] = 'Error. Please try a different question or GPT model.'\n",
    "\n",
    "            #Append answer to spreadsheet\n",
    "\n",
    "            answer_header = answer_index\n",
    "\n",
    "            try:\n",
    "            \n",
    "                df_individual.loc[judgment_index, answer_header] = answers_dict[answer_index]\n",
    "\n",
    "            except:\n",
    "\n",
    "                df_individual.loc[judgment_index, answer_header] = str(answers_dict[answer_index])\n",
    "                \n",
    "        #Calculate GPT costs\n",
    "\n",
    "        #If check for questions\n",
    "        GPT_cost = (answers_output_tokens + questions_check_output_tokens/len(df_individual))*gpt_output_cost(gpt_model) + (answers_input_tokens + questions_check_input_tokens/len(df_individual))*gpt_input_cost(gpt_model)\n",
    "\n",
    "        #If no check for questions\n",
    "        #GPT_cost = answers_output_tokens*gpt_output_cost(gpt_model) + answers_input_tokens*gpt_input_cost(gpt_model)\n",
    "\n",
    "        #Calculate and append GPT cost to individual df\n",
    "        df_individual.loc[judgment_index, 'GPT cost estimate (USD excl GST)'] = GPT_cost\n",
    "    \n",
    "    return df_individual\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf9623-73d7-41a4-9ac1-54a8256696bf",
   "metadata": {},
   "source": [
    "## Batch mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d62fa1-6e97-4cc4-9eac-ad9609c8b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cutoff for requiring activate batch mode\n",
    "\n",
    "judgment_batch_cutoff = 25\n",
    "\n",
    "#max number of judgments under batch mode\n",
    "judgment_batch_max = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c8f34e5-ec14-4b6e-81c2-625c76779d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create custom id for one judgment_json file\n",
    "\n",
    "#custom_id should be mnc plus time now\n",
    "\n",
    "def gpt_get_custom_id(judgment_json):\n",
    "    \n",
    "    #Returns time now by default\n",
    "    time_now = str(datetime.now()).replace(' ', '_').replace(':', '_').replace('.', '_')[8:]\n",
    "\n",
    "    mnc = ''\n",
    "\n",
    "    if 'Medium neutral citation' in judgment_json.keys():\n",
    "        mnc = judgment_json['Medium neutral citation'].replace(' ', '_')\n",
    "    \n",
    "    elif 'mnc' in judgment_json.keys():\n",
    "\n",
    "        mnc = judgment_json['mnc'].replace(' ', '_')\n",
    "\n",
    "    else:\n",
    "        mnc = ''\n",
    "\n",
    "    case_name = ''\n",
    "\n",
    "    if 'Case name' in judgment_json.keys():\n",
    "        case_name = judgment_json['Case name'].replace(' ', '_')\n",
    "    \n",
    "    elif 'title' in judgment_json.keys():\n",
    "\n",
    "        case_name = judgment_json['title'].replace(' ', '_')\n",
    "    \n",
    "    elif 'File name' in judgment_json.keys():\n",
    "    \n",
    "        case_name = judgment_json['File name'].replace(' ', '_')\n",
    "    \n",
    "    else:\n",
    "        case_name = ''\n",
    "    \n",
    "    custom_id = f\"{time_now}_{case_name}_{mnc}\"[0:50]\n",
    "\n",
    "    return custom_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55f1ffa3-f79b-49ec-9704-c55b95e357ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function for creating custom id and one line of jsonl file for batching\n",
    "#Returns a dictionary of custom id and one line\n",
    "\n",
    "def gpt_batch_input_id_line(questions_json, df_example, judgment_json, gpt_model, system_instruction):\n",
    "    #'question_json' variable is a json of questions to GPT\n",
    "    #'jugdment' variable is a judgment_json   \n",
    "\n",
    "    #Loop through non-b64 keys\n",
    "    for key in ['judgment', 'opinions', 'recap_documents', 'extracted_text']:\n",
    "        if key in judgment_json.keys():      \n",
    "            judgment_for_GPT = [{\"role\": \"user\", \"content\": judgment_prompt_json(judgment_json, gpt_model)}]\n",
    "            \n",
    "            break\n",
    "            \n",
    "    else: #If one of b64 key ['judgment_b64', 'b64_list'] in judgment_json.keys():\n",
    "        \n",
    "        #Add images to messages to GPT\n",
    "        image_content_value = [{\"type\": \"text\", \"text\": 'Based on the following images:'}]\n",
    "        \n",
    "        for key in ['judgment', 'b64_list']:\n",
    "            if key in judgment_json.keys():\n",
    "                for image_b64 in judgment_json[key]:\n",
    "                    image_message_to_attach = {\"type\": \"image_url\", \"image_url\": {\"url\": image_b64,}}\n",
    "                    image_content_value.append(image_message_to_attach)\n",
    "                    \n",
    "                break\n",
    "        \n",
    "        image_content = [{\"role\": \"user\", \n",
    "                          \"content\": image_content_value\n",
    "                         }\n",
    "                      ]\n",
    "\n",
    "        judgment_for_GPT = image_content\n",
    "\n",
    "    json_direction = [{\"role\": \"user\", \"content\": 'You will be given questions to answer in JSON form.'}]\n",
    "\n",
    "    #Create answer format\n",
    "    answers_json = {}\n",
    "\n",
    "    if len(df_example.replace('\"', '')) > 0:\n",
    "\n",
    "        try:\n",
    "            \n",
    "            if isinstance(df_example, str):\n",
    "                \n",
    "                answers_json = json.loads(df_example)\n",
    "\n",
    "            if isinstance(df_example, dict):\n",
    "                \n",
    "                answers_json = df_example\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Example provided but can't produce json to send to GPT.\")\n",
    "            print(e)\n",
    "    \n",
    "    #Check if answers format succesfully created by following any example uploaded\n",
    "    q_keys = [*questions_json]\n",
    "    \n",
    "    if len(answers_json) == 0:\n",
    "        q_counter = 1\n",
    "        for q_index in q_keys:\n",
    "            answers_json.update({f'GPT question {q_counter}: {questions_json[q_index]}': f'Your answer. (The paragraphs, pages or sections from which you obtained your answer)'})\n",
    "            q_counter += 1\n",
    "            \n",
    "    #Create questions, which include the answer format\n",
    "    \n",
    "    question_for_GPT = [{\"role\": \"user\", \"content\": json.dumps(questions_json, default = str) + ' \\n Respond in the following JSON form: ' + json.dumps(answers_json, default = str)}]\n",
    "    \n",
    "    #Create messages in one prompt for GPT\n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction}]\n",
    "    messages_for_GPT = intro_for_GPT + judgment_for_GPT + json_direction + question_for_GPT\n",
    "\n",
    "    #Create one line in batch input\n",
    "    #Format for one line in batch input file is\n",
    "    #{\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-3.5-turbo-0125\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},{\"role\": \"user\", \"content\": \"Hello world!\"}],\"max_tokens\": 1000}}\n",
    "\n",
    "    body = {\"model\": gpt_model, \n",
    "            \"messages\": messages_for_GPT, \n",
    "            \"response_format\": {\"type\": \"json_object\"}, \n",
    "            \"max_tokens\": max_output(gpt_model, messages_for_GPT), \n",
    "            \"temperature\": 0.1, \n",
    "            #\"top_p\" = 0.1\n",
    "           }\n",
    "\n",
    "    custom_id = gpt_get_custom_id(judgment_json)\n",
    "    \n",
    "    oneline = {\"custom_id\": custom_id, \n",
    "              \"method\": \"POST\", \n",
    "              \"url\": \"/v1/chat/completions\", \n",
    "              \"body\": body\n",
    "             }\n",
    "\n",
    "    return {\"custom_id\": custom_id, \"oneline\": oneline}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13d194e1-4edf-4de4-9a32-7f521de5a3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 08:49:47.095 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "#Define function for creating jsonl file for batching together with df_individual with custom id inserted\n",
    "\n",
    "def gpt_batch_input(questions_json, df_example, df_individual, GPT_activation, gpt_model, system_instruction):\n",
    "    # Variable questions_json refers to the json of questions\n",
    "    # Variable df_individual refers to each respondent's df\n",
    "    # Variable activation refers to status of GPT activation (real or test)\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "\n",
    "    #Create list for conversion to jsonl\n",
    "\n",
    "    batch_input_list = []\n",
    "    \n",
    "    #Process questions\n",
    "    \n",
    "    for judgment_index in df_individual.index:\n",
    "        \n",
    "        judgment_json = df_individual.to_dict('index')[judgment_index]\n",
    "\n",
    "        #Check wither error in getting the full text\n",
    "        text_error = False\n",
    "        for text_key in ['judgment', 'opinions', 'recap_documents', 'extracted_text', 'judgment_b64', 'b64_list']:\n",
    "            if text_key in judgment_json.keys():\n",
    "\n",
    "                #Checking if judgment_json[text_key] is np.nan\n",
    "                if isinstance(judgment_json[text_key], float):\n",
    "                \n",
    "                    judgment_json[text_key] = ''\n",
    "                \n",
    "                if len(judgment_json[text_key]) == 0:\n",
    "                    text_error = True\n",
    "                    df_individual.loc[judgment_index, 'Note'] = search_error_note\n",
    "                    print(f\"Case/file indexed {judgment_index} not sent to GPT given full text was not scrapped.\")\n",
    "                    \n",
    "                break\n",
    "        \n",
    "        #Calculate and append number of tokens of judgment, regardless of whether given to GPT\n",
    "        judgment_tokens = num_tokens_from_string(str(judgment_json), \"cl100k_base\")\n",
    "        df_individual.loc[judgment_index, f\"File length in tokens (up to {tokens_cap(gpt_model)} given to GPT)\"] = judgment_tokens       \n",
    "\n",
    "        #Indicate whether judgment truncated\n",
    "                \n",
    "        if judgment_tokens > tokens_cap(gpt_model):\n",
    "            df_individual.loc[judgment_index, 'Note'] = truncation_note\n",
    "\n",
    "        #Create columns for respondent's GPT cost\n",
    "        df_individual.loc[judgment_index, 'GPT cost estimate (USD excl GST)'] = ''\n",
    "                \n",
    "        #Calculate GPT start time\n",
    "\n",
    "        GPT_start_time = datetime.now()\n",
    "\n",
    "        #Depending on activation status, apply GPT_json function to each judgment, gives answers as a string containing a dictionary\n",
    "        if ((int(GPT_activation) > 0) and (text_error == False)):\n",
    "\n",
    "            get_id_oneline = gpt_batch_input_id_line(questions_json, df_example, judgment_json, gpt_model, system_instruction)\n",
    "            \n",
    "            df_individual.loc[judgment_index, 'custom_id'] = get_id_oneline['custom_id']\n",
    "\n",
    "            batch_input_list.append(get_id_oneline['oneline'])\n",
    "\n",
    "            #Remove full text/b64\n",
    "            for text_key in ['judgment', 'opinions', 'recap_documents', 'extracted_text', 'judgment_b64', 'b64_list']:\n",
    "                if text_key in df_individual.columns:\n",
    "                    df_individual.loc[judgment_index, text_key] = ''\n",
    "                    break\n",
    "            \n",
    "            df_individual.loc[judgment_index, 'GPT submission time'] = str(GPT_start_time)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            print(f'Case {judgment_index}: GPT not activated.')\n",
    "\n",
    "    #Convert batch_input_list to jsonl\n",
    "    #The following steps are based on\n",
    "    #https://stackoverflow.com/questions/51775175/pandas-dataframe-to-jsonl-json-lines-conversion\n",
    "    #https://github.com/openai/openai-python/tree/main#file-uploads\n",
    "        #Replace 'client.' with 'openai.'\n",
    "        #Need to convert jsonl_for_batching to bytes mode, see https://www.datacamp.com/tutorial/string-to-bytes-conversion\n",
    "\n",
    "    df_jsonl = pd.DataFrame(batch_input_list)\n",
    "\n",
    "    jsonl_for_batching = df_jsonl.to_json(orient='records', lines=True)\n",
    "\n",
    "    #print(f\"jsonl_for_batching == {jsonl_for_batching}\")\n",
    "    \n",
    "    batch_input_file = openai.files.create(\n",
    "        file = jsonl_for_batching.encode(encoding=\"utf-8\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    batch_input_file_id = batch_input_file.id\n",
    "    \n",
    "    batch_record = openai.batches.create(\n",
    "        input_file_id=batch_input_file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\", \n",
    "            #metadata={\n",
    "      #\"name\":\n",
    "        #\"email\":\n",
    "    #}\n",
    "    )\n",
    "    \n",
    "    return {'batch_record': batch_record, 'df_individual': df_individual}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a86e5317-82ab-4afd-835b-c39932c9906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch function\n",
    "\n",
    "@st.dialog(\"Requesting data\")\n",
    "def batch_request_function():\n",
    "\n",
    "    if int(st.session_state.df_master.loc[0, 'Consent']) == 0:\n",
    "        st.warning(\"You must tick 'Yes, I agree.' to use the app.\")\n",
    "\n",
    "    elif len(st.session_state.df_individual)>0:\n",
    "        st.warning('You must :red[REMOVE] the data already produced before producing new data.')\n",
    "\n",
    "    elif st.session_state['df_master'].loc[0, 'Use GPT'] == False:\n",
    "        st.error(\"You must tick 'Use GPT'.\")\n",
    "        \n",
    "    else:\n",
    " \n",
    "        if ((st.session_state.own_account == True) and (st.session_state['df_master'].loc[0, 'Use GPT'] == True)):\n",
    "                                \n",
    "            if is_api_key_valid(st.session_state.df_master.loc[0, 'Your GPT API key']) == False:\n",
    "                st.error('Your API key is not valid.')\n",
    "                \n",
    "                st.session_state[\"batch_ready_for_submission\"] = False\n",
    "\n",
    "                st.stop()\n",
    "\n",
    "            else:\n",
    "                \n",
    "                st.session_state[\"batch_ready_for_submission\"] = True\n",
    "\n",
    "        if st.session_state.jurisdiction_page == 'pages/US.py':\n",
    "            \n",
    "            if len(str(st.session_state.df_master.loc[0, 'CourtListener API token'])) < 20:\n",
    "\n",
    "                st.session_state[\"batch_ready_for_submission\"] = False\n",
    "\n",
    "                st.write('Please enter a valid CourtListener API token. You can sign up for one [here](https://www.courtlistener.com/sign-in/).')\n",
    "\n",
    "                batch_token_entry = st.text_input(label = 'your CourtListener API token (mandatory)', value = st.session_state['df_master'].loc[0, 'CourtListener API token'])\n",
    "\n",
    "                if st.button(label = 'CONFIRM your CourtListener API token', disabled = bool(st.session_state.batch_submitted)):\n",
    "                    \n",
    "                    st.session_state['df_master'].loc[0, 'CourtListener API token'] = batch_token_entry\n",
    "\n",
    "                    if len(str(st.session_state.df_master.loc[0, 'CourtListener API token'])) < 20:\n",
    "                \n",
    "                        st.error('You must enter a valid CourtListener API token.')\n",
    "                        st.stop()\n",
    "                    else:\n",
    "                        st.session_state[\"batch_ready_for_submission\"] = True\n",
    "        \n",
    "        #Check if valid email address entered\n",
    "        if '@' not in st.session_state['df_master'].loc[0, 'Your email address']:\n",
    "            \n",
    "            st.session_state[\"batch_ready_for_submission\"] = False\n",
    "\n",
    "            st.write('Please enter a valid email address to receive your request data.')\n",
    "            \n",
    "            batch_email_entry = st.text_input(label = \"Your email address (mandatory)\", value =  st.session_state['df_master'].loc[0, 'Your email address'])\n",
    "\n",
    "            if st.button(label = 'CONFIRM your email address', disabled = bool(st.session_state.batch_submitted)):\n",
    "                \n",
    "                st.session_state['df_master'].loc[0, 'Your email address'] = batch_email_entry\n",
    "    \n",
    "                if '@' not in st.session_state['df_master'].loc[0, 'Your email address']:\n",
    "                \n",
    "                    st.error('You must enter a valid email address to receive your request data.')\n",
    "                    st.stop()\n",
    "                else:\n",
    "                    st.session_state[\"batch_ready_for_submission\"] = True\n",
    "\n",
    "        if st.session_state[\"batch_ready_for_submission\"] == True:\n",
    "        \n",
    "            with st.spinner(spinner_text):\n",
    "                \n",
    "                try:\n",
    "    \n",
    "                    #Create spreadsheet of responses\n",
    "                    df_master = st.session_state.df_master\n",
    "\n",
    "                    jurisdiction_page = st.session_state.jurisdiction_page\n",
    "    \n",
    "                    df_master['jurisdiction_page'] = jurisdiction_page\n",
    "                    \n",
    "                    df_master['status'] = 'to_process'\n",
    "    \n",
    "                    df_master['submission_time'] = str(datetime.now())\n",
    "\n",
    "                    #Activate user's own key or mine\n",
    "                    if st.session_state.own_account == True:\n",
    "                        \n",
    "                        API_key = st.session_state.df_master.loc[0, 'Your GPT API key']\n",
    "        \n",
    "                    else:\n",
    "                        \n",
    "                        API_key = st.secrets[\"openai\"][\"gpt_api_key\"]\n",
    "                        \n",
    "                        st.session_state['df_master'].loc[0, 'Maximum number of judgments'] = st.session_state[\"judgment_counter_max\"]\n",
    "\n",
    "                    #Check questions for potential privacy violation\n",
    "                    openai.api_key = API_key\n",
    "\n",
    "                    if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "                        gpt_model = \"gpt-4o\"\n",
    "                    else:        \n",
    "                        gpt_model = \"gpt-4o-mini\"\n",
    "\n",
    "                    questions_checked_dict = GPT_questions_check(df_master.loc[0, 'Enter your questions for GPT'], gpt_model, questions_check_system_instruction)\n",
    "\n",
    "                    #Use checked questions\n",
    "                    df_master.loc[0, 'Enter your questions for GPT'] = questions_checked_dict['questions_string']\n",
    "                    \n",
    "                    #Initiate aws s3\n",
    "                    s3_resource = boto3.resource('s3',region_name=st.secrets[\"aws\"][\"AWS_DEFAULT_REGION\"], aws_access_key_id=st.secrets[\"aws\"][\"AWS_ACCESS_KEY_ID\"], aws_secret_access_key=st.secrets[\"aws\"][\"AWS_SECRET_ACCESS_KEY\"])\n",
    "                    \n",
    "                    #Get a list of all files on s3\n",
    "                    bucket = s3_resource.Bucket('lawtodata')\n",
    "    \n",
    "                    #Get all_df_masters\n",
    "                    for obj in bucket.objects.all():\n",
    "                        key = obj.key\n",
    "                        if key == 'all_df_masters.csv':\n",
    "                            body = obj.get()['Body'].read()\n",
    "                            all_df_masters = pd.read_csv(BytesIO(body), index_col=0)\n",
    "                            break\n",
    "                            \n",
    "                    #Add df_master to all_df_masters \n",
    "                    all_df_masters = pd.concat([all_df_masters, df_master], ignore_index=True)\n",
    "    \n",
    "                    #Upload all_df_masters to aws\n",
    "                    csv_buffer = StringIO()\n",
    "                    all_df_masters.to_csv(csv_buffer)\n",
    "                    s3_resource = boto3.resource('s3',region_name=st.secrets[\"aws\"][\"AWS_DEFAULT_REGION\"], aws_access_key_id=st.secrets[\"aws\"][\"AWS_ACCESS_KEY_ID\"], aws_secret_access_key=st.secrets[\"aws\"][\"AWS_SECRET_ACCESS_KEY\"])\n",
    "                    s3_resource.Object('lawtodata', 'all_df_masters.csv').put(Body=csv_buffer.getvalue())\n",
    "                                           \n",
    "                    #Send me an email to let me know\n",
    "                    send_notification_email(ULTIMATE_RECIPIENT_NAME = st.session_state['df_master'].loc[0, 'Your name'], \n",
    "                                            ULTIMATE_RECIPIENT_EMAIL = st.session_state['df_master'].loc[0, 'Your email address'], \n",
    "                                            jurisdiction_page = st.session_state['df_master'].loc[0, 'jurisdiction_page']\n",
    "                                           )\n",
    "\n",
    "                    st.session_state[\"batch_submitted\"] = True\n",
    "                                        \n",
    "                    st.rerun()\n",
    "                \n",
    "                except Exception as e:\n",
    "\n",
    "                    st.error('Sorry, an error has occurred. Please change your questions or wait a few hours, and try again.')\n",
    "                    \n",
    "                    st.error(e)\n",
    "                    \n",
    "                    st.error(traceback.format_exc())\n",
    "    \n",
    "                    print(e)\n",
    "    \n",
    "                    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52570fba-249a-4255-8012-cfc4a5121826",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521ebf83-c7bc-4cc2-8956-f30111aedfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jurisdiction specific instruction and functions\n",
    "\n",
    "def gpt_run(jurisdiction_page, df_master):\n",
    "\n",
    "    if jurisdiction_page == 'pages/HCA.py':\n",
    "        \n",
    "        system_instruction = role_content\n",
    "        \n",
    "        from functions.hca_functions import hca_run#, hca_collections, hca_search, hca_pdf_judgment, hca_meta_labels_droppable, hca_meta_judgment_dict, hca_meta_judgment_dict_alt, hca_mnc_to_link_browse, hca_citation_to_link, hca_mnc_to_link, hca_load_data, hca_data_url, hca_df, hca_judgment_to_exclude, hca_search_results_to_judgment_links_filtered_df, hca_year_range, hca_judge_list, hca_party_list, hca_terms_to_add, hca_enhanced_search  \n",
    "        #hca_search_results_to_judgment_links\n",
    "        \n",
    "        run = copy.copy(hca_run)\n",
    "\n",
    "    if jurisdiction_page == 'pages/NSW.py':\n",
    "        \n",
    "        system_instruction = role_content\n",
    "\n",
    "        from nswcaselaw.search import Search\n",
    "        \n",
    "        from functions.nsw_functions import nsw_run#, nsw_search, nsw_meta_labels_droppable, nsw_courts, nsw_courts_positioning, nsw_default_courts, nsw_tribunals, nsw_tribunals_positioning, nsw_court_choice, nsw_tribunal_choice, nsw_date, nsw_link, nsw_short_judgment, nsw_tidying_up, nsw_tidying_up_pre_gpt\n",
    "    \n",
    "        run = copy.copy(nsw_run)\n",
    "    \n",
    "    if jurisdiction_page == 'pages/FCA.py':\n",
    "        \n",
    "        system_instruction = role_content\n",
    "        \n",
    "        from functions.fca_functions import fca_run#, fca_courts, fca_courts_list, fca_search, fca_search_url, fca_search_results_to_judgment_links, fca_metalabels, fca_metalabels_droppable, fca_meta_judgment_dict, fca_pdf_name_mnc_list, fca_pdf_name\n",
    "        #fca_link_to_doc\n",
    "        \n",
    "        run = copy.copy(fca_run)\n",
    "\n",
    "    if jurisdiction_page == 'pages/US.py':\n",
    "        \n",
    "        system_instruction = role_content\n",
    "        \n",
    "        from functions.us_functions import us_run#, us_search_function, us_court_choice_clean, us_order_by, us_pacer_order_by, us_precedential_status, us_fed_app_courts, us_fed_dist_courts, us_fed_hist_courts, us_bankr_courts, us_state_courts, us_more_courts, all_us_jurisdictions, us_date, us_collections, us_pacer_fed_app_courts, us_pacer_fed_dist_courts, us_pacer_bankr_courts, us_pacer_more_courts, all_us_pacer_jurisdictions, us_court_choice_clean_pacer\n",
    "        #us_court_choice_to_list\n",
    "        \n",
    "        run = copy.copy(us_run)\n",
    "\n",
    "    if jurisdiction_page == 'pages/CA.py':\n",
    "        \n",
    "        system_instruction = role_content\n",
    "        \n",
    "        from functions.ca_functions import ca_run#, all_ca_jurisdictions, ca_courts, bc_courts, ab_courts, sk_courts, mb_courts, on_courts, qc_courts, nb_courts, ns_courts, pe_courts, nl_courts, yk_courts, nt_courts, nu_courts, all_ca_jurisdiction_court_pairs, ca_court_tribunal_types, all_subjects, ca_search, ca_search_url, ca_search_results_to_judgment_links, ca_meta_labels_droppable, ca_meta_dict, ca_date, ca_meta_judgment_dict\n",
    "        \n",
    "        run = copy.copy(ca_run)\n",
    "\n",
    "    if jurisdiction_page == 'pages/UK.py':\n",
    "        \n",
    "        system_instruction = role_content\n",
    "        \n",
    "        from functions.uk_functions import uk_run#, uk_courts_default_list, uk_courts, uk_courts_list, uk_court_choice, uk_link, uk_search, uk_search_results_to_judgment_links, uk_meta_labels_droppable, uk_meta_judgment_dict\n",
    "        \n",
    "        run = copy.copy(uk_run)\n",
    "\n",
    "    if jurisdiction_page == 'pages/AFCA.py':\n",
    "\n",
    "        system_instruction = role_content\n",
    "                \n",
    "        from functions.afca_functions import afca_run#, afca_old_run, afca_new_run, product_line_options, product_category_options, product_name_options, issue_type_options, issue_options, afca_search, afca_meta_judgment_dict,  afca_meta_labels_droppable, afca_old_pdf_judgment, afca_old_element_meta, afca_old_search, afca_old_meta_labels_droppable, afca_meta_labels_droppable, streamlit_timezone\n",
    "                \n",
    "        if streamlit_timezone() == True:\n",
    "\n",
    "            st.warning('One or more Chrome window may be launched. It must be kept open.')\n",
    "\n",
    "        run = copy.copy(afca_run)\n",
    "\n",
    "    if jurisdiction_page == 'pages/ER.py':\n",
    "\n",
    "        from functions.er_functions import er_run, role_content_er#, er_run_b64, er_methods_list, er_method_types, er_search, er_search_results_to_case_link_pairs, er_judgment_text, er_meta_judgment_dict, er_judgment_tokens_b64, er_meta_judgment_dict_b64\n",
    "\n",
    "        system_instruction = role_content_er\n",
    "\n",
    "        run = copy.copy(er_run)\n",
    "\n",
    "    if jurisdiction_page == 'pages/KR.py':\n",
    "\n",
    "        system_instruction = role_content\n",
    "                \n",
    "        from functions.kr_functions import kr_run#, kr_methods_list, kr_method_types, kr_search, kr_search_results_to_case_link_pairs, kr_judgment_text, kr_meta_judgment_dict\n",
    "        \n",
    "        run = copy.copy(kr_run)\n",
    "\n",
    "    if jurisdiction_page == 'pages/SCTA.py':\n",
    "\n",
    "        system_instruction = role_content\n",
    "        \n",
    "        from functions.scta_functions import scta_run#, scta_methods_list, scta_method_types, scta_search, scta_search_results_to_case_link_pairs, scta_judgment_text, scta_meta_judgment_dict\n",
    "        \n",
    "        run = copy.copy(scta_run)\n",
    "\n",
    "    if jurisdiction_page == 'pages/UKPO.py':\n",
    "\n",
    "        system_instruction = role_content\n",
    "        \n",
    "        from functions.ukpo_functions import ukpo_run\n",
    "                \n",
    "        run = copy.copy(ukpo_run)\n",
    "    \n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction}]\n",
    "\n",
    "    df_individual = run(df_master)\n",
    "\n",
    "    return df_individual\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e7d5ad-6bc1-4db9-af6f-69ff1674002d",
   "metadata": {},
   "source": [
    "# GPT batch run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf7e161-5cd7-4fcc-b6aa-a175b83b424b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Jurisdiction specific instruction and functions\n",
    "\n",
    "def gpt_batch_input_submit(jurisdiction_page, df_master):\n",
    "\n",
    "    if jurisdiction_page == 'pages/HCA.py':\n",
    "        \n",
    "        system_instruction = role_content\n",
    "        \n",
    "        from functions.hca_functions import hca_batch#, hca_collections, hca_search, hca_pdf_judgment, hca_meta_labels_droppable, hca_meta_judgment_dict, hca_meta_judgment_dict_alt, hca_mnc_to_link_browse, hca_citation_to_link, hca_mnc_to_link, hca_load_data, hca_data_url, hca_df, hca_judgment_to_exclude, hca_search_results_to_judgment_links_filtered_df, hca_year_range, hca_judge_list, hca_party_list, hca_terms_to_add, hca_enhanced_search  \n",
    "        #hca_search_results_to_judgment_links\n",
    "        \n",
    "        batch =  copy.copy(hca_batch)\n",
    "\n",
    "    if jurisdiction_page == 'pages/NSW.py':\n",
    "        \n",
    "        system_instruction = role_content\n",
    "\n",
    "        from nswcaselaw.search import Search\n",
    "\n",
    "        from functions.nsw_functions import nsw_batch#, nsw_search, nsw_tidying_up_pre_gpt, nsw_meta_labels_droppable, nsw_courts, nsw_courts_positioning, nsw_default_courts, nsw_tribunals, nsw_tribunals_positioning, nsw_court_choice, nsw_tribunal_choice, nsw_date, nsw_link, nsw_short_judgment\n",
    "    \n",
    "        batch =  copy.copy(nsw_batch)\n",
    "    \n",
    "    if jurisdiction_page == 'pages/FCA.py':\n",
    "        \n",
    "        system_instruction = role_content\n",
    "        \n",
    "        from functions.fca_functions import fca_batch#, fca_courts, fca_courts_list, fca_search, fca_search_url, fca_search_results_to_judgment_links, fca_metalabels, fca_metalabels_droppable, fca_meta_judgment_dict, fca_pdf_name_mnc_list, fca_pdf_name\n",
    "        #fca_link_to_doc\n",
    "        batch = copy.copy(fca_batch)\n",
    "\n",
    "    if jurisdiction_page == 'pages/US.py':\n",
    "        \n",
    "        system_instruction = role_content\n",
    "        \n",
    "        from functions.us_functions import us_batch#, us_search_function, us_court_choice_clean, us_order_by, us_pacer_order_by, us_precedential_status, us_fed_app_courts, us_fed_dist_courts, us_fed_hist_courts, us_bankr_courts, us_state_courts, us_more_courts, all_us_jurisdictions, us_date, us_collections, us_pacer_fed_app_courts, us_pacer_fed_dist_courts, us_pacer_bankr_courts, us_pacer_more_courts, all_us_pacer_jurisdictions, us_court_choice_clean_pacer\n",
    "            \n",
    "        batch = copy.copy(us_batch)\n",
    "\n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction}]\n",
    "\n",
    "    batch_record_df_individual = batch(df_master)\n",
    "    \n",
    "    return batch_record_df_individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa11a6e-b05d-4e66-9462-e2fcf0f5640c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
