{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b8b879",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2312235",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Preliminary modules\n",
    "import base64 \n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date\n",
    "from dateutil import parser\n",
    "#from dateutil.relativedelta import *\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import pause\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import httplib2\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import urllib.request\n",
    "import pypdf\n",
    "import io\n",
    "from io import BytesIO\n",
    "import pdf2image\n",
    "#from PIL import Image\n",
    "import math\n",
    "from math import ceil\n",
    "import copy\n",
    "\n",
    "#Streamlit\n",
    "import streamlit as st\n",
    "#from streamlit_gsheets import GSheetsConnection\n",
    "from streamlit.components.v1 import html\n",
    "#import streamlit_ext as ste\n",
    "from streamlit_extras.stylable_container import stylable_container\n",
    "\n",
    "#OpenAI\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "#Google\n",
    "#from google.oauth2 import service_account\n",
    "\n",
    "#Excel\n",
    "from pyxlsb import open_workbook as open_xlsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36b77045-bdcc-4ee1-a9a4-b7dd03b6bfd4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface == True\n",
      "Running locally or on Streamlit\n"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from functions.common_functions import own_account_allowed, pop_judgment, convert_df_to_json, convert_df_to_csv, convert_df_to_excel, save_input, date_parser\n",
    "#Import variables\n",
    "from functions.common_functions import today_in_nums, errors_list, scraper_pause_mean, default_judgment_counter_bound, no_results_msg, search_error_note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e819e27",
   "metadata": {},
   "source": [
    "# BAILII search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "151727b0-e03c-4ab1-be18-4fd113d064af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.common_functions import link, pdf_image_judgment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b70edf87-7f5a-4d8e-b464-1db43d3cbf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape javascript\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import ActionChains\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from webdriver_manager.core.os_manager import ChromeType\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait as Wait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument('--no-sandbox')  \n",
    "options.add_argument('--disable-dev-shm-usage')  \n",
    "\n",
    "#@st.cache_resource(show_spinner = False, ttl=600)\n",
    "def get_driver():\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "try:\n",
    "    browser = get_driver()\n",
    "    \n",
    "    #browser.implicitly_wait(5)\n",
    "    #browser.set_page_load_timeout(15)\n",
    "\n",
    "    #browser.quit()\n",
    "    \n",
    "except Exception as e:\n",
    "    st.error('Sorry, your internet connection is not stable enough for this app. Please check or change your internet connection and try again.')\n",
    "    print(e)\n",
    "    quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae66180-5668-492c-a0ad-f82b39cba6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Threshold characters count for getting pdf instead of html\n",
    "bailii_pdf_judgment_threshold = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f2f670-342c-44ab-a87b-58aa2b023b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definitions for search function\n",
    "bailii_methods_list = ['using autosearch', 'this Boolean query', 'any of these words', 'all of these words', 'this phrase', 'this case name']\n",
    "\n",
    "bailii_method_types = ['auto', 'boolean', 'any', 'all', 'phrase', 'title']\n",
    "\n",
    "bailii_sort_dict = {'Relevance': 'rank',\n",
    "                    'Title': 'desc',\n",
    "                    'Jurisdiction': 'juris',\n",
    "                    'Date': 'date', \n",
    "                    'Date (oldest first)': 'fdate', \n",
    "                   }\n",
    "\n",
    "bailii_highlight_dict = {'Yes': '1', 'No': '0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "412afab5-b54a-4a64-9732-088ea0d80a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise default courts\n",
    "bailii_courts_default_list = ['House of Lords', \n",
    "'Supreme Court',\n",
    " 'Privy Council',\n",
    " 'Court of Appeal (Civil Division)',\n",
    " 'Court of Appeal (Criminal Division)',\n",
    " 'High Court Administrative Court',\n",
    " 'High Court Admiralty Court',\n",
    " 'High Court Chancery Division',\n",
    " 'High Court Commercial Court',\n",
    " 'High Court Family Division',\n",
    " 'High Court Intellectual Property Enterprise Court',\n",
    " \"High Court King's/Queen's Bench Division\",\n",
    " 'High Court Mercantile Court',\n",
    " 'High Court Patents Court',\n",
    " 'High Court Senior Courts Costs Office',\n",
    " 'High Court Technology and Construction Court'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f41da1-cc28-413e-aaf5-d13df7d7f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#auxiliary lists and variables\n",
    "bailii_courts = {'House of Lords': 'uk/cases/UKHL',\n",
    " 'Supreme Court': 'uk/cases/UKSC',\n",
    " 'Privy Council': 'uk/cases/UKPC',\n",
    " 'Court of Appeal (Civil Division)': 'ew/cases/EWCA/CIV',\n",
    " 'Court of Appeal (Criminal Division)': 'ew/cases/EWCA/CRIM',\n",
    " 'High Court Administrative Court': 'ew/cases/EWHC/ADMIN',\n",
    " 'High Court Admiralty Court': 'ew/cases/EWHC/ADMLTY',\n",
    " 'High Court Chancery Division': 'ew/cases/EWHC/CH',\n",
    " 'High Court Commercial Court': 'ew/cases/EWHC/COMM',\n",
    " 'High Court Family Division': 'ew/cases/EWHC/FAM',\n",
    " 'High Court Intellectual Property Enterprise Court': 'ew/cases/EWHC/IPEC',\n",
    " \"High Court King's/Queen's Bench Division\": 'ew/cases/EWHC/KB',\n",
    " 'High Court Mercantile Court': 'ew/cases/EWHC/MERCANTILE',\n",
    " 'High Court Patents Court': 'ew/cases/EWHC/PAT',\n",
    " 'High Court Senior Courts Costs Office': 'ew/cases/EWHC/SCCO',\n",
    " 'High Court Technology and Construction Court': 'ew/cases/EWHC/TCC',\n",
    " 'Court of Protection': 'ew/cases/EWCOP',\n",
    " 'Family Court (High Court Judges)': 'ew/cases/EWFC/HCJ',\n",
    " 'Family Court (Other Judges)': 'ew/cases/EWFC/OJ',\n",
    " \"Magistrates' Court (Family)\": 'ew/cases/EWMC/FPC',\n",
    " 'County Court (Family)': 'ew/cases/EWCC/FAM'}\n",
    "\n",
    "bailii_courts_list = list(bailii_courts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a29c9d42-d817-4483-96f4-16bc1b4feb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bailii_court_choice(chosen_list):\n",
    "\n",
    "    chosen_indice = []\n",
    "\n",
    "    if isinstance(chosen_list, str):\n",
    "        chosen_list = ast.literal_eval(chosen_list)\n",
    "\n",
    "    for i in chosen_list:\n",
    "        \n",
    "        chosen_indice.append(bailii_courts[i])\n",
    "    \n",
    "    return chosen_indice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2a0e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function turning search terms to search results url\n",
    "class bailii_search_tool:\n",
    "\n",
    "    def __init__(self,\n",
    "                 citation= '',\n",
    "                case_name = '',\n",
    "                all_of_these_words = '',\n",
    "                exact_phrase = '',\n",
    "                any_of_these_words = '',\n",
    "                advanced_query = '',\n",
    "                datelow = None,\n",
    "                datehigh = None,\n",
    "                sort = list(bailii_sort_dict.keys())[0],\n",
    "                highlight = True,\n",
    "                courts = [],\n",
    "                 judgment_counter_bound = default_judgment_counter_bound\n",
    "             ):\n",
    "\n",
    "        #Initialise parameters\n",
    "        self.citation= citation\n",
    "        self.case_name = case_name\n",
    "        self.all_of_these_words = all_of_these_words\n",
    "        self.exact_phrase = exact_phrase\n",
    "        self.any_of_these_words = any_of_these_words\n",
    "        self.advanced_query = advanced_query\n",
    "        self.datelow = datelow\n",
    "        self.datehigh = datehigh\n",
    "        self.sort = sort\n",
    "        self.highlight = highlight\n",
    "        self.courts = courts\n",
    "        self.judgment_counter_bound = judgment_counter_bound\n",
    "        \n",
    "        self.results_count = 0\n",
    "\n",
    "        self.total_pages = 0\n",
    "        \n",
    "        self.results_url = ''\n",
    "        \n",
    "        self.soup = None\n",
    "        \n",
    "        self.case_infos = []\n",
    "        \n",
    "    #Function for getting url for search results and the soup of first page\n",
    "    def get_url(self):\n",
    "    \n",
    "        #If citation is given, then all other search paras are ignored\n",
    "        if len(self.citation) > 0:\n",
    "    \n",
    "            findby = 'find_by_citation.cgi?'\n",
    "    \n",
    "            base_url = \"https://www.bailii.org/cgi-bin/\" + findby\n",
    "            \n",
    "            params = {'citation': self.citation}\n",
    "    \n",
    "    \n",
    "        else:\n",
    "            \n",
    "            findby = 'lucy_search_1.cgi?'\n",
    "    \n",
    "            base_url = \"https://www.bailii.org/cgi-bin/\" + findby\n",
    "    \n",
    "            #Initialise list of search terms\n",
    "            query_list = []\n",
    "    \n",
    "            if len(self.case_name) > 0:\n",
    "                \n",
    "                case_name_query = f'(title:( {self.case_name} ))'\n",
    "    \n",
    "                query_list.append(case_name_query)\n",
    "    \n",
    "            if len(self.all_of_these_words) > 0:\n",
    "    \n",
    "                all_of_these_words_query_raw_list = self.all_of_these_words.split(' ')\n",
    "    \n",
    "                all_of_these_words_query_list = []\n",
    "    \n",
    "                for word in all_of_these_words_query_raw_list:\n",
    "    \n",
    "                    all_of_these_words_query_list.append(f\" ({word}) \")\n",
    "    \n",
    "                all_of_these_words_query = ' AND '.join(all_of_these_words_query_list)\n",
    "    \n",
    "                query_list.append(all_of_these_words_query)\n",
    "    \n",
    "            if len(self.exact_phrase) > 0:\n",
    "    \n",
    "                exact_phrase_query = f'(\"{self.exact_phrase}\")'\n",
    "    \n",
    "                query_list.append(exact_phrase_query)\n",
    "    \n",
    "            if len(self.any_of_these_words) > 0:\n",
    "    \n",
    "                any_of_these_words_query_raw_list = self.any_of_these_words.split(' ')\n",
    "    \n",
    "                any_of_these_words_query_raw = ' OR '.join(any_of_these_words_query_raw_list)\n",
    "    \n",
    "                any_of_these_words_query = f\"({any_of_these_words_query_raw})\"\n",
    "    \n",
    "                query_list.append(any_of_these_words_query)\n",
    "    \n",
    "            if len(self.advanced_query) > 0:\n",
    "    \n",
    "                advanced_query_query = f'({self.advanced_query})'\n",
    "    \n",
    "                query_list.append(advanced_query_query)\n",
    "                \n",
    "            query = ' AND '.join(query_list)\n",
    "    \n",
    "            #print(f\"Search terms are as follows: {query}\")\n",
    "    \n",
    "            #Datelow param\n",
    "            if self.datelow not in [None, '']:\n",
    "                \n",
    "                self.datelow = date_parser(self.datelow)\n",
    "                \n",
    "                if isinstance(self.datelow, datetime):\n",
    "        \n",
    "                    self.datelow = self.datelow.strftime('%Y%m%d')\n",
    "        \n",
    "                else:\n",
    "                    \n",
    "                    print(\"Can't get datelow param.\")\n",
    "            \n",
    "            #Datehigh param\n",
    "            if self.datehigh not in [None, '']:\n",
    "        \n",
    "                self.datehigh = date_parser(self.datehigh)\n",
    "                \n",
    "                if isinstance(self.datehigh, datetime):\n",
    "        \n",
    "                    self.datehigh = self.datehigh.strftime('%Y%m%d')\n",
    "    \n",
    "                else:\n",
    "                    \n",
    "                    print(\"Can't get datehigh param.\")\n",
    "    \n",
    "            #Sort param\n",
    "            try:\n",
    "                \n",
    "                self.sort = bailii_sort_dict[self.sort]\n",
    "                \n",
    "            except:\n",
    "                \n",
    "                self.sort = bailii_sort_dict[list(bailii_sort_dict.keys())[0]]\n",
    "                \n",
    "                print(f\"Can't get sort param. Kept default {self.sort}.\")\n",
    "    \n",
    "            #Highlight param            \n",
    "            try:\n",
    "                \n",
    "                self.highlight = int(bool(self.highlight))\n",
    "                \n",
    "            except:\n",
    "                \n",
    "                self.highlight = True\n",
    "                \n",
    "                print(f\"Can't get highlight param. Kept default {self.highlight}.\")\n",
    "    \n",
    "            #Choice of courts\n",
    "            courts_indices = ' '.join(bailii_court_choice(self.courts))\n",
    "    \n",
    "            #Limiting to EW cases\n",
    "            mask_path = 'ew/cases uk/cases/UKHL uk/cases/UKPC uk/cases/UKSC ' \n",
    "            \n",
    "            mask_path += courts_indices\n",
    "    \n",
    "            params = {'method': 'boolean',\n",
    "                      'query': query,\n",
    "                      'datelow': self.datelow,\n",
    "                      'datehigh': self.datehigh,\n",
    "                      'sort': self.sort,\n",
    "                      'highlight': self.highlight,\n",
    "                      'mask_path': mask_path\n",
    "                      }\n",
    "    \n",
    "        #print(f\"params == {params}\")\n",
    "    \n",
    "        #params = urllib.parse.urlencode(params, quote_via=urllib.parse.quote)\n",
    "        \n",
    "        #headers = {'User-Agent': 'whatever'}\n",
    "        #response = requests.get(base_url, params=params, headers=headers)\n",
    "    \n",
    "        #soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    \n",
    "        self.results_url = base_url + urllib.parse.urlencode(params, quote_via=urllib.parse.quote)\n",
    "    \n",
    "        #print(f\"self.results_url == {self.results_url}\")\n",
    "    \n",
    "        #return {'results_url': self.results_url, 'soup': self.soup}\n",
    "\n",
    "    def search(self):\n",
    "\n",
    "        #Reset infos of cases found\n",
    "        self.case_infos = []\n",
    "        \n",
    "        if len(self.results_url) == 0:\n",
    "\n",
    "            self.get_url()\n",
    "            \n",
    "        #If citation for a case directly give, then return only that case\n",
    "        if 'find_by_citation.cgi' in self.results_url:\n",
    "\n",
    "            headers = {'User-Agent': 'whatever'}\n",
    "            \n",
    "            direct_link = requests.get(self.results_url, headers=headers).url\n",
    "            \n",
    "            case_info = {'Case name': '',\n",
    "             'Medium neutral citation' : self.citation, \n",
    "            'Date': '',\n",
    "             'Reports': '', \n",
    "             'Hyperlink to BAILII': direct_link, \n",
    "            }\n",
    "    \n",
    "            self.case_infos.append(case_info)\n",
    "\n",
    "            self.results_count = int(1)\n",
    "            self.total_pages = math.ceil(self.results_count/10)\n",
    "        \n",
    "        else:\n",
    "\n",
    "            browser.get(self.results_url)\n",
    "\n",
    "            self.soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "            \n",
    "            results_pattern = re.compile(r\"Total\\sresults.+\")\n",
    "            results_num_list = self.soup.find_all(\"td\", text= results_pattern)\n",
    "            \n",
    "            if len(results_num_list) > 0:\n",
    "            \n",
    "                results_count_raw = results_num_list[0].text.split(' ')[-1].replace(',', '')\n",
    "                self.results_count = int(float(results_count_raw))\n",
    "                self.total_pages = math.ceil(self.results_count/10)\n",
    "\n",
    "            print(f\"Found {self.results_count} results or {self.total_pages} pages\")\n",
    "            \n",
    "            if self.results_count > 0:\n",
    "                \n",
    "                #Start counter\n",
    "                counter = 0\n",
    "    \n",
    "                for page in range(1, self.total_pages + 1):\n",
    "    \n",
    "                    if counter < min(self.results_count, self.judgment_counter_bound):\n",
    "\n",
    "                        #For subsequent pages, need to press next\n",
    "                        if page > 1:\n",
    "                            \n",
    "                            pause.seconds(np.random.randint(scraper_pause_mean - 5, scraper_pause_mean + 5))\n",
    "                            \n",
    "                            submit_buttons = Wait(browser, 30).until(EC.presence_of_all_elements_located((By.XPATH, \"//input[@type='submit']\")))\n",
    "                            next_button = submit_buttons[-1]\n",
    "                            next_button.click()\n",
    "                            self.soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "    \n",
    "                    else:\n",
    "    \n",
    "                        break\n",
    "\n",
    "                    print(f\"Processing page {page} of {self.total_pages}\")\n",
    "                    \n",
    "                    cases_raw = self.soup.find_all(\"li\")\n",
    "                    \n",
    "                    for case_raw in cases_raw:\n",
    "                \n",
    "                        if counter < min(self.results_count, self.judgment_counter_bound):\n",
    "                \n",
    "                            #Initialise default values\n",
    "                            case_name = ''\n",
    "                            mnc = ''\n",
    "                            date = ''\n",
    "                            reports = ''\n",
    "                            link = ''\n",
    "                \n",
    "                            try:           \n",
    "                                \n",
    "                                link = 'https://www.bailii.org' + case_raw.find(\"a\", href = True).get('href')\n",
    "                            \n",
    "                            except Exception as e:\n",
    "                                \n",
    "                                print(f\"Can't get link from case_raw == {case_raw}.\")\n",
    "                \n",
    "                            #Get all other metas\n",
    "                            meta_list = case_raw.get_text().split('\\n')\n",
    "                \n",
    "                            if len(meta_list) > 0:\n",
    "                \n",
    "                                try:\n",
    "                        \n",
    "                                    for meta in meta_list:\n",
    "                                    \n",
    "                                        if len(meta) > 0:\n",
    "                                    \n",
    "                                            case_name = meta\n",
    "                                    \n",
    "                                            break\n",
    "                                    \n",
    "                                    date_list = re.findall(r'\\(\\d.+\\d\\)', case_name)\n",
    "                                    \n",
    "                                    if len(date_list) > 0:\n",
    "                                        \n",
    "                                        date = date_list[-1]\n",
    "                                    \n",
    "                                        if isinstance(date, tuple):\n",
    "                                    \n",
    "                                            date = date[0]\n",
    "                                        \n",
    "                                        case_name = case_name.replace(date, '')\n",
    "                                        \n",
    "                                        date =  date.replace('(', '').replace(')', '')\n",
    "                                    \n",
    "                                    mnc_list = re.findall(r'(\\[\\d{4}\\].+\\d+(\\s\\(\\w+\\))?)', case_name)\n",
    "                                    \n",
    "                                    if len(mnc_list) > 0:\n",
    "                                    \n",
    "                                        mnc = mnc_list[0]\n",
    "                                    \n",
    "                                        if isinstance(mnc, tuple):\n",
    "                                    \n",
    "                                            mnc = mnc[0]\n",
    "\n",
    "                                        if len(mnc) > 0:\n",
    "\n",
    "                                            while mnc[-1] == ' ':\n",
    "\n",
    "                                                mnc = mnc[:-1]\n",
    "                                        \n",
    "                                        case_name = case_name.replace(mnc, '')\n",
    "                                    \n",
    "                                    while case_name[-1] == ' ':\n",
    "                                        \n",
    "                                        case_name = case_name[:-1]\n",
    "                                    \n",
    "                                    reports = meta_list[-2]\n",
    "                \n",
    "                                    while reports[0] in ['(', ',', ' ']:\n",
    "                                        reports = reports[1:]\n",
    "                                    \n",
    "                                    while reports[-1] == ';':\n",
    "                                        reports = reports[:-1]\n",
    "\n",
    "                                except Exception as e:\n",
    "\n",
    "                                    print(f\"Can't get some metadata for {link}\")\n",
    "            \n",
    "                            case_info = {'Case name': case_name,\n",
    "                                     'Medium neutral citation' : mnc, \n",
    "                                    'Date': date,\n",
    "                                     'Reports': reports, \n",
    "                                     'Hyperlink to BAILII': link, \n",
    "                                    }\n",
    "                \n",
    "                            self.case_infos.append(case_info)\n",
    "\n",
    "                            counter += 1\n",
    "                            #print(f\"counter == {counter}\")\n",
    "\n",
    "    #Function for getting all requested judgments\n",
    "    def get_judgments(self):\n",
    "\n",
    "        self.case_infos_w_judgments = []\n",
    "\n",
    "        for case_info in self.case_infos:\n",
    "\n",
    "            if len(self.case_infos_w_judgments) < min(self.results_count, self.judgment_counter_bound):\n",
    "\n",
    "                #Pause to avoid getting kicked out\n",
    "                pause.seconds(np.random.randint(5, 10))\n",
    "\n",
    "                #Initialise default return value\n",
    "                case_info_w_judgment = copy.deepcopy(case_info)\n",
    "                \n",
    "                #Initialise default text\n",
    "                text = ''\n",
    "                \n",
    "                judgment_url = case_info['Hyperlink to BAILII']\n",
    "                headers = {'User-Agent': 'whatever'}\n",
    "                \n",
    "                page = requests.get(judgment_url, headers=headers)\n",
    "                soup = BeautifulSoup(page.content, \"lxml\")\n",
    "                text = soup.get_text()\n",
    "            \n",
    "                if '[Help]' in text:\n",
    "                    try:\n",
    "                        text = text.split('[Help]')[-1]\n",
    "                    except:\n",
    "                        print(f\"Can't get rid of layout type content\")\n",
    "                \n",
    "                #Attach judgment text to case_info_w_judgment dict\n",
    "                case_info_w_judgment.update({'judgment': text})\n",
    "\n",
    "                #If judgment text is too short, get pdf instead\n",
    "                if len(text) < bailii_pdf_judgment_threshold:\n",
    "\n",
    "                    pause.seconds(np.random.randint(scraper_pause_mean - 5, scraper_pause_mean + 5))\n",
    "                    \n",
    "                    print(f\"{case_info['Case name']}: judgment from html is too short, try to get judgment from any pdf.\")                        \n",
    "\n",
    "                    pdf_links_raw = soup.find_all(\"a\", href=re.compile(\"\\.pdf\"))\n",
    "\n",
    "                    if len(pdf_links_raw) > 0:\n",
    "\n",
    "                        pdf_link = 'https://www.bailii.org' + pdf_links_raw[0]['href']\n",
    "\n",
    "                        print(f\"{case_info['Case name']}: trying to get judgment from {pdf_link}.\")                        \n",
    "                        \n",
    "                        try:\n",
    "                            \n",
    "                            text = pdf_image_judgment(pdf_link)\n",
    "\n",
    "                            case_info_w_judgment.update({'judgment': text})\n",
    "\n",
    "                            print(f\"{case_info['Case name']}: got judgment from pdf.\")                        \n",
    "                        \n",
    "                        except Exception as e:\n",
    "\n",
    "                            print(f\"{case_info['Case name']}: can't get judgment from pdf due to error: {e}\")                        \n",
    "\n",
    "                    #Get metadata if not obtained already\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        if len(case_info['Case name']) == 0:\n",
    "\n",
    "                            case_name = ''\n",
    "                            \n",
    "                            date = ''\n",
    "\n",
    "                            case_name_raw = soup.find('title').get_text()\n",
    "\n",
    "                            if case_info['Medium neutral citation'] in case_name_raw:\n",
    "\n",
    "                                case_name = case_name_raw.split(case_info['Medium neutral citation'])[0]\n",
    "\n",
    "                                date = case_name_raw.split(case_info['Medium neutral citation'])[-1]\n",
    "\n",
    "                                while case_name[-1] == ' ':\n",
    "\n",
    "                                    case_name = case_name[:-1]\n",
    "\n",
    "                                while date[-1] in [' ', ')']:\n",
    "\n",
    "                                    date = date[:-1]\n",
    "\n",
    "                                while date[0] in [' ', '(']:\n",
    "                                    \n",
    "                                    date = date[1:]\n",
    "                            \n",
    "                            case_info_w_judgment.update({'Case name': case_name})\n",
    "    \n",
    "                            case_info_w_judgment.update({'Date': date})\n",
    "\n",
    "                    except Exception as e:\n",
    "                        \n",
    "                        print(f\"{case_info['Case name']}: can't get case name or date due to erro: {e}\")                        \n",
    "                \n",
    "                #Make links clickable\n",
    "                for key in case_info_w_judgment:\n",
    "                    \n",
    "                    if 'Hyperlink' in key:\n",
    "                        \n",
    "                        direct_link = case_info_w_judgment[key]\n",
    "\n",
    "                        if '&query' in direct_link:\n",
    "                            \n",
    "                            direct_link = direct_link.split('&query')[0]\n",
    "                        \n",
    "                        case_info_w_judgment[key] = link(direct_link)\n",
    "\n",
    "                        break\n",
    "                \n",
    "                self.case_infos_w_judgments.append(case_info_w_judgment)\n",
    "                    \n",
    "                print(f\"Scraped {len(self.case_infos_w_judgments)}/{min(self.results_count, self.judgment_counter_bound)} judgments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a63874-213d-4ef7-b756-348bd5d35ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@st.cache_data(show_spinner = False)\n",
    "def bailii_search_url(df_master):\n",
    "\n",
    "    df_master = df_master.fillna('')\n",
    "    \n",
    "    #Conduct search\n",
    "    \n",
    "    bailii_search = bailii_search_tool(\n",
    "                citation= df_master.loc[0, 'Citation'],\n",
    "                case_name = df_master.loc[0, 'Case name'],\n",
    "                all_of_these_words = df_master.loc[0, 'All of these words'],\n",
    "                exact_phrase = df_master.loc[0, 'Exact phrase'],\n",
    "                any_of_these_words = df_master.loc[0, 'Any of these words'],\n",
    "                advanced_query = df_master.loc[0, 'Advanced query'],\n",
    "                datelow = df_master.loc[0, 'From date'],\n",
    "                datehigh = df_master.loc[0, 'To date'],\n",
    "                sort = df_master.loc[0, 'Sort results by'],\n",
    "                highlight = df_master.loc[0, 'Highlight search terms in result'],\n",
    "                courts = df_master.loc[0, 'Courts'],\n",
    "                judgment_counter_bound = df_master.loc[0, 'Maximum number of judgments']\n",
    "             )\n",
    "\n",
    "    bailii_search.search()\n",
    "    \n",
    "    return {'results_url': bailii_search.results_url, 'results_count': bailii_search.results_count, 'case_infos': bailii_search.case_infos}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a2d594",
   "metadata": {},
   "source": [
    "# GPT functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c24172f8-acaf-4be0-ac0c-dd0dfc6737a9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gpt_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Import functions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_by_line, GPT_label_dict, is_api_key_valid, gpt_input_cost, gpt_output_cost, tokens_cap, max_output, num_tokens_from_string, judgment_prompt_json, GPT_json, engage_GPT_json  \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Import variables\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m question_characters_bound, default_judgment_counter_bound\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gpt_functions'"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from functions.gpt_functions import GPT_label_dict, is_api_key_valid, gpt_input_cost, gpt_output_cost, tokens_cap, max_output, num_tokens_from_string, judgment_prompt_json, GPT_json, engage_GPT_json\n",
    "#Import variables\n",
    "from functions.gpt_functions import basic_model, flagship_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecf1d9d-6f84-4ec2-976f-73a4fc752235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For checking questions and answers\n",
    "from functions.common_functions import check_questions_answers\n",
    "\n",
    "from functions.gpt_functions import questions_check_system_instruction, GPT_questions_check, GPT_answers_check, unanswered_questions, checked_questions_json, answers_check_system_instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83981e9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'st' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Obtain parameters\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;129m@st\u001b[39m\u001b[38;5;241m.\u001b[39mcache_data(show_spinner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, ttl\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbailii_run\u001b[39m(df_master):\n\u001b[1;32m      5\u001b[0m     df_master \u001b[38;5;241m=\u001b[39m df_master\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#Apply split and format functions for headnotes choice, court choice and GPT questions\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'st' is not defined"
     ]
    }
   ],
   "source": [
    "#Obtain parameters\n",
    "\n",
    "@st.cache_data(show_spinner = False, ttl=600)\n",
    "def bailii_run(df_master):\n",
    "    df_master = df_master.fillna('')\n",
    "\n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    df_master['questions_json'] = df_master['Enter your questions for GPT'].apply(GPT_label_dict)\n",
    "    \n",
    "    #Create judgments file\n",
    "    judgments_file = []\n",
    "    \n",
    "    #Conduct search\n",
    "    bailii_search = bailii_search_tool(\n",
    "                    citation= df_master.loc[0, 'Citation'],\n",
    "                    case_name = df_master.loc[0, 'Case name'],\n",
    "                    all_of_these_words = df_master.loc[0, 'All of these words'],\n",
    "                    exact_phrase = df_master.loc[0, 'Exact phrase'],\n",
    "                    any_of_these_words = df_master.loc[0, 'Any of these words'],\n",
    "                    advanced_query = df_master.loc[0, 'Advanced query'],\n",
    "                    datelow = df_master.loc[0, 'From date'],\n",
    "                    datehigh = df_master.loc[0, 'To date'],\n",
    "                    sort = df_master.loc[0, 'Sort results by'],\n",
    "                    highlight = df_master.loc[0, 'Highlight search terms in result'],\n",
    "                    courts = df_master.loc[0, 'Courts'],\n",
    "                    judgment_counter_bound = df_master.loc[0, 'Maximum number of judgments']\n",
    "                 )\n",
    "    \n",
    "    bailii_search.search()\n",
    "\n",
    "    bailii_search.get_judgments()\n",
    "\n",
    "    for case_info in bailii_search.case_infos_w_judgments:\n",
    "        \n",
    "        judgments_file.append(case_info)\n",
    "    \n",
    "    #Create and export json file with search results\n",
    "    json_individual = json.dumps(judgments_file, indent=2)\n",
    "    \n",
    "    df_individual = pd.read_json(json_individual)\n",
    "    \n",
    "    #Instruct GPT\n",
    "    \n",
    "    #GPT model\n",
    "\n",
    "    if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "        gpt_model = flagship_model\n",
    "    else:        \n",
    "        gpt_model = basic_model\n",
    "    \n",
    "    #apply GPT_individual to each respondent's judgment spreadsheet\n",
    "    \n",
    "    GPT_activation = int(df_master.loc[0, 'Use GPT'])\n",
    "    \n",
    "    questions_json = df_master.loc[0, 'questions_json']\n",
    "\n",
    "    system_instruction = df_master.loc[0, 'System instruction']\n",
    "    \n",
    "    #Engage GPT\n",
    "    df_updated = engage_GPT_json(questions_json = questions_json, df_example = df_master.loc[0, 'Example'], df_individual = df_individual, GPT_activation = GPT_activation, gpt_model = gpt_model, system_instruction = system_instruction)\n",
    "\n",
    "    if (pop_judgment() > 0) and ('judgment' in df_updated.columns):\n",
    "        \n",
    "        df_updated.pop('judgment')\n",
    "    \n",
    "    return df_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f00d98-d517-4e50-8151-a7f37a4ac499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
