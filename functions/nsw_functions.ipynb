{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51218f00-183e-4f7a-9fe5-2393c3de15b0",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07019cf3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Preliminary modules\n",
    "import base64 \n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date\n",
    "from dateutil import parser\n",
    "#from dateutil.relativedelta import *\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import pause\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import httplib2\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "#import pypdf\n",
    "import io\n",
    "from io import BytesIO\n",
    "import ast\n",
    "\n",
    "\n",
    "#Streamlit\n",
    "import streamlit as st\n",
    "#from streamlit_gsheets import GSheetsConnection\n",
    "from streamlit.components.v1 import html\n",
    "#import streamlit_ext as ste\n",
    "from streamlit_extras.stylable_container import stylable_container\n",
    "\n",
    "#NSWCaseLaw\n",
    "from nswcaselaw.search import Search\n",
    "\n",
    "#OpenAI\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "#Google\n",
    "#from google.oauth2 import service_account\n",
    "\n",
    "#Excel\n",
    "from pyxlsb import open_workbook as open_xlsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5120eafa-8759-48bb-9e56-cee8e8cd2ac1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Import functions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m own_account_allowed, convert_df_to_json, convert_df_to_csv, convert_df_to_excel, clear_cache, list_range_check, au_date, save_input\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Import variables\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m today_in_nums, errors_list, scraper_pause_mean, judgment_text_lower_bound, default_judgment_counter_bound, no_results_msg\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'functions'"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from functions.common_functions import own_account_allowed, pop_judgment, convert_df_to_json, convert_df_to_csv, convert_df_to_excel, clear_cache, date_parser, save_input, split_title_mnc, pdf_judgment\n",
    "#Import variables\n",
    "from functions.common_functions import huggingface, today_in_nums, errors_list, scraper_pause_mean, judgment_text_lower_bound, default_judgment_counter_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e432b7",
   "metadata": {},
   "source": [
    "# CaseLaw NSW functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b999856b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Auxiliary lists\n",
    "#search_criteria = ['Free text', 'Case name', 'Before', 'Catchwords', 'Party names', 'Medium neutral citation', 'Decision date from', 'Decision date to', 'File number', 'Legislation cited', 'Cases cited']\n",
    "nsw_meta_labels_droppable = [\"Catchwords\", \"Before\", \"Decision date(s)\", \"Hearing date(s)\", \"Date(s) of order\",  \"Jurisdiction\", \"Decision\", \"Legislation cited\", \"Cases cited\", \"Texts cited\", \"Category\", \"Parties\", \"File number\", \"Representation\", \"Decision under appeal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28525de2-5be8-495a-af01-b3b172106e9f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#List of nsw courts\n",
    "\n",
    "#For showing as menu\n",
    "nsw_courts =[\"Court of Appeal\", \n",
    "             \"Court of Criminal Appeal\", \n",
    "             \"Supreme Court\", \n",
    "             'Land and Environment Court (Judges)', \n",
    "             'Land and Environment Court (Commissioners)', \n",
    "             'District Court', \n",
    "             'Local Court',\n",
    "             \"Children's Court\", \n",
    "             'Compensation Court', \n",
    "             'Drug Court', \n",
    "             'Industrial Court',\n",
    "             'Industrial Relations Commission (Judges)', \n",
    "             'Industrial Relations Commission (Commissioners)'\n",
    "            ] #, \"All of the above Courts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9bc62e-a54e-458c-8c4a-6b26a166ecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default courts\n",
    "nsw_default_courts = [\"Court of Appeal\", \"Court of Criminal Appeal\", \"Supreme Court\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7183b01d-ed57-4d21-836c-0ac3a49bb09e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#List of NSW tribunals\n",
    "\n",
    "nsw_tribunals = ['Administrative Decisions Tribunal (Appeal Panel)',\n",
    " 'Administrative Decisions Tribunal (Divisions)',\n",
    " 'Civil and Administrative Tribunal (Administrative and Equal Opportunity Division)',\n",
    " 'Civil and Administrative Tribunal (Appeal Panel)',\n",
    " 'Civil and Administrative Tribunal (Consumer and Commercial Division)',\n",
    " 'Civil and Administrative Tribunal (Enforcement)',\n",
    " 'Civil and Administrative Tribunal (Guardianship Division)',\n",
    " 'Civil and Administrative Tribunal (Occupational Division)',\n",
    " 'Dust Diseases Tribunal',\n",
    " 'Equal Opportunity Tribunal',\n",
    " 'Fair Trading Tribunal',\n",
    " 'Legal Services Tribunal',\n",
    " 'Medical Tribunal',\n",
    " 'Transport Appeal Boards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef07dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function to convert the list of chosen courts to a list; 13 = NSWSC, 3 = NSWCA, 4 = NSWCCA\n",
    "#For more, see https://github.com/Sydney-Informatics-Hub/nswcaselaw/blob/main/src/nswcaselaw/constants.py\n",
    "\n",
    "nsw_courts_positioning = [\"Placeholder\", \"Children's Court\",\n",
    " 'Compensation Court',\n",
    " 'Court of Appeal',\n",
    " 'Court of Criminal Appeal',\n",
    " 'District Court',\n",
    " 'Drug Court',\n",
    " 'Industrial Court',\n",
    " 'Industrial Relations Commission (Commissioners)',\n",
    " 'Industrial Relations Commission (Judges)',\n",
    " 'Land and Environment Court (Commissioners)',\n",
    " 'Land and Environment Court (Judges)',\n",
    " 'Local Court',\n",
    " 'Supreme Court']\n",
    "\n",
    "def nsw_court_choice(chosen_list):\n",
    "    \n",
    "    chosen_indice = []\n",
    "\n",
    "    if isinstance(chosen_list, str):\n",
    "        chosen_list = ast.literal_eval(chosen_list)\n",
    "\n",
    "    for i in chosen_list:\n",
    "        chosen_indice.append(nsw_courts_positioning.index(i))       \n",
    "        \n",
    "    return chosen_indice\n",
    "\n",
    "nsw_tribunals_positioning = ['Placeholder',\n",
    " 'Administrative Decisions Tribunal (Appeal Panel)',\n",
    " 'Administrative Decisions Tribunal (Divisions)',\n",
    " 'Civil and Administrative Tribunal (Administrative and Equal Opportunity Division)',\n",
    " 'Civil and Administrative Tribunal (Appeal Panel)',\n",
    " 'Civil and Administrative Tribunal (Consumer and Commercial Division)',\n",
    " 'Civil and Administrative Tribunal (Enforcement)',\n",
    " 'Civil and Administrative Tribunal (Guardianship Division)',\n",
    " 'Civil and Administrative Tribunal (Occupational Division)',\n",
    " 'Dust Diseases Tribunal',\n",
    " 'Equal Opportunity Tribunal',\n",
    " 'Fair Trading Tribunal',\n",
    " 'Legal Services Tribunal',\n",
    " 'Medical Tribunal',\n",
    " 'Transport Appeal Boards']\n",
    "\n",
    "def nsw_tribunal_choice(chosen_list):\n",
    "    \n",
    "    chosen_indice = []\n",
    "\n",
    "    if isinstance(chosen_list, str):\n",
    "        chosen_list = ast.literal_eval(chosen_list)\n",
    "\n",
    "    for i in chosen_list:\n",
    "        chosen_indice.append(nsw_tribunals_positioning.index(i))            \n",
    "\n",
    "    return chosen_indice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6998ebb8-a339-4d7d-9dc5-eec41e568bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for tidying up\n",
    "\n",
    "#Tidy up dates\n",
    "def nsw_date(x):\n",
    "    if len(str(x)) > 0:\n",
    "        return str(x).split()[0]\n",
    "    else:\n",
    "        return str(x)\n",
    "\n",
    "# Headnotes fields\n",
    "headnotes_fields = [\"Free text\", \"Case name\", \"Before\", \"Decision date(s)\", \"Catchwords\", \"Hearing date(s)\", \"Date(s) of order\",  \"Jurisdiction\", \"Decision\", \"Legislation cited\", \"Cases cited\", \"Texts cited\", \"Category\", \"Parties\", \"Medium neutral citation\", \"Decision date from\", \"Decision date to\", \"File number\", \"Representation\", \"Decision under appeal\"]\n",
    "headnotes_keys = [\"body\", \"title\", \"before\", \"decisionDate\", \"catchwords\", \"hearingDates\", \"dateOfOrders\", \"jurisdiction\", \"decision\", \"legislationCited\", \"casesCited\", \"textsCited\", \"category\", \"parties\", \"mnc\", \"startDate\", \"endDate\", \"fileNumber\", \"representation\", \"decisionUnderAppeal\"]\n",
    "\n",
    "#Functions for tidying up headings of columns\n",
    "\n",
    "#Tidy up hyperlink\n",
    "def nsw_link(x):\n",
    "    link='https://www.caselaw.nsw.gov.au'+ str(x)\n",
    "    value = '=HYPERLINK(\"' + link + '\")'\n",
    "    return value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12bc2af9-d2ed-4d64-95a0-7e0a84dda2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-19 07:58:46.969 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "#Define function for short judgments, which checks if judgment is in PDF\n",
    "#returns a list of judgment type and judgment text\n",
    "\n",
    "@st.cache_data(show_spinner = False, ttl=600)\n",
    "def nsw_short_judgment(uri):\n",
    "    \n",
    "    html_link = 'https://www.caselaw.nsw.gov.au'+ uri\n",
    "    page_html = requests.get(html_link)\n",
    "    soup_html = BeautifulSoup(page_html.content, \"lxml\")\n",
    "\n",
    "    judgment_type = ''\n",
    "\n",
    "    #Check if judgment contains PDF link\n",
    "    PDF_raw_link = soup_html.find('a', string='See Attachment (PDF)')\n",
    "    \n",
    "    if str(PDF_raw_link).lower() != 'none':\n",
    "        PDF_link = 'https://www.caselaw.nsw.gov.au' + PDF_raw_link.get('href')    \n",
    "        headers = {'User-Agent': 'whatever'}\n",
    "        judgment_text = pdf_judgment(PDF_link)\n",
    "        judgment_type = 'pdf'\n",
    "        \n",
    "    #Return html text if no PDF\n",
    "    else:\n",
    "        judgment_text = soup_html.get_text(separator=\"\\n\", strip=True)\n",
    "        judgment_type = 'html'\n",
    "\n",
    "    return [judgment_type, judgment_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be4246f-3e74-41be-bf00-736a39e3ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@st.cache_data(show_spinner = False, ttl=600)\n",
    "def nsw_search(courts = [],\n",
    "    tribunals = [],\n",
    "    body = '',\n",
    "    title = '',\n",
    "    before = '',\n",
    "    catchwords = '',\n",
    "    party = '',\n",
    "    mnc = '',\n",
    "    startDate = '',\n",
    "    endDate = '',\n",
    "    fileNumber = '',\n",
    "    legislationCited = '',\n",
    "    casesCited = '',\n",
    "    pause = int(0)\n",
    "    ):\n",
    "    query = Search(courts = nsw_court_choice(courts),\n",
    "                    tribunals = nsw_tribunal_choice(tribunals), \n",
    "                    body = body, \n",
    "                    title = title, \n",
    "                    before = before, \n",
    "                    catchwords = catchwords, \n",
    "                    party = party, \n",
    "                    mnc = mnc, \n",
    "                    startDate = startDate, \n",
    "                    endDate = endDate,\n",
    "                    fileNumber = fileNumber, \n",
    "                    legislationCited  = legislationCited, \n",
    "                    casesCited = casesCited,\n",
    "                    pause = pause\n",
    "                    )\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a424b-b3fd-4a26-94c0-4020fbf24a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@st.cache_data(show_spinner = False)\n",
    "def nsw_search_preview(df_master):\n",
    "    df_master = df_master.fillna('')\n",
    "    \n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    #df_master['Courts'] = df_master['Courts'].apply(nsw_court_choice)\n",
    "    #df_master['Tribunals'] = df_master['Tribunals'].apply(nsw_tribunal_choice)\n",
    "\n",
    "    #Combining search terms into new column\n",
    "    \n",
    "    search_dict = {'body': df_master.loc[0, 'Free text']}\n",
    "    search_dict.update({'title': df_master.loc[0, 'Case name']})\n",
    "    search_dict.update({'before': df_master.loc[0, 'Before']})\n",
    "    search_dict.update({'catchwords': df_master.loc[0, 'Catchwords']})\n",
    "    search_dict.update({'party': df_master.loc[0, 'Party names']})\n",
    "    search_dict.update({'mnc': df_master.loc[0, 'Medium neutral citation']})\n",
    "    search_dict.update({'startDate': df_master.loc[0, 'Decision date from']})\n",
    "    search_dict.update({'endDate': df_master.loc[0, 'Decision date to']})\n",
    "    search_dict.update({'fileNumber': df_master.loc[0, 'File number']})\n",
    "    search_dict.update({'legislationCited': df_master.loc[0, 'Legislation cited']})\n",
    "    search_dict.update({'casesCited': df_master.loc[0, 'Cases cited']})\n",
    "    \n",
    "    df_master.loc[0, 'SearchCriteria']=[search_dict]\n",
    "\n",
    "    #Conduct search\n",
    "    query = nsw_search(courts=df_master.loc[0, 'Courts'], \n",
    "                   tribunals=df_master.loc[0, 'Tribunals'], \n",
    "                   body = df_master.loc[0, \"SearchCriteria\"]['body'], \n",
    "                   title = df_master.loc[0, \"SearchCriteria\"]['title'], \n",
    "                   before = df_master.loc[0, \"SearchCriteria\"]['before'], \n",
    "                   catchwords = df_master.loc[0, \"SearchCriteria\"]['catchwords'], \n",
    "                   party = df_master.loc[0, \"SearchCriteria\"]['party'], \n",
    "                   mnc = df_master.loc[0, \"SearchCriteria\"]['mnc'], \n",
    "                   startDate = nsw_date(df_master.loc[0, \"SearchCriteria\"]['startDate']), \n",
    "                   endDate = nsw_date(df_master.loc[0, \"SearchCriteria\"]['endDate']),\n",
    "                   fileNumber = df_master.loc[0, \"SearchCriteria\"]['fileNumber'], \n",
    "                   legislationCited  = df_master.loc[0, \"SearchCriteria\"]['legislationCited'], \n",
    "                   casesCited = df_master.loc[0, \"SearchCriteria\"]['casesCited'],\n",
    "                   pause = 0\n",
    "                  )\n",
    "\n",
    "    #Create results to show\n",
    "    judgments_file = []\n",
    "    \n",
    "    #Counter to limit search results to append\n",
    "    counter = 0\n",
    "\n",
    "    #Go through search results\n",
    "    \n",
    "    judgments_counter_bound = int(df_master.loc[0, 'Maximum number of judgments'])\n",
    "    \n",
    "    #Create list of relevant cases\n",
    "    for decision in query.results():\n",
    "        \n",
    "        if counter < judgments_counter_bound:\n",
    "\n",
    "            #Append to judgments_file to create df_individual\n",
    "            decision_w_meta = decision.values.copy()\n",
    "\n",
    "            #add search results to json\n",
    "            judgments_file.append(decision_w_meta)\n",
    "\n",
    "            counter +=1            \n",
    "            \n",
    "        else:\n",
    "            break\n",
    "                \n",
    "    results_to_show = judgments_file\n",
    "\n",
    "    #Get url to NSW Caselaw search page\n",
    "    results_url = query.url\n",
    "    \n",
    "    #Create total number of results\n",
    "    results_count = int(0)\n",
    "\n",
    "    if len(results_to_show) > 0:\n",
    "        \n",
    "        pause.seconds(scraper_pause_mean)\n",
    "        \n",
    "        page_html = requests.get(query.url)\n",
    "        soup_html = BeautifulSoup(page_html.content, \"lxml\")\n",
    "        results_count_raw = soup_html.find('div', {'id': 'paginationcontainer'})\n",
    "        results_count_text = results_count_raw.get_text(strip = True)\n",
    "        results_count_text = results_count_text.replace(',', '').replace('.', '')\n",
    "        results_count = int(float(results_count_text.split(' ')[-2]))\n",
    "\n",
    "    return {'results_to_show': results_to_show, 'results_url': results_url, 'results_count': results_count}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e69fbb2-62e9-4f28-be14-5c539b630c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT IN USE\n",
    "\n",
    "def nsw_search_url(df_master):\n",
    "    df_master = df_master.fillna('')\n",
    "    \n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    #df_master['Courts'] = df_master['Courts'].apply(nsw_court_choice)\n",
    "    #df_master['Tribunals'] = df_master['Tribunals'].apply(nsw_tribunal_choice)\n",
    "\n",
    "    #Combining search terms into new column\n",
    "    \n",
    "    search_dict = {'body': df_master.loc[0, 'Free text']}\n",
    "    search_dict.update({'title': df_master.loc[0, 'Case name']})\n",
    "    search_dict.update({'before': df_master.loc[0, 'Before']})\n",
    "    search_dict.update({'catchwords': df_master.loc[0, 'Catchwords']})\n",
    "    search_dict.update({'party': df_master.loc[0, 'Party names']})\n",
    "    search_dict.update({'mnc': df_master.loc[0, 'Medium neutral citation']})\n",
    "    search_dict.update({'startDate': df_master.loc[0, 'Decision date from']})\n",
    "    search_dict.update({'endDate': df_master.loc[0, 'Decision date to']})\n",
    "    search_dict.update({'fileNumber': df_master.loc[0, 'File number']})\n",
    "    search_dict.update({'legislationCited': df_master.loc[0, 'Legislation cited']})\n",
    "    search_dict.update({'casesCited': df_master.loc[0, 'Cases cited']})\n",
    "    \n",
    "    df_master.loc[0, 'SearchCriteria']=[search_dict]\n",
    "\n",
    "    #Conduct search\n",
    "    query = nsw_search(courts=df_master.loc[0, 'Courts'], \n",
    "                   tribunals=df_master.loc[0, 'Tribunals'], \n",
    "                   body = df_master.loc[0, \"SearchCriteria\"]['body'], \n",
    "                   title = df_master.loc[0, \"SearchCriteria\"]['title'], \n",
    "                   before = df_master.loc[0, \"SearchCriteria\"]['before'], \n",
    "                   catchwords = df_master.loc[0, \"SearchCriteria\"]['catchwords'], \n",
    "                   party = df_master.loc[0, \"SearchCriteria\"]['party'], \n",
    "                   mnc = df_master.loc[0, \"SearchCriteria\"]['mnc'], \n",
    "                   startDate = nsw_date(df_master.loc[0, \"SearchCriteria\"]['startDate']), \n",
    "                   endDate = nsw_date(df_master.loc[0, \"SearchCriteria\"]['endDate']),\n",
    "                   fileNumber = df_master.loc[0, \"SearchCriteria\"]['fileNumber'], \n",
    "                   legislationCited  = df_master.loc[0, \"SearchCriteria\"]['legislationCited'], \n",
    "                   casesCited = df_master.loc[0, \"SearchCriteria\"]['casesCited'],\n",
    "                   pause = 0\n",
    "                  )    \n",
    "    return query.url\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e984b836",
   "metadata": {},
   "source": [
    "# GPT functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "264f776f-9383-4727-bc0f-fdee31463433",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gpt_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Import functions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_by_line, GPT_label_dict, is_api_key_valid, gpt_input_cost, gpt_output_cost, tokens_cap, num_tokens_from_string, judgment_prompt_json, GPT_json_tokens, engage_GPT_json_tokens  \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Import variables\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m question_characters_bound, default_judgment_counter_bound, role_content\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gpt_functions'"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from functions.gpt_functions import split_by_line, GPT_label_dict, is_api_key_valid, gpt_input_cost, gpt_output_cost, tokens_cap, max_output, num_tokens_from_string, judgment_prompt_json, GPT_json, engage_GPT_json\n",
    "#Import variables\n",
    "from functions.gpt_functions import question_characters_bound, basic_model, flagship_model#, role_content\n",
    "#For batch mode\n",
    "from functions.gpt_functions import gpt_get_custom_id, gpt_batch_input_id_line, gpt_batch_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb6dde1-e2da-480b-80d9-f839f8e1778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For checking questions and answers\n",
    "from functions.common_functions import check_questions_answers\n",
    "\n",
    "from functions.gpt_functions import questions_check_system_instruction, GPT_questions_check, checked_questions_json, answers_check_system_instruction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa8b81-719b-4672-a141-7d9331cde96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jurisdiction specific instruction\n",
    "#system_instruction = role_content\n",
    "\n",
    "#intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913b046b-d0f2-47ac-b02b-bc2032811b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to tidy up after GPT output is produced\n",
    "\n",
    "def nsw_tidying_up(df_master, df_individual):\n",
    "\n",
    "    #Rename column titles\n",
    "    try:\n",
    "        df_individual['Hyperlink to NSW Caselaw'] = df_individual['uri'].apply(nsw_link)\n",
    "        df_individual.pop('uri')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #Replace abbreviated column names with full names\n",
    "    for col_name in headnotes_keys:\n",
    "        if col_name in df_individual.columns:\n",
    "            col_index = headnotes_keys.index(col_name)\n",
    "            new_col_name = headnotes_fields[col_index]\n",
    "            df_individual.rename(columns={col_name: new_col_name}, inplace=True)\n",
    "            #df_individual[new_col_name] = df_individual[col_name]\n",
    "            #df_individual.pop(col_name)\n",
    "\n",
    "    #Reorganise columns\n",
    "    df_individual = df_individual.loc[:,~df_individual.columns.duplicated()].copy()\n",
    "\n",
    "    old_columns = df_individual.columns.to_list()\n",
    "    \n",
    "    for i in ['Case name', 'Medium neutral citation', 'Hyperlink to NSW Caselaw']:\n",
    "        if i in old_columns:\n",
    "            old_columns.remove(i)\n",
    "    \n",
    "    new_columns = ['Case name', 'Medium neutral citation', 'Hyperlink to NSW Caselaw'] + old_columns\n",
    "    \n",
    "    df_individual = df_individual.reindex(columns=new_columns)\n",
    "\n",
    "    #Drop metadata if not wanted \n",
    "    if int(float(df_master.loc[0, 'Metadata inclusion'])) == 0:\n",
    "        for meta_label in nsw_meta_labels_droppable:\n",
    "            if meta_label in df_individual.columns:\n",
    "                df_individual.pop(meta_label)\n",
    "\n",
    "    #Remove judgment column\n",
    "    GPT_activation = int(df_master.loc[0, 'Use GPT'])\n",
    "    \n",
    "    if (pop_judgment() > 0) and ('judgment' in df_individual.columns):\n",
    "        df_individual.pop(\"judgment\")\n",
    "        \n",
    "    #Check case name, medium neutral citation \n",
    "\n",
    "    for k in df_individual.index:\n",
    "        \n",
    "        most_informative_key = ''\n",
    "\n",
    "        if len(str(df_individual.loc[k, \"Case name\"])) > len(str(df_individual.loc[k, \"Medium neutral citation\"])):\n",
    "            most_informative_key = \"Case name\"\n",
    "        else:\n",
    "            most_informative_key = \"Medium neutral citation\"\n",
    "        \n",
    "        case_name_mnc = split_title_mnc(df_individual.loc[k, most_informative_key])\n",
    "        case_name = case_name_mnc[0]\n",
    "        mnc = case_name_mnc[1]\n",
    "        \n",
    "        df_individual.loc[k, \"Case name\"] = case_name\n",
    "        df_individual.loc[k, \"Medium neutral citation\"] = mnc\n",
    "\n",
    "    return df_individual\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c798e6-3059-4a0a-93cf-df0c2e32506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to tidy up before GPT output is produced\n",
    "\n",
    "def nsw_tidying_up_pre_gpt(df_master, df_individual):\n",
    "\n",
    "    #Rename column titles\n",
    "    try:\n",
    "        df_individual['Hyperlink to NSW Caselaw'] = df_individual['uri'].apply(nsw_link)\n",
    "        df_individual.pop('uri')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #Replace abbreviated column names with full names\n",
    "    for col_name in headnotes_keys:\n",
    "        if col_name in df_individual.columns:\n",
    "            col_index = headnotes_keys.index(col_name)\n",
    "            new_col_name = headnotes_fields[col_index]\n",
    "            df_individual.rename(columns={col_name: new_col_name}, inplace=True)\n",
    "\n",
    "    #Reorganise columns\n",
    "    old_columns = list(df_individual.columns)\n",
    "    \n",
    "    for i in ['Case name', 'Medium neutral citation', 'Hyperlink to NSW Caselaw']:\n",
    "        if i in old_columns:\n",
    "            old_columns.remove(i)\n",
    "    \n",
    "    new_columns = ['Case name', 'Medium neutral citation', 'Hyperlink to NSW Caselaw'] + old_columns\n",
    "    \n",
    "    df_individual = df_individual.reindex(columns=new_columns)\n",
    "\n",
    "    #Drop metadata if not wanted\n",
    "    if int(float(df_master.loc[0, 'Metadata inclusion'])) == 0:\n",
    "        for meta_label in nsw_meta_labels_droppable:\n",
    "            if meta_label in df_individual.columns:\n",
    "                df_individual.pop(meta_label)\n",
    "        \n",
    "    #Check case name, medium neutral citation \n",
    "    for k in df_individual.index:\n",
    "\n",
    "        most_informative_key = ''\n",
    "\n",
    "        if len(str(df_individual.loc[k, \"Case name\"])) > len(str(df_individual.loc[k, \"Medium neutral citation\"])):\n",
    "            most_informative_key = \"Case name\"\n",
    "        else:\n",
    "            most_informative_key = \"Medium neutral citation\"\n",
    "        \n",
    "        case_name_mnc = split_title_mnc(df_individual.loc[k, most_informative_key])\n",
    "        case_name = case_name_mnc[0]\n",
    "        mnc = case_name_mnc[1]\n",
    "        \n",
    "        df_individual.loc[k, \"Case name\"] = case_name\n",
    "        df_individual.loc[k, \"Medium neutral citation\"] = mnc\n",
    "\n",
    "    return df_individual\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35635339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 08:59:15.894 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "#Download directly from Caselaw NSW without looking in OALC first\n",
    "#NOT IN USE\n",
    "\n",
    "@st.cache_data(show_spinner = False, ttl=600)\n",
    "def nsw_run_direct(df_master):\n",
    "    df_master = df_master.fillna('')\n",
    "    \n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    #df_master['Courts'] = df_master['Courts'].apply(nsw_court_choice)\n",
    "    #df_master['Tribunals'] = df_master['Tribunals'].apply(nsw_tribunal_choice)\n",
    "    df_master['Enter your questions for GPT'] = df_master['Enter your questions for GPT'][0: question_characters_bound].apply(split_by_line)\n",
    "    df_master['questions_json'] = df_master['Enter your questions for GPT'].apply(GPT_label_dict)\n",
    "    \n",
    "    #Do search\n",
    "\n",
    "    search_dict = {'body': df_master.loc[0, 'Free text']}\n",
    "    search_dict.update({'title': df_master.loc[0, 'Case name']})\n",
    "    search_dict.update({'before': df_master.loc[0, 'Before']})\n",
    "    search_dict.update({'catchwords': df_master.loc[0, 'Catchwords']})\n",
    "    search_dict.update({'party': df_master.loc[0, 'Party names']})\n",
    "    search_dict.update({'mnc': df_master.loc[0, 'Medium neutral citation']})\n",
    "    search_dict.update({'startDate': df_master.loc[0, 'Decision date from']})\n",
    "    search_dict.update({'endDate': df_master.loc[0, 'Decision date to']})\n",
    "    search_dict.update({'fileNumber': df_master.loc[0, 'File number']})\n",
    "    search_dict.update({'legislationCited': df_master.loc[0, 'Legislation cited']})\n",
    "    search_dict.update({'casesCited': df_master.loc[0, 'Cases cited']})\n",
    "    df_master.loc[0, 'SearchCriteria']=[search_dict]\n",
    "\n",
    "    #Conduct search\n",
    "    \n",
    "    query = nsw_search(courts=df_master.loc[0, 'Courts'], \n",
    "                   tribunals=df_master.loc[0, 'Tribunals'], \n",
    "                   body = df_master.loc[0, \"SearchCriteria\"]['body'], \n",
    "                   title = df_master.loc[0, \"SearchCriteria\"]['title'], \n",
    "                   before = df_master.loc[0, \"SearchCriteria\"]['before'], \n",
    "                   catchwords = df_master.loc[0, \"SearchCriteria\"]['catchwords'], \n",
    "                   party = df_master.loc[0, \"SearchCriteria\"]['party'], \n",
    "                   mnc = df_master.loc[0, \"SearchCriteria\"]['mnc'], \n",
    "                   startDate = nsw_date(df_master.loc[0, \"SearchCriteria\"]['startDate']), \n",
    "                   endDate = nsw_date(df_master.loc[0, \"SearchCriteria\"]['endDate']),\n",
    "                   fileNumber = df_master.loc[0, \"SearchCriteria\"]['fileNumber'], \n",
    "                   legislationCited  = df_master.loc[0, \"SearchCriteria\"]['legislationCited'], \n",
    "                   casesCited = df_master.loc[0, \"SearchCriteria\"]['casesCited'],\n",
    "                   pause = 0\n",
    "                  )\n",
    "\n",
    "    #Create judgments file\n",
    "    judgments_file = []\n",
    "\n",
    "    #Counter to limit search results to append\n",
    "    counter = 0\n",
    "\n",
    "    #Go through search results\n",
    "    \n",
    "    judgments_counter_bound = int(df_master.loc[0, 'Maximum number of judgments'])\n",
    "    \n",
    "    for decision in query.results():\n",
    "        if counter < judgments_counter_bound:\n",
    "            #Get case info from results page\n",
    "            decision_w_meta = decision.values.copy()\n",
    "\n",
    "            try:\n",
    "                #Get case info from individual case page\n",
    "                decision.fetch()\n",
    "    \n",
    "                #Attach new info\n",
    "                decision_w_meta_judgment = decision.values\n",
    "                #for key in decision_w_meta_judgment.keys():\n",
    "                    #if key not in decision_w_meta.keys():\n",
    "                        #decision_w_meta.update({key: decision_w_meta_judgment[key]})\n",
    "    \n",
    "                decision_w_meta.update({'judgment': str(decision_w_meta_judgment)})\n",
    "            \n",
    "            except:\n",
    "                decision_w_meta.update({'judgment': ''})\n",
    "                print(f'{decision_w_meta[\"title\"]}: judgment text scraping error.')\n",
    "\n",
    "            #add search results to json\n",
    "            judgments_file.append(decision_w_meta)\n",
    "            counter +=1\n",
    "    \n",
    "            pause.seconds(np.random.randint(scraper_pause_mean - 5, scraper_pause_mean + 5))\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            break\n",
    "\n",
    "    #Create and export json file with search results\n",
    "    json_individual = json.dumps(judgments_file, indent=2)\n",
    "    \n",
    "    df_individual = pd.read_json(json_individual)\n",
    "\n",
    "    #Check length of judgment text, replace with raw html if smaller than lower boound\n",
    "    for judgment_index in df_individual.index:\n",
    "\n",
    "        #Checking if judgment text has been scrapped or too short\n",
    "        try:\n",
    "            judgment_raw_text = str(df_individual.loc[judgment_index, \"judgment\"])\n",
    "                    \n",
    "            if num_tokens_from_string(judgment_raw_text, \"cl100k_base\") < judgment_text_lower_bound:\n",
    "                \n",
    "                judgment_type_text = nsw_short_judgment(df_individual.loc[judgment_index, \"uri\"])\n",
    "    \n",
    "                #attach judgment text; judgment_type_text[0] has judgment type, eg 'pdf', while judgment_type_text[1] is the judgment text\n",
    "                df_individual.loc[judgment_index, \"judgment\"] = judgment_type_text[1]\n",
    "\n",
    "                pause.seconds(np.random.randint(scraper_pause_mean - 5, scraper_pause_mean + 5))\n",
    "            \n",
    "        except Exception as e:\n",
    "            \n",
    "            df_individual.loc[judgment_index, \"judgment\"] = ''\n",
    "            print(f'{df_individual.loc[judgment_index, \"title\"]}: judgment text scraping error.')\n",
    "            print(e)\n",
    "    \n",
    "    #Instruct GPT\n",
    "    \n",
    "    #GPT model\n",
    "\n",
    "    if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "        gpt_model = flagship_model\n",
    "    else:        \n",
    "        gpt_model = basic_model\n",
    "    \n",
    "    #apply GPT_individual to each respondent's judgment spreadsheet\n",
    "    \n",
    "    GPT_activation = int(df_master.loc[0, 'Use GPT'])\n",
    "    \n",
    "    questions_json = df_master.loc[0, 'questions_json']\n",
    "\n",
    "    system_instruction = df_master.loc[0, 'System instruction']\n",
    "    \n",
    "    #Engage GPT    \n",
    "    df_updated = engage_GPT_json(questions_json = questions_json, df_example = df_master.loc[0, 'Example'], df_individual = df_individual, GPT_activation = GPT_activation, gpt_model = gpt_model, system_instruction = system_instruction)\n",
    "\n",
    "    #tidy up\n",
    "    df_updated = nsw_tidying_up(df_master, df_updated)\n",
    "    \n",
    "    return df_updated\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709d9235-8db3-4b78-b08f-91e3bc18ffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download from Caselaw NSW if can't find judgment in OALC\n",
    "\n",
    "@st.cache_data(show_spinner = False, ttl=600)\n",
    "def nsw_run(df_master):\n",
    "    \n",
    "    df_master = df_master.fillna('')\n",
    "    \n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    #df_master['Courts'] = df_master['Courts'].apply(nsw_court_choice)\n",
    "    #df_master['Tribunals'] = df_master['Tribunals'].apply(nsw_tribunal_choice)\n",
    "    df_master['Enter your questions for GPT'] = df_master['Enter your questions for GPT'][0: question_characters_bound].apply(split_by_line)\n",
    "    df_master['questions_json'] = df_master['Enter your questions for GPT'].apply(GPT_label_dict)\n",
    "    \n",
    "    #Do search\n",
    "\n",
    "    search_dict = {'body': df_master.loc[0, 'Free text']}\n",
    "    search_dict.update({'title': df_master.loc[0, 'Case name']})\n",
    "    search_dict.update({'before': df_master.loc[0, 'Before']})\n",
    "    search_dict.update({'catchwords': df_master.loc[0, 'Catchwords']})\n",
    "    search_dict.update({'party': df_master.loc[0, 'Party names']})\n",
    "    search_dict.update({'mnc': df_master.loc[0, 'Medium neutral citation']})\n",
    "    search_dict.update({'startDate': df_master.loc[0, 'Decision date from']})\n",
    "    search_dict.update({'endDate': df_master.loc[0, 'Decision date to']})\n",
    "    search_dict.update({'fileNumber': df_master.loc[0, 'File number']})\n",
    "    search_dict.update({'legislationCited': df_master.loc[0, 'Legislation cited']})\n",
    "    search_dict.update({'casesCited': df_master.loc[0, 'Cases cited']})\n",
    "    df_master.loc[0, 'SearchCriteria']=[search_dict]\n",
    "\n",
    "    #Conduct search\n",
    "    \n",
    "    query = nsw_search(courts=df_master.loc[0, 'Courts'], \n",
    "                   tribunals=df_master.loc[0, 'Tribunals'], \n",
    "                   body = df_master.loc[0, \"SearchCriteria\"]['body'], \n",
    "                   title = df_master.loc[0, \"SearchCriteria\"]['title'], \n",
    "                   before = df_master.loc[0, \"SearchCriteria\"]['before'], \n",
    "                   catchwords = df_master.loc[0, \"SearchCriteria\"]['catchwords'], \n",
    "                   party = df_master.loc[0, \"SearchCriteria\"]['party'], \n",
    "                   mnc = df_master.loc[0, \"SearchCriteria\"]['mnc'], \n",
    "                   startDate = nsw_date(df_master.loc[0, \"SearchCriteria\"]['startDate']), \n",
    "                   endDate = nsw_date(df_master.loc[0, \"SearchCriteria\"]['endDate']),\n",
    "                   fileNumber = df_master.loc[0, \"SearchCriteria\"]['fileNumber'], \n",
    "                   legislationCited  = df_master.loc[0, \"SearchCriteria\"]['legislationCited'], \n",
    "                   casesCited = df_master.loc[0, \"SearchCriteria\"]['casesCited'],\n",
    "                   pause = 0\n",
    "                  )\n",
    "\n",
    "    #Create judgments file\n",
    "    judgments_file = []\n",
    "\n",
    "    #Counter to limit search results to append\n",
    "    counter = 0\n",
    "\n",
    "    judgments_counter_bound = int(df_master.loc[0, 'Maximum number of judgments'])\n",
    "\n",
    "    if huggingface == False: #If not running on HuggingFace\n",
    "\n",
    "        for decision in query.results():\n",
    "            if counter < judgments_counter_bound:\n",
    "                #Get case info from results page\n",
    "                decision_w_meta = decision.values.copy()\n",
    "\n",
    "                try:\n",
    "                    #Get case info from individual case  page\n",
    "                    decision.fetch()\n",
    "        \n",
    "                    #Attach new info\n",
    "                    decision_w_meta_judgment = decision.values\n",
    "                    #for key in decision_w_meta_judgment.keys():\n",
    "                        #if #key not in decision_w_meta.keys():\n",
    "                            #decision_w_meta.update({key: decision_w_meta_judgment[key]})\n",
    "    \n",
    "                    decision_w_meta.update({'judgment': str(decision_w_meta_judgment)})\n",
    "\n",
    "                except:\n",
    "                    decision_w_meta.update({'judgment': ''})\n",
    "                    print(f'{decision_w_meta[\"title\"]}: judgment text scraping error.')\n",
    "                \n",
    "                #add search results to json\n",
    "                judgments_file.append(decision_w_meta)\n",
    "                counter +=1\n",
    "        \n",
    "                pause.seconds(np.random.randint(scraper_pause_mean - 5, scraper_pause_mean + 5))\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "\n",
    "    else: #If running on HuggingFace\n",
    "        \n",
    "        #Load oalc\n",
    "        from functions.oalc_functions import load_corpus, get_judgment_from_oalc\n",
    "\n",
    "        #Create a list of mncs for HuggingFace:\n",
    "        mnc_list = []\n",
    "\n",
    "        #Create list of relevant cases\n",
    "        for decision in query.results():\n",
    "            \n",
    "            if counter < judgments_counter_bound:\n",
    "    \n",
    "                #Append to judgments_file to create df_individual\n",
    "                decision_w_meta = decision.values.copy()\n",
    "    \n",
    "                #Create and mnc\n",
    "                mnc = split_title_mnc(decision_w_meta['title'])[1]\n",
    "                decision_w_meta.update({'mnc': mnc})\n",
    "                \n",
    "                #add search results to json\n",
    "                judgments_file.append(decision_w_meta)\n",
    "\n",
    "                #Add mnc to list for HuggingFace\n",
    "                mnc_list.append(mnc)\n",
    "                \n",
    "                counter +=1            \n",
    "                \n",
    "            else:\n",
    "                break\n",
    "\n",
    "        #Get judgments from oalc first\n",
    "        mnc_judgment_dict = get_judgment_from_oalc(mnc_list)\n",
    "    \n",
    "        #Append judgment to judgments_file \n",
    "        for decision in judgments_file:\n",
    "            \n",
    "            #Append judgments from oalc first\n",
    "            if decision['mnc'] in mnc_judgment_dict.keys():\n",
    "                decision.update({'judgment': mnc_judgment_dict[decision['mnc']]})\n",
    "                print(f\"{decision['title']} got judgment from OALC.\")\n",
    "                \n",
    "            else: #Get case from Caselaw NSW if can't get from oalc\n",
    "                \n",
    "                for case in query.results():\n",
    "\n",
    "                    case_meta = case.values.copy()\n",
    "\n",
    "                    #st.write(case_meta)\n",
    "                    \n",
    "                    if decision['mnc'] in case_meta['title']:\n",
    "                        try:\n",
    "                            case.fetch()\n",
    "                            case_w_meta_jugdment = case.values.copy()\n",
    "                            decision.update({'judgment': str(case_w_meta_jugdment)})\n",
    "\n",
    "                            print(f\"{decision['title']} got judgment from NSW Caselaw directly.\")\n",
    "                            \n",
    "                        except:\n",
    "                            decision.update({'judgment': ''})\n",
    "                            print(f'{decision[\"title\"]}: judgment text scraping error.')\n",
    "\n",
    "                        break\n",
    "\n",
    "                #Pause only if need to get judgment from Caselaw NSW\n",
    "                pause.seconds(np.random.randint(scraper_pause_mean - 5, scraper_pause_mean + 5))\n",
    "\n",
    "\n",
    "    #Create and export json file with search results\n",
    "    json_individual = json.dumps(judgments_file, indent=2)\n",
    "    \n",
    "    df_individual = pd.read_json(json_individual)\n",
    "\n",
    "    #Check length of judgment text, replace with raw html if smaller than lower boound\n",
    "\n",
    "    for judgment_index in df_individual.index:\n",
    "\n",
    "        #Checking if judgment text has been scrapped or too short\n",
    "        try:\n",
    "            \n",
    "            judgment_raw_text = str(df_individual.loc[judgment_index, \"judgment\"])\n",
    "                    \n",
    "            if num_tokens_from_string(judgment_raw_text, \"cl100k_base\") < judgment_text_lower_bound:\n",
    "\n",
    "                judgment_type_text = nsw_short_judgment(df_individual.loc[judgment_index, \"uri\"])\n",
    "    \n",
    "                #attach judgment text\n",
    "                df_individual.loc[judgment_index, \"judgment\"] = judgment_type_text[1]\n",
    "\n",
    "                #judgment_type_text[0] has judgment type, eg 'pdf'\n",
    "                \n",
    "                pause.seconds(np.random.randint(scraper_pause_mean - 5, scraper_pause_mean + 5))\n",
    "            \n",
    "        except Exception as e:\n",
    "            \n",
    "            df_individual.loc[judgment_index, \"judgment\"] = ''\n",
    "            print(f'{df_individual.loc[judgment_index, \"title\"]}: judgment text scraping error.')\n",
    "            print(e)\n",
    "\n",
    "    #Instruct GPT\n",
    "    \n",
    "    #GPT model\n",
    "\n",
    "    if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "        gpt_model = flagship_model\n",
    "    else:        \n",
    "        gpt_model = basic_model\n",
    "    \n",
    "    #apply GPT_individual to each respondent's judgment spreadsheet\n",
    "    \n",
    "    GPT_activation = int(df_master.loc[0, 'Use GPT'])\n",
    "    \n",
    "    questions_json = df_master.loc[0, 'questions_json']\n",
    "\n",
    "    system_instruction = df_master.loc[0, 'System instruction']\n",
    "    \n",
    "    #Engage GPT    \n",
    "    df_updated = engage_GPT_json(questions_json = questions_json, df_example = df_master.loc[0, 'Example'], df_individual = df_individual, GPT_activation = GPT_activation, gpt_model = gpt_model, system_instruction = system_instruction)\n",
    "\n",
    "    #tidy up\n",
    "    df_updated = nsw_tidying_up(df_master, df_updated)\n",
    "\n",
    "    return df_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afbacc1-8134-4a9d-a3f2-eee6f755a013",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#For batch mode\n",
    "\n",
    "@st.cache_data(show_spinner = False, ttl=600)\n",
    "def nsw_batch(df_master):\n",
    "\n",
    "    df_master = df_master.fillna('')\n",
    "    \n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    #df_master['Courts'] = df_master['Courts'].apply(nsw_court_choice)\n",
    "    #df_master['Tribunals'] = df_master['Tribunals'].apply(nsw_tribunal_choice)\n",
    "    df_master['Enter your questions for GPT'] = df_master['Enter your questions for GPT'][0: question_characters_bound].apply(split_by_line)\n",
    "    df_master['questions_json'] = df_master['Enter your questions for GPT'].apply(GPT_label_dict)\n",
    "    \n",
    "    #Do search\n",
    "\n",
    "    search_dict = {'body': df_master.loc[0, 'Free text']}\n",
    "    search_dict.update({'title': df_master.loc[0, 'Case name']})\n",
    "    search_dict.update({'before': df_master.loc[0, 'Before']})\n",
    "    search_dict.update({'catchwords': df_master.loc[0, 'Catchwords']})\n",
    "    search_dict.update({'party': df_master.loc[0, 'Party names']})\n",
    "    search_dict.update({'mnc': df_master.loc[0, 'Medium neutral citation']})\n",
    "    search_dict.update({'startDate': df_master.loc[0, 'Decision date from']})\n",
    "    search_dict.update({'endDate': df_master.loc[0, 'Decision date to']})\n",
    "    search_dict.update({'fileNumber': df_master.loc[0, 'File number']})\n",
    "    search_dict.update({'legislationCited': df_master.loc[0, 'Legislation cited']})\n",
    "    search_dict.update({'casesCited': df_master.loc[0, 'Cases cited']})\n",
    "    df_master.loc[0, 'SearchCriteria']=[search_dict]\n",
    "\n",
    "    #Conduct search\n",
    "    \n",
    "    query = nsw_search(courts=df_master.loc[0, 'Courts'], \n",
    "                   tribunals=df_master.loc[0, 'Tribunals'], \n",
    "                   body = df_master.loc[0, \"SearchCriteria\"]['body'], \n",
    "                   title = df_master.loc[0, \"SearchCriteria\"]['title'], \n",
    "                   before = df_master.loc[0, \"SearchCriteria\"]['before'], \n",
    "                   catchwords = df_master.loc[0, \"SearchCriteria\"]['catchwords'], \n",
    "                   party = df_master.loc[0, \"SearchCriteria\"]['party'], \n",
    "                   mnc = df_master.loc[0, \"SearchCriteria\"]['mnc'], \n",
    "                   startDate = nsw_date(df_master.loc[0, \"SearchCriteria\"]['startDate']), \n",
    "                   endDate = nsw_date(df_master.loc[0, \"SearchCriteria\"]['endDate']),\n",
    "                   fileNumber = df_master.loc[0, \"SearchCriteria\"]['fileNumber'], \n",
    "                   legislationCited  = df_master.loc[0, \"SearchCriteria\"]['legislationCited'], \n",
    "                   casesCited = df_master.loc[0, \"SearchCriteria\"]['casesCited'],\n",
    "                   pause = 0\n",
    "                  )\n",
    "\n",
    "    #Create judgments file\n",
    "    judgments_file = []\n",
    "\n",
    "    #Counter to limit search results to append\n",
    "    counter = 0\n",
    "\n",
    "    judgments_counter_bound = int(df_master.loc[0, 'Maximum number of judgments'])\n",
    "\n",
    "    #st.write(f\"judgments_counter_bound == {judgments_counter_bound}\")\n",
    "    \n",
    "    if huggingface == False: #If not running on HuggingFace\n",
    "\n",
    "        for decision in query.results():\n",
    "            if counter < judgments_counter_bound:\n",
    "                #Get case info from results page\n",
    "                decision_w_meta = decision.values.copy()\n",
    "\n",
    "                try:\n",
    "                    #Get case info from individual case  page\n",
    "                    decision.fetch()\n",
    "        \n",
    "                    #Attach new info\n",
    "                    decision_w_meta_judgment = decision.values\n",
    "                    #for key in decision_w_meta_judgment.keys():\n",
    "                        #if #key not in decision_w_meta.keys():\n",
    "                            #decision_w_meta.update({key: decision_w_meta_judgment[key]})\n",
    "    \n",
    "                    decision_w_meta.update({'judgment': str(decision_w_meta_judgment)})\n",
    "\n",
    "                except:\n",
    "                    decision_w_meta.update({'judgment': ''})\n",
    "                    print(f'{decision_w_meta[\"title\"]}: judgment text scraping error.')\n",
    "                \n",
    "                #add search results to json\n",
    "                judgments_file.append(decision_w_meta)\n",
    "                counter +=1\n",
    "        \n",
    "                pause.seconds(np.random.randint(scraper_pause_mean - 5, scraper_pause_mean + 5))\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "\n",
    "    else: #If running on HuggingFace\n",
    "        \n",
    "        #Load oalc\n",
    "        from functions.oalc_functions import load_corpus, get_judgment_from_oalc\n",
    "\n",
    "        #Create a list of mncs for HuggingFace:\n",
    "        mnc_list = []\n",
    "\n",
    "        #Create list of relevant cases\n",
    "        for decision in query.results():\n",
    "            \n",
    "            if counter < judgments_counter_bound:\n",
    "    \n",
    "                #Append to judgments_file to create df_individual\n",
    "                decision_w_meta = decision.values.copy()\n",
    "    \n",
    "                #Create and mnc\n",
    "                mnc = split_title_mnc(decision_w_meta['title'])[1]\n",
    "                decision_w_meta.update({'mnc': mnc})\n",
    "                \n",
    "                #add search results to json\n",
    "                judgments_file.append(decision_w_meta)\n",
    "\n",
    "                #Add mnc to list for HuggingFace\n",
    "                mnc_list.append(mnc)\n",
    "                \n",
    "                counter +=1            \n",
    "                \n",
    "            else:\n",
    "                break\n",
    "\n",
    "        #Get judgments from oalc first\n",
    "        mnc_judgment_dict = get_judgment_from_oalc(mnc_list)\n",
    "    \n",
    "        #Append judgment to judgments_file \n",
    "        for decision in judgments_file:\n",
    "            \n",
    "            #Append judgments from oalc first\n",
    "            if decision['mnc'] in mnc_judgment_dict.keys():\n",
    "                decision.update({'judgment': mnc_judgment_dict[decision['mnc']]})\n",
    "                print(f\"{decision['title']} got judgment from OALC.\")\n",
    "                \n",
    "            else: #Get case from Caselaw NSW if can't get from oalc\n",
    "                \n",
    "                for case in query.results():\n",
    "\n",
    "                    case_meta = case.values.copy()\n",
    "\n",
    "                    #st.write(case_meta)\n",
    "                    \n",
    "                    if decision['mnc'] in case_meta['title']:\n",
    "                        try:\n",
    "                            case.fetch()\n",
    "                            case_w_meta_jugdment = case.values.copy()\n",
    "                            decision.update({'judgment': str(case_w_meta_jugdment)})\n",
    "\n",
    "                            print(f\"{decision['title']} got judgment from NSW Caselaw directly.\")\n",
    "                            \n",
    "                        except:\n",
    "                            decision.update({'judgment': ''})\n",
    "                            print(f'{decision[\"title\"]}: judgment text scraping error.')\n",
    "\n",
    "                        break\n",
    "\n",
    "                #Pause only if need to get judgment from Caselaw NSW\n",
    "                pause.seconds(np.random.randint(scraper_pause_mean - 5, scraper_pause_mean + 5))\n",
    "\n",
    "\n",
    "    #Create and export json file with search results\n",
    "    json_individual = json.dumps(judgments_file, indent=2)\n",
    "    \n",
    "    df_individual = pd.read_json(json_individual)\n",
    "\n",
    "    #Check length of judgment text, replace with raw html if smaller than lower boound\n",
    "\n",
    "    for judgment_index in df_individual.index:\n",
    "\n",
    "        #Checking if judgment text has been scrapped or too short\n",
    "        try:\n",
    "            \n",
    "            judgment_raw_text = str(df_individual.loc[judgment_index, \"judgment\"])\n",
    "                    \n",
    "            if num_tokens_from_string(judgment_raw_text, \"cl100k_base\") < judgment_text_lower_bound:\n",
    "\n",
    "                judgment_type_text = nsw_short_judgment(df_individual.loc[judgment_index, \"uri\"])\n",
    "    \n",
    "                #attach judgment text\n",
    "                df_individual.loc[judgment_index, \"judgment\"] = judgment_type_text[1]\n",
    "\n",
    "                #judgment_type_text[0] has judgment type, eg 'pdf'\n",
    "                \n",
    "                pause.seconds(np.random.randint(scraper_pause_mean - 5, scraper_pause_mean + 5))\n",
    "            \n",
    "        except Exception as e:\n",
    "            \n",
    "            df_individual.loc[judgment_index, \"judgment\"] = ''\n",
    "            print(f'{df_individual.loc[judgment_index, \"title\"]}: judgment text scraping error.')\n",
    "            print(e)\n",
    "\n",
    "    #Instruct GPT\n",
    "    \n",
    "    #GPT model\n",
    "\n",
    "    if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "        gpt_model = flagship_model\n",
    "    else:        \n",
    "        gpt_model = basic_model\n",
    "    \n",
    "    #apply GPT_individual to each respondent's judgment spreadsheet\n",
    "    \n",
    "    GPT_activation = int(df_master.loc[0, 'Use GPT'])\n",
    "    \n",
    "    questions_json = df_master.loc[0, 'questions_json']\n",
    "\n",
    "    system_instruction = df_master.loc[0, 'System instruction']\n",
    "    \n",
    "    #Tidu up then send batch input to gpt\n",
    "    df_individual = nsw_tidying_up_pre_gpt(df_master, df_individual)\n",
    "\n",
    "    #Engage GPT\n",
    "    batch_record_df_individual = gpt_batch_input(questions_json = questions_json, df_example = df_master.loc[0, 'Example'], df_individual = df_individual, GPT_activation = GPT_activation, gpt_model = gpt_model, system_instruction = system_instruction)\n",
    "    \n",
    "    return batch_record_df_individual"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
