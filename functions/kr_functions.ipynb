{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b8b879",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2312235",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Preliminary modules\n",
    "import base64 \n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date\n",
    "from dateutil import parser\n",
    "#from dateutil.relativedelta import *\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import pause\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import httplib2\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import urllib.request\n",
    "import io\n",
    "from io import BytesIO\n",
    "import math\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "#Streamlit\n",
    "import streamlit as st\n",
    "#from streamlit_gsheets import GSheetsConnection\n",
    "from streamlit.components.v1 import html\n",
    "#import streamlit_ext as ste\n",
    "from streamlit_extras.stylable_container import stylable_container\n",
    "\n",
    "#OpenAI\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "#Google\n",
    "#from google.oauth2 import service_account\n",
    "\n",
    "#Excel\n",
    "from pyxlsb import open_workbook as open_xlsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b19432-dc40-4740-ac69-cb4e7f6d356b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface == True\n",
      "Running locally or on Streamlit\n"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from functions.common_functions import own_account_allowed, pop_judgment, convert_df_to_json, convert_df_to_csv, convert_df_to_excel, save_input\n",
    "#Import variables\n",
    "from functions.common_functions import today_in_nums, scraper_pause_mean, judgment_text_lower_bound, default_judgment_counter_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e819e27",
   "metadata": {},
   "source": [
    "# Kercher Reports search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55cab971-389c-47ed-a636-32d12f113b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.common_functions import link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7480c74-8d82-4bdd-990c-29dc3c2b48c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape javascript\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import ActionChains\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from webdriver_manager.core.os_manager import ChromeType\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait as Wait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "#options.add_argument(\"--headless\")\n",
    "options.add_argument('--no-sandbox')  \n",
    "options.add_argument('--disable-dev-shm-usage')  \n",
    "\n",
    "if 'Users/Ben' not in os.getcwd(): \n",
    "\n",
    "    from pyvirtualdisplay import Display\n",
    "    \n",
    "    display = Display(visible=0, size=(1200, 1600))  \n",
    "    display.start()\n",
    "\n",
    "    options.add_argument(\"window-size=1200x600\")\n",
    "\n",
    "#@st.cache_resource(show_spinner = False, ttl=600)\n",
    "def get_driver():\n",
    "\n",
    "    browser = webdriver.Chrome(options=options)\n",
    "\n",
    "    browser.implicitly_wait(15)\n",
    "    browser.set_page_load_timeout(30)\n",
    "\n",
    "    if 'Users/Ben' in os.getcwd():\n",
    "        browser.minimize_window()\n",
    "    \n",
    "    return browser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abf6c5df-6168-4618-8da7-1fe379184c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kr_selenium_judgment_text(case_info):\n",
    "    url = case_info['Hyperlink to AustLII']\n",
    "\n",
    "    browser = get_driver()\n",
    "        \n",
    "    #Get search results\n",
    "    browser.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "    text = soup.get_text()\n",
    "    try:\n",
    "        text = soup.get_text().split('Print (pretty)')[0].split('\\n Any \\n')[-1]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    browser.quit()\n",
    "    \n",
    "    return text\n",
    "\n",
    "#Meta labels and judgment combined\n",
    "\n",
    "#@st.cache_data(show_spinner = False)\n",
    "def kr_selenium_meta_judgment_dict(case_info):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        judgment_dict = {'Case name': '',\n",
    "                         'Medium neutral citation' : '', \n",
    "                         'Other reports': '', \n",
    "                         'Hyperlink to AustLII': '', \n",
    "                         'Date' : '', \n",
    "                         'judgment': ''\n",
    "                        }\n",
    "    \n",
    "        case_name = case_info['Case name']\n",
    "        date = case_info['Case name'].split('(')[-1].replace(')', '')\n",
    "        year = case_info['Case name'].split('[')[1][0:4]\n",
    "        case_number_raw = case_info['Case name'].split('NSWSupC ')[1].split(' (')[0]\n",
    "        \n",
    "        if \";\" in case_number_raw:\n",
    "            case_number = case_number_raw.split(';')[0]\n",
    "        else:\n",
    "            case_number = case_number_raw\n",
    "        \n",
    "        mnc = '[' + year +']' + ' NSWSupC ' + case_number\n",
    "        nr_cite = ''\n",
    "            \n",
    "        try:\n",
    "            case_name = case_info['Case name'].split('[')[0][:-1]\n",
    "            nr_cite = case_info['Case name'].split('; ')[1].replace(' (' + date + ')', '')\n",
    "        except:\n",
    "            pass\n",
    "                    \n",
    "        judgment_dict['Case name'] = case_name\n",
    "        judgment_dict['Medium neutral citation'] = mnc\n",
    "        judgment_dict['Other reports'] = nr_cite\n",
    "        judgment_dict['Date'] = date\n",
    "        judgment_dict['Hyperlink to AustLII'] = link(case_info['Hyperlink to AustLII'])\n",
    "        judgment_dict['judgment'] = kr_selenium_judgment_text(case_info)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{judgment_dict['Case name']}: judgment not scrapped\")\n",
    "        print(e)\n",
    "        \n",
    "    return judgment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "368a329f-35d4-4e2f-9694-a11fc3cc5c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class kr_search_tool:\n",
    "\n",
    "    def __init__(self,\n",
    "             query= '',\n",
    "            year = '',\n",
    "            letter = '',\n",
    "            judgment_counter_bound = default_judgment_counter_bound\n",
    "         ):\n",
    "    \n",
    "        #Initialise parameters\n",
    "        self.query = query\n",
    "        self.year = str(year).replace('.', '')\n",
    "        self.letter = letter\n",
    "        self.judgment_counter_bound = judgment_counter_bound\n",
    "        \n",
    "        self.results_count = 0\n",
    "        \n",
    "        self.total_pages = 0\n",
    "        \n",
    "        self.results_url = ''\n",
    "        \n",
    "        self.base_url = 'https://www.austlii.edu.au/cgi-bin/viewdb/au/cases/nsw/NSWSupC/'\n",
    "        \n",
    "        self.soup = None\n",
    "        \n",
    "        self.case_infos = []\n",
    "\n",
    "    def get_url(self):\n",
    "    \n",
    "        if len(self.year) > 0:\n",
    "    \n",
    "            self.results_url = f'https://www.austlii.edu.au/cgi-bin/viewtoc/au/cases/nsw/NSWSupC/{self.year}/'\n",
    "    \n",
    "        elif len(self.letter) > 0:\n",
    "    \n",
    "            self.results_url = f'https://www.austlii.edu.au/cgi-bin/viewtoc/au/cases/nsw/NSWSupC/toc-{self.letter.upper()}.html'\n",
    "    \n",
    "        else:\n",
    "            \n",
    "            params = {'meta' : '',\n",
    "                      'mask_path' : 'au/cases/nsw/NSWSupC', \n",
    "                      'method' : 'auto',\n",
    "                      'query' : self.query\n",
    "                     }\n",
    "    \n",
    "            self.results_url = self.base_url + urllib.parse.urlencode(params)\n",
    "        \n",
    "        #return {'results_url': self.results_url, 'self.soup': self.soup}\n",
    "\n",
    "    def search(self):\n",
    "\n",
    "        if len(self.results_url) == 0:\n",
    "\n",
    "            self.get_url()\n",
    "\n",
    "        browser = get_driver()\n",
    "\n",
    "        #If year or letter given, then search self.results_url\n",
    "        if (len(self.year) > 0) or (len(self.letter) > 0):\n",
    "\n",
    "            browser.get(self.results_url)\n",
    "\n",
    "            pause.seconds(np.random.randint(10, 15))\n",
    "            \n",
    "            self.soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "            \n",
    "            #number of search results\n",
    "\n",
    "            #Get self.case_infos\n",
    "            #hrefs = self.soup.find_all('a', href=re.compile('/cgi-bin/viewdoc/au/cases/nsw/NSWSupC'))\n",
    "                        \n",
    "            #for link in hrefs:\n",
    "                \n",
    "                #if (' NSWSupC ' in str(link)) and ('LawCite' not in str(link)):\n",
    "            \n",
    "                    #self.results_count += 1\n",
    "\n",
    "            hrefs = self.soup.find_all('a', href=re.compile('/cgi-bin/viewdoc/au/cases/nsw/NSWSupC'))\n",
    "\n",
    "            self.results_count = len(hrefs)\n",
    "            \n",
    "            self.total_pages = 1\n",
    "\n",
    "        #If year or letter not given but query given, enter query in search box and enter\n",
    "        else:\n",
    "\n",
    "            browser.get(self.base_url)\n",
    "            \n",
    "            search_box = Wait(browser, 30).until(EC.visibility_of_element_located((By.ID, 'search-box')))\n",
    "\n",
    "            search_box.send_keys(self.query)\n",
    "\n",
    "            search_box.send_keys(Keys.ENTER)\n",
    "\n",
    "            pause.seconds(np.random.randint(10, 15))\n",
    "            \n",
    "            self.soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "    \n",
    "            #print(self.soup)\n",
    "            \n",
    "            #number of search results\n",
    "            #docs_found_string = re.findall(r'\\d+', str(self.soup.find('li', class_='number-docs').text).replace(',', ''))[0]\n",
    "            docs_found_string = re.findall(r'\\d+', str(self.soup.find('title')).replace(',', ''))[0]\n",
    "            \n",
    "            self.results_count = int(float(docs_found_string))\n",
    "            self.total_pages = math.ceil(self.results_count/10) #10 results per page\n",
    "\n",
    "        if self.results_count > 0:\n",
    "\n",
    "            #Start counter\n",
    "            counter = 0\n",
    "\n",
    "            for page in range(1, self.total_pages + 1):\n",
    "\n",
    "                if counter < min(self.results_count, self.judgment_counter_bound):\n",
    "\n",
    "                    if page > 1:\n",
    "                        \n",
    "                        pause.seconds(np.random.randint(10, 15))\n",
    "\n",
    "                        #Get next page buttons from current page\n",
    "                        page_buttons = Wait(browser, 30).until(EC.presence_of_all_elements_located((By.XPATH, \"//div[@id='pagination-sort']//a[contains(@href, '/cgi-bin/sinosrch.cgi?')]\")))\n",
    "\n",
    "                        #Decide whether there is a need to click 'next' to get the next 10 pages\n",
    "                        need_to_click_next = True\n",
    "                        \n",
    "                        for page_button in page_buttons:\n",
    "\n",
    "                            if page_button.text == str(page):\n",
    "\n",
    "                                need_to_click_next = False\n",
    "\n",
    "                                page_button.click()\n",
    "\n",
    "                                break\n",
    "\n",
    "                        #If there is a need to click 'next' to get the next 10 pages \n",
    "                        if need_to_click_next == True:\n",
    "\n",
    "                            #Get the next 10 pages\n",
    "                            next_button = page_buttons[-1]\n",
    "\n",
    "                            next_button.click()\n",
    "\n",
    "                            page_buttons = Wait(browser, 30).until(EC.presence_of_all_elements_located((By.XPATH, \"//div[@id='pagination-sort']//a[contains(@href, '/cgi-bin/sinosrch.cgi?')]\")))\n",
    "\n",
    "                            for page_button in page_buttons:\n",
    "    \n",
    "                                if page_button.text == str(page):\n",
    "    \n",
    "                                    need_to_click_next = False\n",
    "    \n",
    "                                    page_button.click()\n",
    "    \n",
    "                                    break\n",
    "\n",
    "                        #Update self.soup\n",
    "                        self.soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "                else:\n",
    "\n",
    "                    break\n",
    "\n",
    "                print(f\"Processing page {page} of {self.total_pages}\")\n",
    "        \n",
    "                #Get self.case_infos\n",
    "                #hrefs = self.soup.find_all('a', href=True)\n",
    "\n",
    "                hrefs = self.soup.find_all('a', href=re.compile('/cgi-bin/viewdoc/au/cases/nsw/NSWSupC'))\n",
    "                \n",
    "                for link in hrefs:\n",
    "\n",
    "                    if counter < self.judgment_counter_bound:\n",
    "                    \n",
    "                    #if ((counter < self.judgment_counter_bound) and (' NSWSupC ' in str(link)) and ('LawCite' not in str(link))):\n",
    "                        case = link.get_text()\n",
    "                        link_direct = link.get('href')\n",
    "                        link = 'https://www.austlii.edu.au' + link_direct.split('?context')[0]\n",
    "                        \n",
    "                        dict_object = {'Case name': case, \n",
    "                                       'Hyperlink to AustLII': link}\n",
    "                        \n",
    "                        self.case_infos.append(dict_object)\n",
    "                        \n",
    "                        counter = counter + 1\n",
    "\n",
    "\n",
    "        browser.quit()\n",
    "        \n",
    "        #return self.case_infos\n",
    "\n",
    "    def get_judgments(self):\n",
    "\n",
    "        self.case_infos_w_judgments = []\n",
    "        \n",
    "        for case_info in self.case_infos:\n",
    "\n",
    "            if len(self.case_infos_w_judgments) < min(self.results_count, self.judgment_counter_bound):\n",
    "\n",
    "                #Pause to avoid getting kicked out\n",
    "                pause.seconds(np.random.randint(5, 10))\n",
    "\n",
    "                case_info_w_judgment = kr_selenium_meta_judgment_dict(case_info)\n",
    "                        \n",
    "                self.case_infos_w_judgments.append(case_info_w_judgment)\n",
    "                    \n",
    "                print(f\"Scraped {len(self.case_infos_w_judgments)}/{min(self.results_count, self.judgment_counter_bound)} judgments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c08a5bc-24d2-4a45-8054-aeafa11b4697",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#kr_search = kr_search_tool(query= 'Forbes', year = '', letter = '', judgment_counter_bound = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4a98851-7066-4736-8b56-007e8e8497ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#kr_search.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "76bed733-bd39-441a-9a9a-3973e391d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kr_search.results_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2716cb8e-e610-4471-afea-c7509f46feb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#kr_search.get_judgments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd1a4744-b509-43b6-aeea-f32e8375de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#case_infos_w_judgments = kr_search.case_infos_w_judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "46cef68c-d8af-44a9-a092-54991964ed2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#case_infos_w_judgments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9f2f670-342c-44ab-a87b-58aa2b023b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of search methods\n",
    "#NOT IN USE\n",
    "\n",
    "kr_methods_list = ['Full text', 'Titles only', 'This Boolean query', 'Any of these words', 'All of these words']\n",
    "kr_method_types = ['auto', 'title', 'boolean', 'any', 'all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2a0e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function turning search terms to search results url\n",
    "#NOT IN USE\n",
    "\n",
    "#@st.cache_data(show_spinner = False)\n",
    "def kr_search(query= '', \n",
    "              method = ''\n",
    "             ):\n",
    "    base_url = \"https://www.austlii.edu.au/cgi-bin/sinosrch.cgi?\"\n",
    "\n",
    "    method_index = kr_methods_list.index(method)\n",
    "    method_type = kr_method_types[method_index]\n",
    "\n",
    "    query_text = query\n",
    "\n",
    "    params = {'meta' : '',\n",
    "              'mask_path' : 'au/cases/nsw/NSWSupC', \n",
    "              'method' : method_type,\n",
    "              'query' : query_text\n",
    "             }\n",
    "\n",
    "    headers = {'User-Agent': 'whatever'}\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    \n",
    "    return {'results_url': response.url, 'soup': soup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6321d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function turning search results url to case_link_pairs to judgments\n",
    "\n",
    "#NOT IN USE\n",
    "\n",
    "#@st.cache_data(show_spinner = False)\n",
    "def kr_search_results_to_case_link_pairs(_soup, url_search_results, judgment_counter_bound):\n",
    "    #_soup, url_search_results are from kr_search\n",
    "\n",
    "    hrefs = _soup.find_all('a', href=True)\n",
    "    case_link_pairs = []\n",
    "\n",
    "    #number of search results\n",
    "    docs_found_string = re.findall(r'\\d+', str(soup.find('title')).replace(',', ''))[0]\n",
    "    docs_found = int(float(docs_found_string))\n",
    "\n",
    "    #Start counter\n",
    "    counter = 1\n",
    "    \n",
    "    for link in hrefs:\n",
    "        if ((counter <= judgment_counter_bound) and (' NSWSupC ' in str(link)) and ('LawCite' not in str(link))):\n",
    "#        if ((counter <= judgment_counter_bound) and ('AustLII' in str(link)) and ('cases/EngR' in str(link)) and ('LawCite' not in str(link))):\n",
    "            case = link.get_text()\n",
    "            link_direct = link.get('href')\n",
    "            link = 'https://www.austlii.edu.au' + link_direct.split('?context')[0]\n",
    "            dict_object = { 'Case name': case, 'Hyperlink to AustLII': link}\n",
    "            case_link_pairs.append(dict_object)\n",
    "            counter = counter + 1\n",
    "        \n",
    "    for ending in range(10, docs_found, 10):\n",
    "        if counter <= min(judgment_counter_bound, docs_found):\n",
    "            url_next_page = url_search_results + ';offset=' + f\"{ending}\"\n",
    "            page_judgment_next_page = requests.get(url_next_page, headers=headers)\n",
    "            soup_judgment_next_page = BeautifulSoup(page_judgment_next_page.content, \"lxml\")\n",
    "            \n",
    "            hrefs_next_page = soup_judgment_next_page.find_all('a', href=True)\n",
    "            for extra_link in hrefs_next_page:\n",
    "                if ((counter <= judgment_counter_bound) and (' NSWSupC ' in str(extra_link)) and ('LawCite' not in str(extra_link))):\n",
    "#                if ((counter <= judgment_counter_bound) and ('AustLII' in str(extra_link)) and ('cases/EngR' in str(extra_link)) and ('LawCite' not in str(extra_link))):\n",
    "                    case = extra_link.get_text()\n",
    "                    extra_link_direct = extra_link.get('href')\n",
    "                    extra_link = 'https://www.austlii.edu.au' + extra_link_direct.split('?context')[0]\n",
    "                    dict_object = { 'Case name': case, 'Hyperlink to AustLII': extra_link}\n",
    "                    case_link_pairs.append(dict_object)\n",
    "                    counter = counter + 1\n",
    "\n",
    "            pause.seconds(np.random.randint(5, 15))\n",
    "            \n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    #If no need to get rid of repetitions\n",
    "    #return case_link_pairs\n",
    "    \n",
    "    #Get rid of repetitions\n",
    "    case_link_pairs_no_repeats = []\n",
    "\n",
    "    for case_link_pair in case_link_pairs:\n",
    "        if  case_link_pair not in case_link_pairs_no_repeats:\n",
    "            case_link_pairs_no_repeats.append(case_link_pair)\n",
    "            \n",
    "    return case_link_pairs_no_repeats\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a72562c-8dce-4d9e-a2f4-c7a14a5877a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert case-link pairs to judgment text\n",
    "\n",
    "#NOT IN USE\n",
    "\n",
    "#@st.cache_data(show_spinner = False)\n",
    "def kr_judgment_text(case_link_pair):\n",
    "    url = case_link_pair['Hyperlink to AustLII']\n",
    "    headers = {'User-Agent': 'whatever'}\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "    text = soup.get_text()\n",
    "    try:\n",
    "        text = soup.get_text().split('Print (pretty)')[0].split('\\n Any \\n')[-1]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return text\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a85223-64ca-4d1b-998c-1c2769c5a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Meta labels and judgment combined\n",
    "\n",
    "#NOT IN USE\n",
    "\n",
    "#@st.cache_data(show_spinner = False)\n",
    "def kr_meta_judgment_dict(case_link_pair):\n",
    "    try:\n",
    "        judgment_dict = {'Case name': '',\n",
    "                         'Medium neutral citation' : '', \n",
    "                         'Other reports': '', \n",
    "                         'Hyperlink to AustLII': '', \n",
    "                         'Date' : '', \n",
    "                         'judgment': ''\n",
    "                        }\n",
    "    \n",
    "        case_name = case_link_pair['Case name']\n",
    "        date = case_link_pair['Case name'].split('(')[-1].replace(')', '')\n",
    "        year = case_link_pair['Case name'].split('[')[1][0:4]\n",
    "        case_number_raw = case_link_pair['Case name'].split('NSWSupC ')[1].split(' (')[0]\n",
    "        \n",
    "        if \";\" in case_number_raw:\n",
    "            case_number = case_number_raw.split(';')[0]\n",
    "        else:\n",
    "            case_number = case_number_raw\n",
    "        \n",
    "        mnc = '[' + year +']' + ' NSWSupC ' + case_number\n",
    "        nr_cite = ''\n",
    "            \n",
    "        try:\n",
    "            case_name = case_link_pair['Case name'].split('[')[0][:-1]\n",
    "            nr_cite = case_link_pair['Case name'].split('; ')[1].replace(' (' + date + ')', '')\n",
    "        except:\n",
    "            pass\n",
    "                    \n",
    "        judgment_dict['Case name'] = case_name\n",
    "        judgment_dict['Medium neutral citation'] = mnc\n",
    "        judgment_dict['Other reports'] = nr_cite\n",
    "        judgment_dict['Date'] = date\n",
    "        judgment_dict['Hyperlink to AustLII'] = link(case_link_pair['Hyperlink to AustLII'])\n",
    "        judgment_dict['judgment'] = kr_judgment_text(case_link_pair)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{judgment_dict['Case name']}: judgment not scrapped\")\n",
    "        print(e)\n",
    "        \n",
    "    return judgment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf909b5-ce3e-4571-9a26-6412150e6fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@st.cache_data(show_spinner = False)\n",
    "def kr_search_url(df_master):\n",
    "\n",
    "    df_master = df_master.fillna('')\n",
    "    \n",
    "    #Conduct search\n",
    "    \n",
    "    kr_search = kr_search_tool(query= df_master.loc[0, 'Search query'],\n",
    "                    year = df_master.loc[0, 'Specific year'], \n",
    "                    letter = df_master.loc[0, 'Decision begins with'],\n",
    "                    judgment_counter_bound = df_master.loc[0, 'Maximum number of judgments']\n",
    "                   )\n",
    "\n",
    "    kr_search.search()\n",
    "    \n",
    "    return {'results_url': kr_search.results_url, 'results_count': kr_search.results_count, 'case_infos': kr_search.case_infos}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a2d594",
   "metadata": {},
   "source": [
    "# GPT functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2db138-2b99-4be8-a9a9-7d6e80758a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import functions\n",
    "from functions.gpt_functions import GPT_label_dict, is_api_key_valid, gpt_input_cost, gpt_output_cost, tokens_cap, max_output, num_tokens_from_string, judgment_prompt_json, GPT_json, engage_GPT_json  \n",
    "#Import variables\n",
    "from functions.gpt_functions import basic_model, flagship_model#, role_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3751f91-6a65-49b8-b053-cc02e831c271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For checking questions and answers\n",
    "from functions.common_functions import check_questions_answers\n",
    "\n",
    "from functions.gpt_functions import questions_check_system_instruction, GPT_questions_check, checked_questions_json, answers_check_system_instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83981e9d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Obtain parameters\n",
    "\n",
    "@st.cache_data(show_spinner = False, ttl=600)\n",
    "def kr_run(df_master):\n",
    "    df_master = df_master.fillna('')\n",
    "\n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    df_master['questions_json'] = df_master['Enter your questions for GPT'].apply(GPT_label_dict)\n",
    "    \n",
    "    #Create judgments file\n",
    "    judgments_file = []\n",
    "    \n",
    "    #Conduct search\n",
    "    kr_search = kr_search_tool(query= df_master.loc[0, 'Search query'],\n",
    "                    year = df_master.loc[0, 'Specific year'], \n",
    "                    letter = df_master.loc[0, 'Decision begins with'],\n",
    "                    judgment_counter_bound = df_master.loc[0, 'Maximum number of judgments']\n",
    "                   )\n",
    "    \n",
    "    kr_search.search()\n",
    "\n",
    "    kr_search.get_judgments()\n",
    "\n",
    "    for case_info in kr_search.case_infos_w_judgments:\n",
    "        \n",
    "        judgments_file.append(case_info)\n",
    "    \n",
    "    #Create and export json file with search results\n",
    "    json_individual = json.dumps(judgments_file, indent=2)\n",
    "    df_individual = pd.read_json(json_individual)\n",
    "\n",
    "    #For KR, convert date to string so as to avoid Excel producing random numbers for dates\n",
    "    df_individual['Date'] = df_individual['Date'].astype(str)\n",
    "\n",
    "    #Instruct GPT\n",
    "    \n",
    "    #GPT model\n",
    "\n",
    "    if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "        gpt_model = flagship_model\n",
    "    else:        \n",
    "        gpt_model = basic_model\n",
    "            \n",
    "    #apply GPT_individual to each respondent's judgment spreadsheet\n",
    "    \n",
    "    GPT_activation = int(df_master.loc[0, 'Use GPT'])\n",
    "\n",
    "    questions_json = df_master.loc[0, 'questions_json']\n",
    "\n",
    "    system_instruction = df_master.loc[0, 'System instruction']\n",
    "    \n",
    "    #Engage GPT\n",
    "    df_updated = engage_GPT_json(questions_json = questions_json, df_example = df_master.loc[0, 'Example'], df_individual = df_individual, GPT_activation = GPT_activation, gpt_model = gpt_model, system_instruction = system_instruction)\n",
    "\n",
    "    if (pop_judgment() > 0) and ('judgment' in df_updated.columns):\n",
    "        df_updated.pop('judgment')\n",
    "    \n",
    "    return df_updated"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
