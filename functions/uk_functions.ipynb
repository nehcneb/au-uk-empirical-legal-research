{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b8b879",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2312235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminary modules\n",
    "import base64 \n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date\n",
    "from dateutil import parser\n",
    "from dateutil.relativedelta import *\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import pause\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import httplib2\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "from io import BytesIO\n",
    "import ast\n",
    "\n",
    "#Streamlit\n",
    "import streamlit as st\n",
    "#from streamlit_gsheets import GSheetsConnection\n",
    "from streamlit.components.v1 import html\n",
    "import streamlit_ext as ste\n",
    "from streamlit_extras.stylable_container import stylable_container\n",
    "\n",
    "#OpenAI\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "#Google\n",
    "#from google.oauth2 import service_account\n",
    "\n",
    "#Excel\n",
    "from pyxlsb import open_workbook as open_xlsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2790eb5a-6de3-4548-89c7-902015d5ec10",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface == True\n"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from functions.common_functions import own_account_allowed, convert_df_to_json, convert_df_to_csv, convert_df_to_excel, list_range_check, au_date, save_input\n",
    "#Import variables\n",
    "from functions.common_functions import today_in_nums, errors_list, scraper_pause_mean, judgment_text_lower_bound, default_judgment_counter_bound, no_results_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e819e27",
   "metadata": {},
   "source": [
    "# UK Courts search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1c62702-7127-48dd-bac4-875f92cd7369",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Initialize default courts\n",
    "\n",
    "uk_courts_default_list = ['United Kingdom Supreme Court',\n",
    " 'Privy Council',\n",
    " 'Court of Appeal Civil Division',\n",
    " 'Court of Appeal Criminal Division',\n",
    " 'High Court (England & Wales) Administrative Court',\n",
    " 'High Court (England & Wales) Admiralty Court',\n",
    " 'High Court (England & Wales) Chancery Division',\n",
    " 'High Court (England & Wales) Commercial Court',\n",
    " 'High Court (England & Wales) Family Division',\n",
    " 'High Court (England & Wales) Intellectual Property Enterprise Court',\n",
    " \"High Court (England & Wales) King's/Queen's Bench Division\",\n",
    " 'High Court (England & Wales) Mercantile Court',\n",
    " 'High Court (England & Wales) Patents Court',\n",
    " 'High Court (England & Wales) Senior Courts Costs Office',\n",
    " 'High Court (England & Wales) Technology and Construction Court'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "035a5c8b-f936-4100-9ddf-b7547e4c5d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define format functions for courts choice, and GPT questions\n",
    "\n",
    "#auxiliary lists and variables\n",
    "uk_courts ={'United Kingdom Supreme Court': 'uksc',\n",
    "'Privy Council': 'ukpc',  \n",
    "'Court of Appeal Civil Division': 'ewca/civ', \n",
    " 'Court of Appeal Criminal Division':  'ewca/crim',  \n",
    "'High Court (England & Wales) Administrative Court': 'ewhc/admin',\n",
    "'High Court (England & Wales) Admiralty Court': 'ewhc/admlty',  \n",
    "'High Court (England & Wales) Chancery Division': 'ewhc/ch',  \n",
    "'High Court (England & Wales) Commercial Court': 'ewhc/comm',  \n",
    "'High Court (England & Wales) Family Division': 'ewhc/fam',  \n",
    "'High Court (England & Wales) Intellectual Property Enterprise Court': 'ewhc/ipec',  \n",
    "\"High Court (England & Wales) King's/Queen's Bench Division\" : 'ewhc/kb',\n",
    "'High Court (England & Wales) Mercantile Court': 'ewhc/mercantile',  \n",
    "'High Court (England & Wales) Patents Court': 'ewhc/pat',  \n",
    "'High Court (England & Wales) Senior Courts Costs Office': 'ewhc/scco',  \n",
    "'High Court (England & Wales) Technology and Construction Court': 'ewhc/tcc',  \n",
    "'Court of Protection': 'ewcop',  \n",
    "'Family Court': 'ewfc',  \n",
    "'Employment Appeal Tribunal': 'eat',  \n",
    "'Administrative Appeals Chamber': 'ukut/aac',  \n",
    "'Immigration and Asylum Chamber': 'ukut/iac',\n",
    "'Lands Chamber': 'ukut/lc',  \n",
    "'Tax and Chancery Chamber': 'ukut/tcc',  \n",
    "'General Regulatory Chamber': 'ukftt/grc',  \n",
    "'Tax Chamber' : 'ukftt/tc'\n",
    "}\n",
    "\n",
    "uk_courts_list = list(uk_courts.keys())\n",
    "\n",
    "def uk_court_choice(chosen_list):\n",
    "\n",
    "    chosen_indice = []\n",
    "\n",
    "    if isinstance(chosen_list, str):\n",
    "        chosen_list = ast.literal_eval(chosen_list)\n",
    "\n",
    "    for i in chosen_list:\n",
    "        chosen_indice.append(uk_courts[i])\n",
    "    \n",
    "    return chosen_indice\n",
    "\n",
    "#Tidy up hyperlink\n",
    "def uk_link(x):\n",
    "    y =str(x).replace('.uk/id', '.uk')\n",
    "    value = '=HYPERLINK(\"' + y + '\")'\n",
    "    return value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2a0e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function turning search terms to search results url\n",
    "\n",
    "#@st.cache_data(show_spinner = False)\n",
    "def uk_search(query= '', \n",
    "              from_day= '',\n",
    "              from_month='', \n",
    "              from_year='', \n",
    "              to_day='', \n",
    "              to_month='', \n",
    "              to_year='', \n",
    "              court = [], \n",
    "              party = '', \n",
    "              judge = ''\n",
    "             ):\n",
    "    base_url = \"https://caselaw.nationalarchives.gov.uk/judgments/search?per_page=50&order=relevance\"\n",
    "    params = {'query' : query, \n",
    "              'from_date_0' : from_day,\n",
    "              'from_date_1' : from_month, \n",
    "              'from_date_2' : from_year, \n",
    "              'to_date_0' : to_day, \n",
    "              'to_date_1' : to_month, \n",
    "              'to_date_2' : to_year, \n",
    "              'court' : court, \n",
    "              'party' : party, \n",
    "              'judge' : judge}\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    #Get results count\n",
    "\n",
    "    results_count = 0\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(response.content, \"lxml\")\n",
    "        results_count_raw = soup.find('p', {'class': \"results__results-intro\"})\n",
    "        results_count_cleaned = results_count_raw.get_text(strip = True)\n",
    "        results_count = int(float(results_count_cleaned.split(' ')[0].replace(',', '')))\n",
    "    except:\n",
    "        print('No results found.')\n",
    "\n",
    "\n",
    "    #Get soup\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "\n",
    "    \n",
    "    return {'results_url': response.url, 'results_count': results_count, 'soup': soup}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6321d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 13:34:32.983 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "#Define function turning search results url to case_infos to judgments\n",
    "\n",
    "#@st.cache_data(show_spinner = False)\n",
    "def uk_search_results_to_judgment_links(_soup, judgment_counter_bound):\n",
    "    #Reponse is from scraping per uk_search\n",
    "    \n",
    "    hrefs = _soup.find_all('span', {'class': 'judgment-listing__judgment'})\n",
    "    case_infos = []\n",
    "\n",
    "    #Get total number of pages\n",
    "    page_nums_raw = _soup.find_all('li', attrs={'class': 'pagination__list-item'})\n",
    "    page_nums = []\n",
    "    \n",
    "    for page_num in page_nums_raw:\n",
    "        try:\n",
    "            if ('Previous' not in page_num.get_text()) and ('Next' not in page_num.get_text()):\n",
    "                \n",
    "                page_nums.append(page_num)\n",
    "    \n",
    "        except:\n",
    "            print('No new page')\n",
    "    \n",
    "    if len(page_nums) > 1:\n",
    "        \n",
    "        page_total = int(page_nums[-1].get_text().split('Page')[1].split('\\n')[0])\n",
    "    \n",
    "    else:\n",
    "        page_total = 1\n",
    "    \n",
    "    #Start counter\n",
    "    \n",
    "    counter = 1\n",
    "    \n",
    "    for link in hrefs:\n",
    "        if counter <= judgment_counter_bound:\n",
    "\n",
    "            case_info = {\n",
    "            'Case name': '',\n",
    "             'Medium neutral citation': '',\n",
    "            'Hyperlink to The National Archives' : '', \n",
    "            'Date' : '',\n",
    "            'Court' : ''\n",
    "            }\n",
    "            try:\n",
    "                raw_link = link.find('a', href=True)['href']\n",
    "                \n",
    "                if \"?\" in raw_link:\n",
    "                    cleaned_link = raw_link.split('?')[0]\n",
    "                else:\n",
    "                    cleaned_link = raw_link\n",
    "                \n",
    "                link_direct = f'https://caselaw.nationalarchives.gov.uk{cleaned_link}/data.xml'\n",
    "                case_info['Hyperlink to The National Archives'] = link_direct\n",
    "\n",
    "                \n",
    "                title_raw = link.find('span', {'class': \"judgment-listing__title\"})\n",
    "                title = title_raw.get_text(strip = True)\n",
    "                \n",
    "                case_info['Case name'] = title\n",
    "\n",
    "                \n",
    "                court_raw = link.find('span', {'class': \"judgment-listing__court\"})\n",
    "                court = court_raw.get_text(strip = True)\n",
    "\n",
    "                case_info['Court'] = court\n",
    "\n",
    "                mnc_raw = link.find('span', {'class': \"judgment-listing__neutralcitation\"})\n",
    "                mnc = mnc_raw.get_text(strip = True)\n",
    "\n",
    "                case_info['Medium neutral citation'] = mnc\n",
    "\n",
    "                date_raw = link.find('time', {'class': \"judgment-listing__date\"})\n",
    "                date = date_raw.get_text(strip = True)\n",
    "\n",
    "                case_info['Date'] = date\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"{case_info['Case name']}: Can't get metadata\")\n",
    "                print(e)\n",
    "            \n",
    "            case_infos.append(case_info)\n",
    "            \n",
    "            counter = counter + 1\n",
    "\n",
    "    if page_total > 1:  \n",
    "    \n",
    "        for page_ending in range(page_total):\n",
    "            \n",
    "            if counter <=judgment_counter_bound:\n",
    "\n",
    "                pause.seconds(np.random.randint(10, 20))\n",
    "\n",
    "                url_next_page = url_search_results + f\"&page={page_ending + 1}\"\n",
    "                \n",
    "                page_judgment_next_page = requests.get(url_next_page)\n",
    "                soup_judgment_next_page = BeautifulSoup(page_judgment_next_page.content, \"lxml\")\n",
    "        \n",
    "                #Check if stll more results\n",
    "                if 'No results have been found' not in str(soup_judgment_next_page):\n",
    "                    hrefs_next_page = soup_judgment_next_page.find_all('span', {'class': 'judgment-listing__judgment'})\n",
    "                    for extra_link in hrefs_next_page:\n",
    "                        if counter <= judgment_counter_bound:\n",
    "\n",
    "                            case_info = {\n",
    "                            'Case name': '',\n",
    "                             'Medium neutral citation': '',\n",
    "                            'Hyperlink to The National Archives' : '', \n",
    "                            'Date' : '',\n",
    "                            'Court' : ''\n",
    "                            }\n",
    "                            \n",
    "                            try:\n",
    "                                raw_link = extra_link.find('a', href=True)['href']\n",
    "                                \n",
    "                                if \"?\" in raw_link:\n",
    "                                    cleaned_link = raw_link.split('?')[0]\n",
    "                                else:\n",
    "                                    cleaned_link = raw_link\n",
    "                                \n",
    "                                link_direct = f'https://caselaw.nationalarchives.gov.uk{cleaned_link}/data.xml'\n",
    "                                case_info['Hyperlink to The National Archives'] = link_direct\n",
    "                \n",
    "                                \n",
    "                                title_raw = link.find('span', {'class': \"judgment-listing__title\"})\n",
    "                                title = title_raw.get_text(strip = True)\n",
    "                                \n",
    "                                case_info['Case name'] = title\n",
    "                \n",
    "                                \n",
    "                                court_raw = link.find('span', {'class': \"judgment-listing__court\"})\n",
    "                                court = court_raw.get_text(strip = True)\n",
    "                \n",
    "                                case_info['Court'] = court\n",
    "                \n",
    "                                mnc_raw = link.find('span', {'class': \"judgment-listing__neutralcitation\"})\n",
    "                                mnc = mnc_raw.get_text(strip = True)\n",
    "                \n",
    "                                case_info['Medium neutral citation'] = mnc\n",
    "                \n",
    "                                date_raw = link.find('time', {'class': \"judgment-listing__date\"})\n",
    "                                date = date_raw.get_text(strip = True)\n",
    "                \n",
    "                                case_info['Date'] = date\n",
    "                \n",
    "                            except Exception as e:\n",
    "                                print(f\"{case_info['Case name']}: Can't get metadata\")\n",
    "                                print(e)\n",
    "\n",
    "                            case_infos.append(case_info)\n",
    "                            \n",
    "                            counter = counter + 1\n",
    "    \n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    \n",
    "    return case_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86a85223-64ca-4d1b-998c-1c2769c5a3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 13:34:34.851 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "#Meta labels and judgment combined\n",
    "\n",
    "uk_meta_labels_droppable = ['Date', \n",
    "                         'Court', \n",
    "                         'Case number', \n",
    "                         'Judge(s) (non-exhaustiveive)', \n",
    "                         'Parties', \n",
    "                         'Header'\n",
    "                        ]\n",
    "\n",
    "#@st.cache_data(show_spinner = False)\n",
    "def uk_meta_judgment_dict(judgment_url_xml):\n",
    "\n",
    "    judgment_dict = {'Case name': '',\n",
    "                 'Medium neutral citation': '',\n",
    "                'Hyperlink to The National Archives' : '', \n",
    "                'Date' : '',\n",
    "                'Court' : '', \n",
    "                'Case number': '',\n",
    "                'Judge(s) (non-exhaustiveive)' : [], \n",
    "                'Parties' : [],\n",
    "                'Header' : '',\n",
    "                'judgment': ''\n",
    "                }\n",
    "\n",
    "    #Get metadata\n",
    "\n",
    "    try:\n",
    "        page = requests.get(judgment_url_xml)\n",
    "        soup = BeautifulSoup(page.content, \"lxml\")\n",
    "    \n",
    "        judgment_dict['Case name'] = soup.find(\"frbrname\")['value']\n",
    "        judgment_dict['Medium neutral citation'] = soup.find(\"uk:cite\").getText()\n",
    "        judgment_dict['Hyperlink to The National Archives'] = uk_link(soup.find(\"frbruri\")['value'])\n",
    "        judgment_dict['Date'] = soup.find(\"frbrdate\")['date']\n",
    "        judgment_dict['Court'] = soup.find(\"uk:court\").getText()\n",
    "        judgment_dict['Header'] = soup.find('header').getText()\n",
    "        \n",
    "        if judgment_dict['Header'][0:1] == '\\n':\n",
    "            judgment_dict['Header'] = judgment_dict['Header'][1: ]\n",
    "            \n",
    "        judgment_dict['Case number'] = soup.find(\"docketnumber\").getText()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for person in soup.find_all(\"tlcperson\"):\n",
    "        if 'judge' in str(person):\n",
    "            judgment_dict['Judge(s) (non-exhaustiveive)'].append(person[\"showas\"])\n",
    "        else:\n",
    "            judgment_dict['Parties'].append(person[\"showas\"])\n",
    "\n",
    "    #Get judgment\n",
    "\n",
    "    pause.seconds(np.random.randint(5, 10))\n",
    "\n",
    "    try:\n",
    "        html_link = judgment_url_xml.replace('/data.xml', '')\n",
    "        page_html = requests.get(html_link)\n",
    "        soup_html = BeautifulSoup(page_html.content, \"lxml\")\n",
    "        \n",
    "        judgment_text = soup_html.get_text(separator=\"\\n\", strip=True)\n",
    "    \n",
    "        try:\n",
    "            before_end_of_doc = judgment_text.split('End of document')[0]\n",
    "            after_skip_to_end = before_end_of_doc.split('Skip to end')[1]\n",
    "            judgment_text = after_skip_to_end\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        judgment_dict['judgment'] = judgment_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"judgment_dict['Case name']: can't scrape judgment\")\n",
    "        \n",
    "    return judgment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae6a6df5-1809-4c65-9055-d51e8ddd1f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uk_search_url(df_master):\n",
    "\n",
    "    df_master = df_master.fillna('')\n",
    "\n",
    "    df_master['Courts'] = df_master['Courts'].apply(uk_court_choice)\n",
    "    \n",
    "    #Combining catchwords into new column\n",
    "    \n",
    "    #Conduct search\n",
    "    \n",
    "    results_url_count = uk_search(query= df_master.loc[0, 'Free text'], \n",
    "                                   from_day= df_master.loc[0, 'From day'],\n",
    "                                   from_month=df_master.loc[0, 'From month'], \n",
    "                                   from_year=df_master.loc[0, 'From year'], \n",
    "                                   to_day=df_master.loc[0, 'To day'], \n",
    "                                   to_month=df_master.loc[0, 'To month'], \n",
    "                                   to_year=df_master.loc[0, 'To year'], \n",
    "                                   court= df_master.loc[0, 'Courts'], \n",
    "                                   party = df_master.loc[0, 'Party'], \n",
    "                                   judge = df_master.loc[0, 'Judge']\n",
    "                                  )\n",
    "\n",
    "    results_url = results_url_count['results_url']\n",
    "\n",
    "    results_count = results_url_count['results_count']\n",
    "\n",
    "    search_results_soup = results_url_count['soup']\n",
    "    \n",
    "    return {'results_url': results_url, 'results_count': results_count, 'soup': search_results_soup}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a2d594",
   "metadata": {},
   "source": [
    "# GPT functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c19fca0c-c315-45ce-b07a-f3cfc9950737",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gpt_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Import functions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_by_line, GPT_label_dict, is_api_key_valid, gpt_input_cost, gpt_output_cost, tokens_cap, max_output, num_tokens_from_string, judgment_prompt_json, GPT_json, engage_GPT_json  \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Import variables\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m question_characters_bound, default_judgment_counter_bound, role_content\u001b[38;5;66;03m#, intro_for_GPT\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gpt_functions'"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from functions.gpt_functions import split_by_line, GPT_label_dict, is_api_key_valid, gpt_input_cost, gpt_output_cost, tokens_cap, max_output, num_tokens_from_string, judgment_prompt_json, GPT_json, engage_GPT_json  \n",
    "#Import variables\n",
    "from functions.gpt_functions import question_characters_bound, role_content#, intro_for_GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809fd5a6-dd7a-42ee-9992-400d9b23909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For checking questions and answers\n",
    "from functions.common_functions import check_questions_answers\n",
    "\n",
    "from functions.gpt_functions import questions_check_system_instruction, GPT_questions_check, checked_questions_json, answers_check_system_instruction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfedd057-c5e3-4cb4-ad49-54144c16ab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jurisdiction specific instruction\n",
    "system_instruction = role_content\n",
    "\n",
    "intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83981e9d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Obtain parameters\n",
    "\n",
    "#@st.cache_data(show_spinner = False)\n",
    "def uk_run(df_master):\n",
    "    df_master = df_master.fillna('')\n",
    "\n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    df_master['Enter your questions for GPT'] = df_master['Enter your questions for GPT'][0: question_characters_bound].apply(split_by_line)\n",
    "    df_master['questions_json'] = df_master['Enter your questions for GPT'].apply(GPT_label_dict)\n",
    "    df_master['Courts'] = df_master['Courts'].apply(uk_court_choice)\n",
    "    \n",
    "    #Create judgments file\n",
    "    judgments_file = []\n",
    "    \n",
    "    #Conduct search\n",
    "    \n",
    "    search_results_soup = uk_search(query= df_master.loc[0, 'Free text'], \n",
    "                                   from_day= df_master.loc[0, 'From day'],\n",
    "                                   from_month=df_master.loc[0, 'From month'], \n",
    "                                   from_year=df_master.loc[0, 'From year'], \n",
    "                                   to_day=df_master.loc[0, 'To day'], \n",
    "                                   to_month=df_master.loc[0, 'To month'], \n",
    "                                   to_year=df_master.loc[0, 'To year'], \n",
    "                                   court= df_master.loc[0, 'Courts'], \n",
    "                                   party = df_master.loc[0, 'Party'], \n",
    "                                   judge = df_master.loc[0, 'Judge']\n",
    "                                  )['soup']\n",
    "        \n",
    "    judgments_counter_bound = int(df_master.loc[0, 'Maximum number of judgments'])\n",
    "\n",
    "    case_infos = uk_search_results_to_judgment_links(search_results_soup, judgments_counter_bound)\n",
    "\n",
    "    for case_info in case_infos:\n",
    "\n",
    "        judgment_dict = uk_meta_judgment_dict(case_info['Hyperlink to The National Archives'])\n",
    "        judgments_file.append(judgment_dict)\n",
    "        pause.seconds(np.random.randint(10, 20))\n",
    "    \n",
    "    #Create and export json file with search results\n",
    "    json_individual = json.dumps(judgments_file, indent=2)\n",
    "    \n",
    "    df_individual = pd.read_json(json_individual)\n",
    "\n",
    "    #For UK, convert date to string so as to avoid Excel producing random numbers for dates\n",
    "    if 'Date' in df_individual.columns:\n",
    "        df_individual['Date'] = df_individual['Date'].astype(str)\n",
    "    \n",
    "    #GPT model\n",
    "\n",
    "    if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "        gpt_model = \"gpt-4o-2024-08-06\"\n",
    "    else:        \n",
    "        gpt_model = \"gpt-4o-mini\"\n",
    "    \n",
    "    #apply GPT_individual to each respondent's judgment spreadsheet\n",
    "    \n",
    "    GPT_activation = int(df_master.loc[0, 'Use GPT'])\n",
    "\n",
    "    questions_json = df_master.loc[0, 'questions_json']\n",
    "            \n",
    "    #Engage GPT\n",
    "    df_updated = engage_GPT_json(questions_json = questions_json, df_example = df_master.loc[0, 'Example'], df_individual = df_individual, GPT_activation = GPT_activation, gpt_model = gpt_model, system_instruction = system_instruction)\n",
    "\n",
    "    if 'judgment' in df_updated.columns:\n",
    "        df_updated.pop('judgment')\n",
    "\n",
    "    #Drop metadata if not wanted\n",
    "\n",
    "    if int(df_master.loc[0, 'Metadata inclusion']) == 0:\n",
    "        for meta_label in uk_meta_labels_droppable:\n",
    "            try:\n",
    "                df_updated.pop(meta_label)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return df_updated"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
