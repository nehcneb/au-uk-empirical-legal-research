{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b8b879",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2312235",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Preliminary modules\n",
    "import base64 \n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "from dateutil.relativedelta import *\n",
    "from datetime import timedelta\n",
    "import sys\n",
    "import pause\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "from math import ceil\n",
    "\n",
    "#Conversion to text\n",
    "import fitz\n",
    "#from io import StringIO\n",
    "from io import BytesIO\n",
    "import pdf2image\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import mammoth\n",
    "\n",
    "#Streamlit\n",
    "import streamlit as st\n",
    "#from streamlit_gsheets import GSheetsConnection\n",
    "from streamlit.components.v1 import html\n",
    "import streamlit_ext as ste\n",
    "from streamlit_extras.stylable_container import stylable_container\n",
    "\n",
    "#OpenAI\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "#Google\n",
    "#from google.oauth2 import service_account\n",
    "\n",
    "#Excel\n",
    "from pyxlsb import open_workbook as open_xlsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "651ed485-489f-4c0c-9412-06d15e64dfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By default, users are allowed to use their own account\n"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from functions.common_functions import own_account_allowed, convert_df_to_json, convert_df_to_csv, convert_df_to_excel, str_to_int, str_to_int_page, save_input\n",
    "#Import variables\n",
    "from functions.common_functions import today_in_nums, errors_list, scraper_pause_mean, default_judgment_counter_bound, default_page_bound, truncation_note\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d6279-572c-4595-8272-f496cc434635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Page bound\n",
    "\n",
    "default_page_bound = 100\n",
    "\n",
    "print(f\"\\nThe maximum number of pages per file is {default_page_bound}.\")\n",
    "\n",
    "#if 'page_bound' not in st.session_state:\n",
    "    #st.session_state['page_bound'] = default_page_bound\n",
    "\n",
    "#Default file counter bound\n",
    "\n",
    "default_file_counter_bound = default_judgment_counter_bound\n",
    "\n",
    "#if 'file_counter_bound' not in st.session_state:\n",
    "    #st.session_state['file_counter_bound'] = default_file_counter_bound\n",
    "\n",
    "print(f\"The default number of files to scrape per request is capped at {default_file_counter_bound}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e819e27",
   "metadata": {},
   "source": [
    "# Functions for Own Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e330af89-11fb-42ec-88fb-ef40580028eb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#File types and languages for processing\n",
    "doc_types = [\"pdf\", \"txt\", 'docx', \"xps\", \"epub\", \"mobi\", 'cs', 'xml', 'html', 'json'] #\"fb2\", \"cbz\", \"svg\",\n",
    "image_types = [\"pdf\", \"jpg\", \"jpeg\", \"png\", \"bmp\", \"gif\", \"tiff\"] #, \"pnm\", \"pgm\", \"pbm\", \"ppm\", \"pam\", \"jxr\", \"jpx\", \"jp2\", \"psd\"]\n",
    "languages_dict = {'English': 'eng', \n",
    "                  'English, Middle (1100-1500)': 'enm', \n",
    "                  'Chinese - Simplified': 'chi_sim', \n",
    "                  'Chinese - Traditional': 'chi_tra', \n",
    "                  'French': 'fra', \n",
    "                  'German' : 'deu',\n",
    "                  'Greek, Modern (1453-)': 'ell', \n",
    "                  'Greek, Ancient (-1453)': 'grc', \n",
    "                  'Hebrew' : 'heb', \n",
    "                  'Hindi' : 'hin', \n",
    "                  'Hungarian': 'hun', \n",
    "                  'Indonesian': 'ind', \n",
    "                  'Italian': 'ita', \n",
    "                  'Italian - Old': 'ita_old', \n",
    "                  'Japanese': 'jpn', \n",
    "                  'Korean': 'kor', \n",
    "                  'Malay': 'msa', \n",
    "                  'Panjabi; Punjabi': 'pan', \n",
    "                  'Polish': 'pol', \n",
    "                  'Portuguese': 'por', \n",
    "                  'Russian': 'rus', \n",
    "                  'Spanish; Castilian': 'spa', \n",
    "                  'Spanish; Castilian - Old': 'spa_old', \n",
    "                  'Swedish': 'swe', \n",
    "                  'Thai': 'tha', \n",
    "                  'Turkish': 'tur', \n",
    "                  'Uighur; Uyghur': 'uig', \n",
    "                  'Ukrainian': 'ukr', \n",
    "                  'Vietnamese': 'vie', \n",
    "                  'Yiddish': 'yid'\n",
    "                 }\n",
    "languages_list = list(languages_dict.keys())\n",
    "\n",
    "#languages_words = ', '.join(languages_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "035a5c8b-f936-4100-9ddf-b7547e4c5d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define format functions for GPT questions    \n",
    "\n",
    "#Create function to split a string into a list by line\n",
    "def split_by_line(x):\n",
    "    y = x.split('\\n')\n",
    "    for i in y:\n",
    "        if len(i) == 0:\n",
    "            y.remove(i)\n",
    "    return y\n",
    "\n",
    "#Create function to split a list into a dictionary for list items longer than 10 characters\n",
    "\n",
    "#Apply split_by_line() before the following function\n",
    "def GPT_label_dict(x_list):\n",
    "    GPT_dict = {}\n",
    "    for i in x_list:\n",
    "        if len(i) > 10:\n",
    "            GPT_index = x_list.index(i) + 1\n",
    "            i_label = 'GPT question ' + f'{GPT_index}'\n",
    "            GPT_dict.update({i_label: i})\n",
    "    return GPT_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b4492d7-d779-44f2-bf47-aafce3baa108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert each uploaded file to file name, text\n",
    "\n",
    "@st.cache_data(show_spinner = False)\n",
    "def doc_to_text(uploaded_doc, language, page_bound):\n",
    "    file_triple = {'File name' : '', 'Language choice': language, 'Page length': '', 'Extracted text': ''}\n",
    "\n",
    "    try:\n",
    "        #Get file name\n",
    "        file_triple['File name']=uploaded_doc.name\n",
    "        \n",
    "        #Get file data\n",
    "        bytes_data = uploaded_doc.getvalue()\n",
    "    \n",
    "        #Get file extension\n",
    "        extension = file_triple['File name'].split('.')[-1].lower()\n",
    "    \n",
    "        #Create list of pages\n",
    "        text_list = []\n",
    "    \n",
    "        #Word format\n",
    "        if extension == 'docx':\n",
    "            doc_string = mammoth.convert_to_html(BytesIO(bytes_data)).value\n",
    "            text_list.append(doc_string)\n",
    "    \n",
    "            file_triple['Page length'] = 1\n",
    "            \n",
    "        else:\n",
    "            #text formats\n",
    "            if extension in ['txt', 'cs', 'xml', 'html', 'json']:\n",
    "                doc = fitz.open(stream=bytes_data, filetype=\"txt\")\n",
    "    \n",
    "            #Other formats\n",
    "            else:\n",
    "                doc = fitz.open(stream=bytes_data)\n",
    "    \n",
    "            max_doc_number=min(len(doc), page_bound)\n",
    "            \n",
    "            for page_index in list(range(0, max_doc_number)):\n",
    "                page = doc.load_page(page_index)\n",
    "                text_page = page.get_text() \n",
    "                text_list.append(text_page)\n",
    "    \n",
    "            #Length of pages\n",
    "            file_triple['Page length'] = len(doc)\n",
    "    \n",
    "        file_triple['Extracted text'] = str(text_list)\n",
    "    except Exception as e:\n",
    "        print(f\"{file_triple['File name']}: failed to get text\")\n",
    "        print(e)\n",
    "    \n",
    "    return file_triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9087b68-2c2b-4240-b04a-a02bf3580a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for images to text\n",
    "\n",
    "@st.cache_data(show_spinner = False)\n",
    "def image_to_text(uploaded_image, language, page_bound):\n",
    "    file_triple = {'File name' : '', 'Language choice': language, 'Page length': '', 'Extracted text': ''}\n",
    "\n",
    "    try:\n",
    "        #Get file name\n",
    "        file_triple['File name']=uploaded_image.name\n",
    "    \n",
    "        #Get file data\n",
    "        bytes_data = uploaded_image.read()\n",
    "    \n",
    "        #Get file extension\n",
    "        extension = file_triple['File name'].split('.')[-1].lower()\n",
    "    \n",
    "        #Obtain images from uploaded file\n",
    "        if extension == 'pdf':\n",
    "            try:\n",
    "                images = pdf2image.convert_from_bytes(bytes_data, timeout=30)\n",
    "            except PDFPopplerTimeoutError as pdf2image_timeout_error:\n",
    "                print(f\"pdf2image error: {pdf2image_timeout_error}.\")\n",
    "    \n",
    "        else:\n",
    "            images = []\n",
    "            image_raw = Image.open(BytesIO(bytes_data))\n",
    "            images.append(image_raw)\n",
    "            \n",
    "        #Extract text from images\n",
    "        text_list = []\n",
    "        \n",
    "        max_images_number=min(len(images), page_bound)\n",
    "    \n",
    "        for image in images[ : max_images_number]:\n",
    "            try:\n",
    "                text_page = pytesseract.image_to_string(image, lang=languages_dict[language], timeout=30)\n",
    "                text_list.append(text_page)\n",
    "                \n",
    "            except RuntimeError as pytesseract_timeout_error:\n",
    "                print(f\"pytesseract error: {pytesseract_timeout_error}.\")\n",
    "    \n",
    "        file_triple['Extracted text'] = str(text_list)\n",
    "    \n",
    "        #Length of pages\n",
    "        file_triple['Page length'] = len(images)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"{file_triple['File name']}: failed to get text\")\n",
    "        print(e)\n",
    "        \n",
    "    return file_triple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a2d594",
   "metadata": {},
   "source": [
    "# GPT functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bfae1df-142c-4583-ab1f-fd94500499b1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gpt_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Import functions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_by_line, GPT_label_dict, is_api_key_valid, gpt_input_cost, gpt_output_cost, tokens_cap, num_tokens_from_string  \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Import variables\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m question_characters_bound, default_judgment_counter_bound\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gpt_functions'"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from functions.gpt_functions import split_by_line, GPT_label_dict, is_api_key_valid, gpt_input_cost, gpt_output_cost, tokens_cap, max_output, num_tokens_from_string  \n",
    "#Import variables\n",
    "from functions.gpt_functions import question_characters_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab06b309-e9be-495e-96fd-1073b25291c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Questions for GPT are capped at {question_characters_bound} characters.\\n\")\n",
    "print(f\"The default number of files to scrape per request is capped at {default_file_counter_bound}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef483929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_prompt(file_triple, gpt_model):\n",
    "                \n",
    "    file_content = 'Based on the following document:  \"\"\"'+ file_triple['Extracted text'] + '\"\"\"'\n",
    "\n",
    "    file_content_tokens = num_tokens_from_string(file_content, \"cl100k_base\")\n",
    "    \n",
    "    if file_content_tokens <= tokens_cap(gpt_model):\n",
    "        \n",
    "        return file_content\n",
    "\n",
    "    else:\n",
    "                \n",
    "        file_chars_capped = int(tokens_cap(gpt_model)*4)\n",
    "        \n",
    "        #Keep first x characters rather than cut out the middle\n",
    "        file_string_trimmed = file_triple['Extracted text'][ : int(file_chars_capped)]\n",
    "\n",
    "        #If want to cut out the middle instead\n",
    "#        file_string_trimmed = file_triple['Extracted text'][ :int(file_chars_capped/2)] + file_triple['Extracted text'][-int(file_chars_capped/2): ]\n",
    "        \n",
    "        file_content_capped = 'Based on the following document:  \"\"\"'+ file_string_trimmed + '\"\"\"'\n",
    "        \n",
    "        return file_content_capped\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f7af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define system role content for GPT\n",
    "role_content_own = 'You are a legal research assistant helping an academic researcher to answer questions about a file. The file may be a document or an image. You will be provided with the file. Please answer questions based only on information contained in the file. Where your answer comes from a part of the file, include a reference to that part of the file. If you cannot answer the questions based on the file, do not make up information, but instead write \"answer not found\".'\n",
    "\n",
    "system_instruction = role_content_own\n",
    "\n",
    "intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d95be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT answer function for answers in json form, YES TOKENS\n",
    "#IN USE\n",
    "\n",
    "@st.cache_data(show_spinner = False)\n",
    "def GPT_json_own(questions_json, df_example, file_triple, gpt_model, system_instruction):\n",
    "    #'question_json' variable is a json of questions to GPT\n",
    "\n",
    "    file_for_GPT = [{\"role\": \"user\", \"content\": file_prompt(file_triple, gpt_model)}]\n",
    "\n",
    "    json_direction = [{\"role\": \"user\", \"content\": 'You will be given questions to answer in JSON form.'}]\n",
    "\n",
    "    #Create answer format\n",
    "    answers_json = {}\n",
    "\n",
    "    #st.write(f\"df_example == {df_example}\")\n",
    "    \n",
    "    #st.write(f\"len(df_example) == {len(df_example)}\")\n",
    "\n",
    "    if len(df_example.replace('\"', '')) > 0:\n",
    "\n",
    "        #st.write(f\"df_example == {df_example}\")\n",
    "\n",
    "        #st.write(type(df_example))\n",
    "\n",
    "        try:\n",
    "            \n",
    "            if isinstance(df_example, str):\n",
    "                \n",
    "                answers_json = json.loads(df_example)\n",
    "\n",
    "            if isinstance(df_example, dict):\n",
    "                \n",
    "                answers_json = df_example\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Example provided but can't produce json to send to GPT.\")\n",
    "            print(e)\n",
    "    \n",
    "    #st.write(f\"answers_json == {answers_json}\")\n",
    "\n",
    "    #Check if answers format succesfully created by following any example uploaded\n",
    "    q_keys = [*questions_json]\n",
    "    \n",
    "    if len(answers_json) == 0:\n",
    "        q_counter = 1\n",
    "        for q_index in q_keys:\n",
    "            answers_json.update({f'GPT question {q_counter}: {questions_json[q_index]}': f'Your answer. (The paragraphs, pages or sections from which you obtained your answer)'})\n",
    "            q_counter += 1\n",
    "\n",
    "    #Create questions, which include the answer format\n",
    "    \n",
    "    question_for_GPT = [{\"role\": \"user\", \"content\": json.dumps(questions_json) + ' Respond in the following JSON form: ' + json.dumps(answers_json)}]\n",
    "    \n",
    "    #Create messages in one prompt for GPT\n",
    "    language_content = f\"The file is written in {file_triple['Language choice']}.\"\n",
    "\n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction + language_content}] \n",
    "\n",
    "    messages_for_GPT = intro_for_GPT + file_for_GPT + json_direction + question_for_GPT\n",
    "    \n",
    "#   return messages_for_GPT\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    try:\n",
    "        #completion = client.chat.completions.create(\n",
    "        completion = openai.chat.completions.create(\n",
    "            model=gpt_model,\n",
    "            messages=messages_for_GPT, \n",
    "            response_format={\"type\": \"json_object\"}, \n",
    "            max_tokens = max_output(gpt_model, messages_for_GPT), \n",
    "            temperature = 0.1, \n",
    "            #top_p = 0.1\n",
    "        )\n",
    "        \n",
    "#        return completion.choices[0].message.content #This gives answers as a string containing a dictionary\n",
    "        \n",
    "        answers_dict = json.loads(completion.choices[0].message.content)\n",
    "        \n",
    "        #Obtain tokens\n",
    "        output_tokens = completion.usage.completion_tokens\n",
    "        \n",
    "        prompt_tokens = completion.usage.prompt_tokens\n",
    "        \n",
    "        return [answers_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "    except Exception as error:\n",
    "        \n",
    "        print('GPT failed to produce answers.')\n",
    "        \n",
    "        for q_index in q_keys:\n",
    "            \n",
    "            answers_json.update({q_index: error})\n",
    "        \n",
    "        return [answers_json, 0, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80714830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT function for each respondent's dataframe, index by file then question, with input and output tokens given by GPT itself\n",
    "#IN USE\n",
    "\n",
    "#The following function DOES NOT check for existence of questions for GPT\n",
    "    # To so check, active line marked as #*\n",
    "\n",
    "@st.cache_data(show_spinner = False)\n",
    "def engage_GPT_json_own(questions_json, df_example, df_individual, GPT_activation, gpt_model, system_instruction):\n",
    "    # Variable questions_json refers to the json of questions\n",
    "    # Variable df_individual refers to each respondent's df\n",
    "    # Variable activation refers to status of GPT activation (real or test)\n",
    "    # The output is a new JSON for the relevant respondent with new columns re:\n",
    "        # f\"Length of first 10 pages in tokens (up to {tokens_cap(gpt_model)} given to GPT)\"\n",
    "        # 'GPT cost estimate (USD excl GST)'\n",
    "        # 'GPT time estimate (seconds)'\n",
    "        # GPT questions/answers\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    for file_index in df_individual.index:\n",
    "        \n",
    "        file_triple = df_individual.to_dict('index')[file_index]\n",
    "\n",
    "        #Check wither error in getting the full text\n",
    "        text_error = False\n",
    "        if 'Extracted text' in file_triple.keys():\n",
    "            if len(file_triple['Extracted text']) == 0:\n",
    "                text_error = True\n",
    "                df_individual.loc[file_index, 'Note'] = search_error_note\n",
    "                print(f\"File indexed {file_index} not sent to GPT given full text was not scrapped.\")\n",
    "        \n",
    "        #Calculate and append number of tokens of file, regardless of whether given to GPT\n",
    "        file_tokens = num_tokens_from_string(str(file_triple), \"cl100k_base\")\n",
    "        df_individual.loc[file_index, f\"Length of first {st.session_state['df_master'].loc[0,'Maximum number of pages per file']} pages in tokens (up to {tokens_cap(gpt_model)} given to GPT)\"] = file_tokens       \n",
    "\n",
    "        #Indicate whether file truncated\n",
    "        if file_tokens > tokens_cap(gpt_model):\n",
    "            df_individual.loc[file_index, 'Note'] = truncation_note\n",
    "\n",
    "        #Create columns for respondent's GPT cost, time\n",
    "        df_individual.loc[file_index, 'GPT cost estimate (USD excl GST)'] = ''\n",
    "        df_individual.loc[file_index, 'GPT time estimate (seconds)'] = ''\n",
    "                \n",
    "        #Calculate GPT start time\n",
    "\n",
    "        GPT_start_time = datetime.now()\n",
    "\n",
    "        #Depending on activation status, apply GPT_json function to each file, gives answers as a string containing a dictionary\n",
    "\n",
    "        if ((int(GPT_activation) > 0) and (text_error == False)):\n",
    "            GPT_file_triple = GPT_json_own(questions_json, df_example, file_triple, gpt_model, system_instruction) #Gives [answers as a JSON, output tokens, input tokens]\n",
    "            answers_dict = GPT_file_triple[0]\n",
    "\n",
    "            #Calculate and append GPT finish time and time difference to individual df\n",
    "            GPT_finish_time = datetime.now()\n",
    "            \n",
    "            GPT_time_difference = GPT_finish_time - GPT_start_time\n",
    "    \n",
    "            df_individual.loc[file_index, 'GPT time estimate (seconds)'] = GPT_time_difference.total_seconds()\n",
    "\n",
    "        else:\n",
    "            answers_dict = {}    \n",
    "            \n",
    "            question_keys = [*questions_json]\n",
    "\n",
    "            for q_index in question_keys:\n",
    "                #Increases file index by 2 to ensure consistency with Excel spreadsheet\n",
    "                answer = ''\n",
    "                answers_dict.update({questions_json[q_index]: answer})\n",
    "            \n",
    "            #Own calculation of GPT costs for mock answers\n",
    "\n",
    "            #Calculate capped file tokens\n",
    "\n",
    "            file_capped_tokens = num_tokens_from_string(file_prompt(file_triple, gpt_model), \"cl100k_base\")\n",
    "\n",
    "            #Calculate questions tokens and cost\n",
    "\n",
    "            questions_tokens = num_tokens_from_string(json.dumps(questions_json), \"cl100k_base\")\n",
    "\n",
    "            #Calculate other instructions' tokens\n",
    "\n",
    "            other_instructions = system_instruction + 'The file is written in some language' + 'you will be given questions to answer in JSON form.' + ' Respond in the following JSON form: '\n",
    "\n",
    "            other_tokens = num_tokens_from_string(other_instructions, \"cl100k_base\") + len(question_keys)*num_tokens_from_string(\"GPT question x:  Your answer to the question with index GPT question x. State specific page numbers or sections of the file.\", \"cl100k_base\")\n",
    "\n",
    "            #Calculate number of tokens of answers\n",
    "            answers_output_tokens = num_tokens_from_string(str(answers_dict), \"cl100k_base\")\n",
    "\n",
    "            answers_input_tokens = file_capped_tokens + questions_tokens + other_tokens\n",
    "            \n",
    "            GPT_file_triple = [answers_dict, answers_output_tokens, answers_input_tokens]\n",
    "\n",
    "        #Create GPT question headings and append answers to individual spreadsheets\n",
    "        for answer_index in answers_dict.keys():\n",
    "\n",
    "            #Check any errors\n",
    "            answer_string = str(answers_dict[answer_index]).lower()\n",
    "            \n",
    "            if ((answer_string.startswith('your answer.')) or (answer_string.startswith('your response.'))):\n",
    "                \n",
    "                answers_dict[answer_index] = 'Error. Please try a different question or GPT model.'\n",
    "\n",
    "            #Append answer to spreadsheet\n",
    "\n",
    "            answer_header = answer_index\n",
    "\n",
    "            try:\n",
    "            \n",
    "                df_individual.loc[file_index, answer_header] = answers_dict[answer_index]\n",
    "\n",
    "            except:\n",
    "\n",
    "                df_individual.loc[file_index, answer_header] = str(answers_dict[answer_index])\n",
    "            \n",
    "        #Calculate GPT costs\n",
    "\n",
    "        GPT_cost = GPT_file_triple[1]*gpt_output_cost(gpt_model) + GPT_file_triple[2]*gpt_input_cost(gpt_model)\n",
    "\n",
    "        #Calculate and append GPT cost to individual df\n",
    "        df_individual.loc[file_index, 'GPT cost estimate (USD excl GST)'] = GPT_cost\n",
    "    \n",
    "    return df_individual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83981e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain parameters\n",
    "\n",
    "@st.cache_data(show_spinner = False)\n",
    "def run_own(df_master, uploaded_docs, uploaded_images):\n",
    "    df_master = df_master.fillna('')\n",
    "\n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    df_master['Enter your questions for GPT'] = df_master['Enter your questions for GPT'][0: question_characters_bound].apply(split_by_line)\n",
    "    df_master['questions_json'] = df_master['Enter your questions for GPT'].apply(GPT_label_dict)\n",
    "    \n",
    "    #Create files file\n",
    "    Files_file = []\n",
    "\n",
    "    #Obtain bounds and language\n",
    "\n",
    "    file_counter_bound = int(df_master.loc[0, 'Maximum number of files'])\n",
    "\n",
    "    page_bound = int(df_master.loc[0,'Maximum number of pages per file'])\n",
    "\n",
    "    language = df_master.loc[0, 'Language choice']\n",
    "    \n",
    "    #Convert uploaded documents to text\n",
    "\n",
    "    file_counter = 1 \n",
    "\n",
    "    for uploaded_doc in uploaded_docs:\n",
    "        if file_counter <= file_counter_bound:\n",
    "            file_triple = doc_to_text(uploaded_doc, language, page_bound)\n",
    "            Files_file.append(file_triple)\n",
    "            file_counter += 1\n",
    "\n",
    "    #Convert uploaded images to text\n",
    "\n",
    "    for uploaded_image in uploaded_images:\n",
    "        if file_counter <= file_counter_bound:\n",
    "            file_triple = image_to_text(uploaded_image, language, page_bound)\n",
    "            Files_file.append(file_triple)\n",
    "            file_counter += 1\n",
    "    \n",
    "    #Create and export json file with search output\n",
    "    json_individual = json.dumps(Files_file, indent=2)\n",
    "    \n",
    "    df_individual = pd.read_json(json_individual)\n",
    "    \n",
    "    #GPT model\n",
    "\n",
    "    #GPT model\n",
    "\n",
    "    if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "        gpt_model = \"gpt-4o-2024-08-06\"\n",
    "    else:        \n",
    "        gpt_model = \"gpt-4o-mini\"\n",
    "        \n",
    "    #apply GPT_individual to each respondent's file spreadsheet\n",
    "    \n",
    "    GPT_activation = int(df_master.loc[0, 'Use GPT'])\n",
    "\n",
    "    questions_json = df_master.loc[0, 'questions_json']\n",
    "        \n",
    "    #Engage GPT    \n",
    "    df_updated = engage_GPT_json_own(questions_json = questions_json, df_example = df_master.loc[0, 'Example'], df_individual = df_individual, GPT_activation = GPT_activation, gpt_model = gpt_model, system_instruction = system_instruction)\n",
    "\n",
    "    if 'Extracted text' in df_updated.columns:\n",
    "        df_updated.pop('Extracted text')\n",
    "    \n",
    "    return df_updated\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caa7ba6-7f30-40a5-a98d-16a956d1f5f6",
   "metadata": {},
   "source": [
    "# For vision, own file only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb2c4cf-70cf-48cb-856b-1df1b044be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import functions\n",
    "from functions.gpt_functions import get_image_dims, calculate_image_token_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed9ee2-0dea-4f6a-840b-bb5ed83eca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache_data(show_spinner = False)\n",
    "def image_to_b64_own(uploaded_image, language, page_bound):\n",
    "    file_triple = {'File name' : '', 'Language choice': language, 'b64_list': [], 'Dimensions (width, height)' : [],\n",
    "                   'Page length': '', 'tokens_raw': 0\n",
    "                  }\n",
    "\n",
    "    try:\n",
    "        file_triple['File name']=uploaded_image.name\n",
    "    \n",
    "        #Get file extension\n",
    "        extension = file_triple['File name'].split('.')[-1].lower()\n",
    "    \n",
    "        bytes_data = uploaded_image.read()\n",
    "    \n",
    "        if extension == 'pdf':\n",
    "            \n",
    "            images = pdf2image.convert_from_bytes(bytes_data, timeout=30, fmt=\"jpeg\")\n",
    "    \n",
    "            file_triple['Page length'] = len(images)\n",
    "    \n",
    "            #Get page bound\n",
    "            max_images_number=min(len(images), page_bound)\n",
    "    \n",
    "            for image in images[ : max_images_number]:\n",
    "    \n",
    "                output = BytesIO()\n",
    "                image.save(output, format='JPEG')\n",
    "                im_data = output.getvalue()\n",
    "                \n",
    "                image_data = base64.b64encode(im_data)\n",
    "                if not isinstance(image_data, str):\n",
    "                    # Python 3, decode from bytes to string\n",
    "                    image_data = image_data.decode()\n",
    "                data_url = 'data:image/jpg;base64,' + image_data\n",
    "    \n",
    "                #b64 = base64.b64encode(image_raw).decode('utf-8')\n",
    "    \n",
    "                b64_to_attach = data_url\n",
    "                #b64_to_attach = f\"data:image/png;base64,{b64}\"\n",
    "    \n",
    "            file_triple['b64_list'].append(b64_to_attach)\n",
    "\n",
    "        else:\n",
    "    \n",
    "            file_triple['Page length'] = 1\n",
    "        \n",
    "            b64 = base64.b64encode(bytes_data).decode('utf-8')\n",
    "        \n",
    "            b64_to_attach = f\"data:image/{extension};base64,{b64}\"\n",
    "            \n",
    "            file_triple['b64_list'].append(b64_to_attach)\n",
    "            \n",
    "        for image_b64 in file_triple['b64_list']:\n",
    "    \n",
    "            #Get dimensions\n",
    "            try:\n",
    "    \n",
    "                file_triple['Dimensions (width, height)'].append(get_image_dims(b64_to_attach))\n",
    "            except Exception as e:\n",
    "                print(f\"Cannot obtain dimensions for {file_triple['File name']}, p {file_triple['b64_list'].index(image_b64)}.\")\n",
    "                print(e)\n",
    "            \n",
    "            file_triple['tokens_raw'] = file_triple['tokens_raw'] + calculate_image_token_cost(image_b64, detail=\"auto\")\n",
    "    except Exception as e:\n",
    "        print(f\"{file_triple['File name']}: failed to get text\")\n",
    "        print(e)\n",
    "        \n",
    "    return file_triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2332d698-126d-4069-8ee3-286af5ab7be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT answer function for answers in json form, YES TOKENS\n",
    "#For gpt-4o vision\n",
    "\n",
    "@st.cache_data(show_spinner = False)\n",
    "def GPT_b64_json_own(questions_json, df_example, file_triple, gpt_model, system_instruction):\n",
    "    #'question_json' variable is a json of questions to GPT\n",
    "\n",
    "    #file_for_GPT = [{\"role\": \"user\", \"content\": file_prompt(file_triple, gpt_model) + 'you will be given questions to answer in JSON form.'}]\n",
    "\n",
    "    #Add images to messages to GPT\n",
    "    image_content_value = [{\"type\": \"text\", \"text\": 'Based on the following images:'}]\n",
    "\n",
    "    for image_b64 in file_triple['b64_list']:\n",
    "        image_message_to_attach = {\"type\": \"image_url\", \"image_url\": {\"url\": image_b64,}}\n",
    "        image_content_value.append(image_message_to_attach)\n",
    "\n",
    "    image_content = [{\"role\": \"user\", \n",
    "                      \"content\": image_content_value\n",
    "                     }\n",
    "                  ]\n",
    "\n",
    "    json_direction = [{\"role\": \"user\", \"content\": 'You will be given questions to answer in JSON form.'}]\n",
    "\n",
    "    file_for_GPT = image_content + json_direction\n",
    "    \n",
    "    #Create answer format\n",
    "    answers_json = {}\n",
    "\n",
    "    #st.write(f\"df_example == {df_example}\")\n",
    "    \n",
    "    #st.write(f\"len(df_example) == {len(df_example)}\")\n",
    "\n",
    "    if len(df_example.replace('\"', '')) > 0:\n",
    "\n",
    "        #st.write(f\"df_example == {df_example}\")\n",
    "\n",
    "        #st.write(type(df_example))\n",
    "\n",
    "        try:\n",
    "            \n",
    "            if isinstance(df_example, str):\n",
    "                \n",
    "                answers_json = json.loads(df_example)\n",
    "\n",
    "            if isinstance(df_example, dict):\n",
    "                \n",
    "                answers_json = df_example\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Example provided but can't produce json to send to GPT.\")\n",
    "            print(e)\n",
    "    \n",
    "    #st.write(f\"answers_json == {answers_json}\")\n",
    "\n",
    "    #Check if answers format succesfully created by following any example uploaded\n",
    "    q_keys = [*questions_json]\n",
    "    \n",
    "    if len(answers_json) == 0:\n",
    "        q_counter = 1\n",
    "        for q_index in q_keys:\n",
    "            answers_json.update({f'GPT question {q_counter}: {questions_json[q_index]}': f'Your answer. (The paragraphs, pages or sections from which you obtained your answer)'})\n",
    "            q_counter += 1\n",
    "\n",
    "    #Create questions, which include the answer format\n",
    "    \n",
    "    question_for_GPT = [{\"role\": \"user\", \"content\": json.dumps(questions_json) + ' Respond in the following JSON form: ' + json.dumps(answers_json)}]\n",
    "    \n",
    "    #Create messages in one prompt for GPT\n",
    "    language_content = f\"The file is written in {file_triple['Language choice']}.\"\n",
    "\n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction + language_content}] \n",
    "\n",
    "    messages_for_GPT = intro_for_GPT + file_for_GPT + question_for_GPT\n",
    "    \n",
    "#   return messages_for_GPT\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    try:\n",
    "        #completion = client.chat.completions.create(\n",
    "        completion = openai.chat.completions.create(\n",
    "            model=gpt_model,\n",
    "            messages=messages_for_GPT, \n",
    "            response_format={\"type\": \"json_object\"}, \n",
    "            temperature = 0.1, \n",
    "            #top_p = 0.1\n",
    "        )\n",
    "        \n",
    "#        return completion.choices[0].message.content #This gives answers as a string containing a dictionary\n",
    "        \n",
    "        answers_dict = json.loads(completion.choices[0].message.content)\n",
    "        \n",
    "        #Obtain tokens\n",
    "        output_tokens = completion.usage.completion_tokens\n",
    "        \n",
    "        prompt_tokens = completion.usage.prompt_tokens\n",
    "        \n",
    "        return [answers_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "    except Exception as error:\n",
    "        \n",
    "        print('GPT failed to produce answers.')\n",
    "        \n",
    "        for q_index in q_keys:\n",
    "            \n",
    "            answers_json.update({q_index: error})\n",
    "        \n",
    "        return [answers_json, 0, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d494f30b-28e6-40fd-bfcb-47e184e2d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT function for each respondent's dataframe, index by file then question, with input and output tokens given by GPT itself\n",
    "#For gpt-4o vision\n",
    "\n",
    "#The following function DOES NOT check for existence of questions for GPT\n",
    "    # To so check, active line marked as #*\n",
    "\n",
    "@st.cache_data(show_spinner = False)\n",
    "def engage_GPT_b64_json_own(questions_json, df_example, df_individual, GPT_activation, gpt_model, system_instruction):\n",
    "    # Variable questions_json refers to the json of questions\n",
    "    # Variable df_individual refers to each respondent's df\n",
    "    # Variable activation refers to status of GPT activation (real or test)\n",
    "    # The output is a new JSON for the relevant respondent with new columns re:\n",
    "        # f\"Length of first 10 pages in tokens (up to {tokens_cap(gpt_model)} given to GPT)\"\n",
    "        # 'GPT cost estimate (USD excl GST)'\n",
    "        # 'GPT time estimate (seconds)'\n",
    "        # GPT questions/answers\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "        \n",
    "    for file_index in df_individual.index:\n",
    "        \n",
    "        file_triple = df_individual.to_dict('index')[file_index]\n",
    "\n",
    "        #Check wither error in getting the full text\n",
    "        text_error = False\n",
    "        if 'Extracted text' in file_triple.keys():\n",
    "            if len(file_triple['Extracted text']) == 0:\n",
    "                text_error = True\n",
    "                df_individual.loc[file_index, 'Note'] = search_error_note\n",
    "                print(f\"File indexed {file_index} not sent to GPT given full text was not scrapped.\")\n",
    "        \n",
    "        #Calculate and append number of tokens of file, regardless of whether given to GPT\n",
    "        df_individual.loc[file_index, f\"Tokens (up to {tokens_cap(gpt_model)} given to GPT)\"] = file_triple['tokens_raw']\n",
    "\n",
    "        #Create columns for respondent's GPT cost, time\n",
    "        df_individual.loc[file_index, 'GPT cost estimate (USD excl GST)'] = ''\n",
    "        df_individual.loc[file_index, 'GPT time estimate (seconds)'] = ''\n",
    "                \n",
    "        #Calculate GPT start time\n",
    "\n",
    "        GPT_start_time = datetime.now()\n",
    "\n",
    "        #Depending on activation status, apply GPT_json function to each file, gives answers as a string containing a dictionary\n",
    "\n",
    "        if ((int(GPT_activation) > 0) and (text_error == False)):\n",
    "            GPT_file_triple = GPT_b64_json_own(questions_json, df_example, file_triple, gpt_model, system_instruction) #Gives [answers as a JSON, output tokens, input tokens]\n",
    "            answers_dict = GPT_file_triple[0]\n",
    "\n",
    "            #Calculate and append GPT finish time and time difference to individual df\n",
    "            GPT_finish_time = datetime.now()\n",
    "            \n",
    "            GPT_time_difference = GPT_finish_time - GPT_start_time\n",
    "    \n",
    "            df_individual.loc[file_index, 'GPT time estimate (seconds)'] = GPT_time_difference.total_seconds()\n",
    "        \n",
    "        else:\n",
    "            answers_dict = {}    \n",
    "            \n",
    "            question_keys = [*questions_json]\n",
    "\n",
    "            for q_index in question_keys:\n",
    "                #Increases file index by 2 to ensure consistency with Excel spreadsheet\n",
    "                answer = ''\n",
    "                answers_dict.update({questions_json[q_index]: answer})\n",
    "            \n",
    "            #Calculate capped file tokens\n",
    "\n",
    "            file_capped_tokens = min(file_triple['tokens_raw'], tokens_cap(gpt_model))\n",
    "\n",
    "            #Calculate questions tokens and cost\n",
    "\n",
    "            questions_tokens = num_tokens_from_string(json.dumps(questions_json), \"cl100k_base\")\n",
    "\n",
    "            #Calculate other instructions' tokens\n",
    "\n",
    "            other_instructions = system_instruction + 'The file is written in some language' + 'you will be given questions to answer in JSON form.' + ' Respond in the following JSON form: '\n",
    "\n",
    "            other_tokens = num_tokens_from_string(other_instructions, \"cl100k_base\") + len(question_keys)*num_tokens_from_string(\"GPT question x:  Your answer to the question with index GPT question x. State specific page numbers or sections of the file.\", \"cl100k_base\")\n",
    "\n",
    "            #Calculate number of tokens of answers\n",
    "            answers_output_tokens = num_tokens_from_string(str(answers_dict), \"cl100k_base\")\n",
    "\n",
    "            answers_input_tokens = file_capped_tokens + questions_tokens + other_tokens\n",
    "            \n",
    "            GPT_file_triple = [answers_dict, answers_output_tokens, answers_input_tokens]\n",
    "\n",
    "        #Create GPT question headings and append answers to individual spreadsheets\n",
    "        for answer_index in answers_dict.keys():\n",
    "\n",
    "            #Check any errors\n",
    "            answer_string = str(answers_dict[answer_index]).lower()\n",
    "            \n",
    "            if ((answer_string.startswith('your answer.')) or (answer_string.startswith('your response.'))):\n",
    "                \n",
    "                answers_dict[answer_index] = 'Error. Please try a different question or GPT model.'\n",
    "\n",
    "            #Append answer to spreadsheet\n",
    "\n",
    "            answer_header = answer_index\n",
    "\n",
    "            try:\n",
    "            \n",
    "                df_individual.loc[file_index, answer_header] = answers_dict[answer_index]\n",
    "\n",
    "            except:\n",
    "\n",
    "                df_individual.loc[file_index, answer_header] = str(answers_dict[answer_index])\n",
    "                \n",
    "        #Calculate GPT costs\n",
    "\n",
    "        GPT_cost = GPT_file_triple[1]*gpt_output_cost(gpt_model) + GPT_file_triple[2]*gpt_input_cost(gpt_model)\n",
    "\n",
    "        #Calculate and append GPT cost to individual df\n",
    "        df_individual.loc[file_index, 'GPT cost estimate (USD excl GST)'] = GPT_cost\n",
    "    \n",
    "    return df_individual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3046c1e-5d6f-463a-92e4-84897573bf5c",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "#For gpt-4o vision\n",
    "\n",
    "@st.cache_data(show_spinner = False)\n",
    "def run_b64_own(df_master, uploaded_images):\n",
    "\n",
    "    df_master = df_master.fillna('')\n",
    "\n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    df_master['Enter your questions for GPT'] = df_master['Enter your questions for GPT'][0: question_characters_bound].apply(split_by_line)\n",
    "    df_master['questions_json'] = df_master['Enter your questions for GPT'].apply(GPT_label_dict)\n",
    "\n",
    "    #Obtain bounds and language\n",
    "\n",
    "    file_counter_bound = int(df_master.loc[0, 'Maximum number of files'])\n",
    "\n",
    "    page_bound = int(df_master.loc[0,'Maximum number of pages per file'])\n",
    "\n",
    "    language = df_master.loc[0, 'Language choice']\n",
    "    \n",
    "    #Convert uploaded documents to b64\n",
    "\n",
    "    file_counter = 1 \n",
    "    \n",
    "    #Create files file\n",
    "    Files_file = []\n",
    "\n",
    "    #Convert images to b64, then send to GPT\n",
    "    for uploaded_image in uploaded_images:\n",
    "        if file_counter <= file_counter_bound:\n",
    "            file_triple = image_to_b64_own(uploaded_image, language, page_bound)\n",
    "            Files_file.append(file_triple)\n",
    "            file_counter += 1\n",
    "\n",
    "    #Create and export json file with search output\n",
    "    json_individual = json.dumps(Files_file, indent=2)\n",
    "    \n",
    "    df_individual = pd.read_json(json_individual)\n",
    "\n",
    "    #Instruct GPT\n",
    "\n",
    "    GPT_activation = int(df_master.loc[0, 'Use GPT'])\n",
    "    \n",
    "    #GPT model\n",
    "\n",
    "    if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "        gpt_model = \"gpt-4o-2024-08-06\"\n",
    "    else:        \n",
    "        gpt_model = \"gpt-4o-mini\"\n",
    "        \n",
    "    #apply GPT_individual to each respondent's judgment spreadsheet\n",
    "\n",
    "    questions_json = df_master.loc[0, 'questions_json']\n",
    "\n",
    "    #apply GPT_individual to each respondent's file spreadsheet    \n",
    "    df_updated = engage_GPT_b64_json_own(questions_json = questions_json, df_example = df_master.loc[0, 'Example'], df_individual = df_individual, GPT_activation = GPT_activation, gpt_model = gpt_model, system_instruction = system_instruction)\n",
    "\n",
    "    #Remove redundant columns\n",
    "\n",
    "    for column in ['tokens_raw', 'b64_list']:\n",
    "        try:\n",
    "            df_updated.pop(column)\n",
    "        except:\n",
    "            print(f\"No {column} column.\")\n",
    "\n",
    "    return df_updated"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
