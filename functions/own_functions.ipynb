{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b8b879",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2312235",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Preliminary modules\n",
    "import base64 \n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "#from dateutil.relativedelta import *\n",
    "from datetime import timedelta\n",
    "import sys\n",
    "import pause\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "from math import ceil\n",
    "import traceback\n",
    "\n",
    "#Conversion to text\n",
    "import pymupdf\n",
    "#import pymupdf4llm\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "import pdf2image\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import mammoth\n",
    "\n",
    "#Streamlit\n",
    "import streamlit as st\n",
    "#from streamlit_gsheets import GSheetsConnection\n",
    "from streamlit.components.v1 import html\n",
    "#import streamlit_ext as ste\n",
    "from streamlit_extras.stylable_container import stylable_container\n",
    "\n",
    "#OpenAI\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "#aws\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "#Google\n",
    "#from google.oauth2 import service_account\n",
    "\n",
    "#Excel\n",
    "from pyxlsb import open_workbook as open_xlsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "651ed485-489f-4c0c-9412-06d15e64dfe0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By default, users are allowed to use their own account\n"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from functions.common_functions import own_account_allowed, pop_judgment, convert_df_to_json, convert_df_to_csv, convert_df_to_excel, str_to_int, str_to_int_page, save_input, send_notification_email, get_aws_s3, aws_df_get, aws_df_put\n",
    "#Import variables\n",
    "from functions.common_functions import today_in_nums, errors_list, scraper_pause_mean, default_judgment_counter_bound, default_page_bound, truncation_note, spinner_text, search_error_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d6279-572c-4595-8272-f496cc434635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Page bound\n",
    "\n",
    "default_page_bound = 100\n",
    "\n",
    "print(f\"\\nThe maximum number of pages per file is {default_page_bound}.\")\n",
    "\n",
    "#if 'page_bound' not in st.session_state:\n",
    "    #st.session_state['page_bound'] = default_page_bound\n",
    "\n",
    "#Default file counter bound\n",
    "\n",
    "default_file_counter_bound = default_judgment_counter_bound\n",
    "\n",
    "#if 'file_counter_bound' not in st.session_state:\n",
    "    #st.session_state['file_counter_bound'] = default_file_counter_bound\n",
    "\n",
    "print(f\"The default number of files to scrape per request is capped at {default_file_counter_bound}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e819e27",
   "metadata": {},
   "source": [
    "# Functions for Own Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97033282-8e80-4793-9ef2-02f41168ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_error_note = 'This app was unable to scrape text from this file. This file was not sent to GPT.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e330af89-11fb-42ec-88fb-ef40580028eb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#File types and languages for processing\n",
    "doc_types = [\"pdf\", \"txt\", 'docx', \"xps\", \"epub\", \"mobi\", 'cs', 'xml', 'html', 'json'] #\"fb2\", \"cbz\", \"svg\",\n",
    "image_types = [\"pdf\", \"jpg\", \"jpeg\", \"png\", \"bmp\", \"gif\", \"tiff\"] #, \"pnm\", \"pgm\", \"pbm\", \"ppm\", \"pam\", \"jxr\", \"jpx\", \"jp2\", \"psd\"]\n",
    "languages_dict = {'English': 'eng', \n",
    "                  'English, Middle (1100-1500)': 'enm', \n",
    "                  'Chinese - Simplified': 'chi_sim', \n",
    "                  'Chinese - Traditional': 'chi_tra', \n",
    "                  'French': 'fra', \n",
    "                  'German' : 'deu',\n",
    "                  'Greek, Modern (1453-)': 'ell', \n",
    "                  'Greek, Ancient (-1453)': 'grc', \n",
    "                  'Hebrew' : 'heb', \n",
    "                  'Hindi' : 'hin', \n",
    "                  'Hungarian': 'hun', \n",
    "                  'Indonesian': 'ind', \n",
    "                  'Italian': 'ita', \n",
    "                  'Italian - Old': 'ita_old', \n",
    "                  'Japanese': 'jpn', \n",
    "                  'Korean': 'kor', \n",
    "                  'Malay': 'msa', \n",
    "                  'Panjabi; Punjabi': 'pan', \n",
    "                  'Polish': 'pol', \n",
    "                  'Portuguese': 'por', \n",
    "                  'Russian': 'rus', \n",
    "                  'Spanish; Castilian': 'spa', \n",
    "                  'Spanish; Castilian - Old': 'spa_old', \n",
    "                  'Swedish': 'swe', \n",
    "                  'Thai': 'tha', \n",
    "                  'Turkish': 'tur', \n",
    "                  'Uighur; Uyghur': 'uig', \n",
    "                  'Ukrainian': 'ukr', \n",
    "                  'Vietnamese': 'vie', \n",
    "                  'Yiddish': 'yid'\n",
    "                 }\n",
    "languages_list = list(languages_dict.keys())\n",
    "\n",
    "#languages_words = ', '.join(languages_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b4492d7-d779-44f2-bf47-aafce3baa108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert each uploaded file to file name, text\n",
    "\n",
    "#@st.cache_data(show_spinner = False)\n",
    "def doc_to_text(uploaded_doc, language, page_bound):\n",
    "    file_triple = {'File name' : '', 'Language choice': language, 'Page length': '', 'extracted_text': ''} #\n",
    "\n",
    "    try:\n",
    "        #Get file name\n",
    "        file_triple['File name']=uploaded_doc.name\n",
    "        \n",
    "        #Get file data\n",
    "        bytes_data = uploaded_doc.getvalue()\n",
    "    \n",
    "        #Get file extension\n",
    "        extension = file_triple['File name'].split('.')[-1].lower()\n",
    "    \n",
    "        #Create list of pages\n",
    "        text_list = []\n",
    "    \n",
    "        #Word format\n",
    "        if extension == 'docx':\n",
    "            doc_string = mammoth.convert_to_html(BytesIO(bytes_data)).value\n",
    "            text_list.append(doc_string)\n",
    "    \n",
    "            #file_triple['Page length'] = 1\n",
    "\n",
    "            file_triple['extracted_text'] = str(text_list)\n",
    "            \n",
    "        else:\n",
    "             \n",
    "            #pdf formats #If want to enable pymupdf4llm. Not useful in my experience.\n",
    "            #if extension == 'pdf':\n",
    "                #doc = pymupdf.open(stream=bytes_data)\n",
    "\n",
    "                #max_doc_number=min(len(doc), page_bound)\n",
    "\n",
    "                #md_text = pymupdf4llm.to_markdown(doc = doc, pages = range(0, max_doc_number), embed_images = True) #Add embed_images = True if want to include images\n",
    "                \n",
    "                #file_triple['extracted_text'] = str(md_text)\n",
    "\n",
    "            #Other formats\n",
    "            #else:\n",
    "                \n",
    "            #text formats\n",
    "            if extension in ['txt', 'cs', 'xml', 'html', 'json']:\n",
    "                doc = pymupdf.open(stream=bytes_data, filetype=\"txt\")\n",
    "    \n",
    "            #Other formats\n",
    "            else:\n",
    "                doc = pymupdf.open(stream=bytes_data)\n",
    "    \n",
    "            max_doc_number=min(len(doc), page_bound)\n",
    "            \n",
    "            for page_index in list(range(0, max_doc_number)):\n",
    "                page = doc.load_page(page_index)\n",
    "                text_page = page.get_text() \n",
    "                text_list.append(text_page)\n",
    "    \n",
    "            file_triple['extracted_text'] = str(text_list)\n",
    "\n",
    "            #Length of pages\n",
    "            file_triple['Page length'] = len(doc)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{file_triple['File name']}: failed to get text\")\n",
    "        print(e)\n",
    "    \n",
    "    return file_triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9087b68-2c2b-4240-b04a-a02bf3580a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for images to text\n",
    "\n",
    "#@st.cache_data(show_spinner = False)\n",
    "def image_to_text(uploaded_image, language, page_bound):\n",
    "    file_triple = {'File name' : '', 'Language choice': language, 'Page length': '', 'extracted_text': ''}\n",
    "\n",
    "    try:\n",
    "        #Get file name\n",
    "        file_triple['File name']=uploaded_image.name\n",
    "    \n",
    "        #Get file data\n",
    "        bytes_data = uploaded_image.read()\n",
    "    \n",
    "        #Get file extension\n",
    "        extension = file_triple['File name'].split('.')[-1].lower()\n",
    "    \n",
    "        #Obtain images from uploaded file\n",
    "        if extension == 'pdf':\n",
    "            try:\n",
    "                images = pdf2image.convert_from_bytes(bytes_data, timeout=30)\n",
    "            except PDFPopplerTimeoutError as pdf2image_timeout_error:\n",
    "                print(f\"pdf2image error: {pdf2image_timeout_error}.\")\n",
    "    \n",
    "        else:\n",
    "            images = []\n",
    "            image_raw = Image.open(BytesIO(bytes_data))\n",
    "            images.append(image_raw)\n",
    "            \n",
    "        #Extract text from images\n",
    "        text_list = []\n",
    "        \n",
    "        max_images_number=min(len(images), page_bound)\n",
    "    \n",
    "        for image in images[ : max_images_number]:\n",
    "            try:\n",
    "                text_page = pytesseract.image_to_string(image, lang=languages_dict[language], timeout=30)\n",
    "                text_list.append(text_page)\n",
    "                \n",
    "            except RuntimeError as pytesseract_timeout_error:\n",
    "                print(f\"pytesseract error: {pytesseract_timeout_error}.\")\n",
    "    \n",
    "        file_triple['extracted_text'] = str(text_list)\n",
    "    \n",
    "        #Length of pages\n",
    "        file_triple['Page length'] = len(images)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"{file_triple['File name']}: failed to get text\")\n",
    "        print(e)\n",
    "        \n",
    "    return file_triple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a2d594",
   "metadata": {},
   "source": [
    "# GPT functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bfae1df-142c-4583-ab1f-fd94500499b1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gpt_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Import functions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_by_line, GPT_label_dict, is_api_key_valid, gpt_input_cost, gpt_output_cost, tokens_cap, num_tokens_from_string  \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Import variables\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m question_characters_bound, default_judgment_counter_bound\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gpt_functions'"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from functions.gpt_functions import GPT_label_dict, is_api_key_valid, gpt_input_cost, gpt_output_cost, tokens_cap, max_output, num_tokens_from_string, gpt_batch_input, engage_GPT_json, GPT_questions_check\n",
    "#Import variables\n",
    "from functions.gpt_functions import questions_check_system_instruction, basic_model, flagship_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f7af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define system role content for GPT\n",
    "role_content_own = \"\"\"You are a legal research assistant helping an academic researcher to answer questions about a file. The file may be a document or an image. You will be provided with the file. \n",
    "Please answer questions based only on information contained in the file. Where your answer comes from a part of the file, include a reference to that part of the file. \n",
    "If you cannot answer the questions based on the file, do not make up information, but instead write \"answer not found\".\n",
    "\"\"\"\n",
    "\n",
    "#Respond in JSON form. In your response, produce as many keys as you need. \n",
    "\n",
    "#system_instruction = role_content_own\n",
    "\n",
    "#intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83981e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain GPT output\n",
    "\n",
    "@st.cache_data(show_spinner = False, ttl=300)\n",
    "def run_own(df_master, uploaded_docs, uploaded_images):\n",
    "    df_master = df_master.fillna('')\n",
    "\n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    df_master['questions_json'] = df_master['Enter your questions for GPT'].apply(GPT_label_dict)\n",
    "    \n",
    "    #Create files file\n",
    "    Files_file = []\n",
    "\n",
    "    #Obtain bounds and language\n",
    "\n",
    "    file_counter_bound = int(df_master.loc[0, 'Maximum number of files'])\n",
    "\n",
    "    page_bound = int(df_master.loc[0,'Maximum number of pages per file'])\n",
    "\n",
    "    language = df_master.loc[0, 'Language choice']\n",
    "    \n",
    "    #Convert uploaded documents to text\n",
    "\n",
    "    file_counter = 1 \n",
    "\n",
    "    for uploaded_doc in uploaded_docs:\n",
    "        if file_counter <= file_counter_bound:\n",
    "            file_triple = doc_to_text(uploaded_doc, language, page_bound)\n",
    "            Files_file.append(file_triple)\n",
    "            file_counter += 1\n",
    "\n",
    "    #Convert uploaded images to text\n",
    "\n",
    "    for uploaded_image in uploaded_images:\n",
    "        if file_counter <= file_counter_bound:\n",
    "            file_triple = image_to_text(uploaded_image, language, page_bound)\n",
    "            Files_file.append(file_triple)\n",
    "            file_counter += 1\n",
    "    \n",
    "    #Create and export json file with search output\n",
    "    json_individual = json.dumps(Files_file, indent=2)\n",
    "    \n",
    "    df_individual = pd.read_json(json_individual)\n",
    "\n",
    "    #GPT model\n",
    "\n",
    "    if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "        gpt_model = flagship_model\n",
    "    else:        \n",
    "        gpt_model = basic_model\n",
    "        \n",
    "    #apply GPT_individual to each respondent's file spreadsheet\n",
    "    \n",
    "    GPT_activation = int(df_master.loc[0, 'Use GPT'])\n",
    "\n",
    "    questions_json = df_master.loc[0, 'questions_json']\n",
    "\n",
    "    system_instruction = df_master.loc[0, 'System instruction']\n",
    "    \n",
    "    #Engage GPT    \n",
    "    #df_updated = engage_GPT_json_own(questions_json = questions_json, df_example = df_master.loc[0, 'Example'], df_individual = df_individual, GPT_activation = GPT_activation, gpt_model = gpt_model, system_instruction = system_instruction)\n",
    "    df_updated = engage_GPT_json(questions_json = questions_json, df_example = df_master.loc[0, 'Example'], df_individual = df_individual, GPT_activation = GPT_activation, gpt_model = gpt_model, system_instruction = system_instruction)\n",
    "\n",
    "    if (pop_judgment() > 0) and ('extracted_text' in df_updated.columns):\n",
    "        df_updated.pop('extracted_text')\n",
    "    \n",
    "    return df_updated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caa7ba6-7f30-40a5-a98d-16a956d1f5f6",
   "metadata": {},
   "source": [
    "# For vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb2c4cf-70cf-48cb-856b-1df1b044be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import functions\n",
    "from functions.gpt_functions import get_image_dims, calculate_image_token_cost, GPT_b64_json, engage_GPT_b64_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed9ee2-0dea-4f6a-840b-bb5ed83eca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@st.cache_data(show_spinner = False)\n",
    "def image_to_b64_own(uploaded_image, language, page_bound):\n",
    "    file_triple = {'File name' : '', 'Language choice': language, 'b64_list': [], 'Dimensions (width, height)' : [],\n",
    "                   'Page length': '', \n",
    "                   'tokens_raw': 0\n",
    "                  }\n",
    "\n",
    "    try:\n",
    "        file_triple['File name']=uploaded_image.name\n",
    "    \n",
    "        #Get file extension\n",
    "        extension = file_triple['File name'].split('.')[-1].lower()\n",
    "    \n",
    "        bytes_data = uploaded_image.read()\n",
    "    \n",
    "        if extension == 'pdf':\n",
    "            \n",
    "            images = pdf2image.convert_from_bytes(bytes_data, timeout=30, fmt=\"jpeg\")\n",
    "    \n",
    "            file_triple['Page length'] = len(images)\n",
    "    \n",
    "            #Get page bound\n",
    "            max_images_number=min(len(images), page_bound)\n",
    "    \n",
    "            for image in images[ : max_images_number]:\n",
    "    \n",
    "                output = BytesIO()\n",
    "                image.save(output, format='JPEG')\n",
    "                im_data = output.getvalue()\n",
    "                \n",
    "                image_data = base64.b64encode(im_data)\n",
    "                if not isinstance(image_data, str):\n",
    "                    # Python 3, decode from bytes to string\n",
    "                    image_data = image_data.decode()\n",
    "                data_url = 'data:image/jpg;base64,' + image_data\n",
    "    \n",
    "                #b64 = base64.b64encode(image_raw).decode('utf-8')\n",
    "    \n",
    "                b64_to_attach = data_url\n",
    "                #b64_to_attach = f\"data:image/png;base64,{b64}\"\n",
    "    \n",
    "            file_triple['b64_list'].append(b64_to_attach)\n",
    "\n",
    "        else:\n",
    "    \n",
    "            #file_triple['Page length'] = 1\n",
    "        \n",
    "            b64 = base64.b64encode(bytes_data).decode('utf-8')\n",
    "        \n",
    "            b64_to_attach = f\"data:image/{extension};base64,{b64}\"\n",
    "            \n",
    "            file_triple['b64_list'].append(b64_to_attach)\n",
    "            \n",
    "        for image_b64 in file_triple['b64_list']:\n",
    "    \n",
    "            #Get dimensions\n",
    "            try:\n",
    "    \n",
    "                file_triple['Dimensions (width, height)'].append(get_image_dims(b64_to_attach))\n",
    "            except Exception as e:\n",
    "                print(f\"Cannot obtain dimensions for {file_triple['File name']}, p {file_triple['b64_list'].index(image_b64)}.\")\n",
    "                print(e)\n",
    "            \n",
    "            file_triple['tokens_raw'] = file_triple['tokens_raw'] + calculate_image_token_cost(image_b64, detail=\"auto\")\n",
    "    except Exception as e:\n",
    "        print(f\"{file_triple['File name']}: failed to get text\")\n",
    "        print(e)\n",
    "        \n",
    "    return file_triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3046c1e-5d6f-463a-92e4-84897573bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For vision\n",
    "\n",
    "@st.cache_data(show_spinner = False, ttl=300)\n",
    "def batch_b64_own(df_master, uploaded_images):\n",
    "\n",
    "    df_master = df_master.fillna('')\n",
    "\n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    df_master['questions_json'] = df_master['Enter your questions for GPT'].apply(GPT_label_dict)\n",
    "\n",
    "    #Obtain bounds and language\n",
    "\n",
    "    file_counter_bound = int(df_master.loc[0, 'Maximum number of files'])\n",
    "\n",
    "    page_bound = int(df_master.loc[0,'Maximum number of pages per file'])\n",
    "\n",
    "    language = df_master.loc[0, 'Language choice']\n",
    "    \n",
    "    #Convert uploaded documents to b64\n",
    "\n",
    "    file_counter = 1 \n",
    "    \n",
    "    #Create files file\n",
    "    Files_file = []\n",
    "\n",
    "    #Convert images to b64, then send to GPT\n",
    "    for uploaded_image in uploaded_images:\n",
    "        if file_counter <= file_counter_bound:\n",
    "            file_triple = image_to_b64_own(uploaded_image, language, page_bound)\n",
    "            Files_file.append(file_triple)\n",
    "            file_counter += 1\n",
    "\n",
    "    #Create and export json file with search output\n",
    "    json_individual = json.dumps(Files_file, indent=2)\n",
    "    \n",
    "    df_individual = pd.read_json(json_individual)\n",
    "\n",
    "    #Instruct GPT\n",
    "\n",
    "    GPT_activation = int(df_master.loc[0, 'Use GPT'])\n",
    "    \n",
    "    #GPT model\n",
    "\n",
    "    if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "        gpt_model = flagship_model\n",
    "    else:        \n",
    "        gpt_model = basic_model\n",
    "        \n",
    "    #apply GPT_individual to each respondent's judgment spreadsheet\n",
    "\n",
    "    questions_json = df_master.loc[0, 'questions_json']\n",
    "\n",
    "    system_instruction = df_master.loc[0, 'System instruction']\n",
    "    \n",
    "    #apply GPT_individual to each respondent's file spreadsheet    \n",
    "    df_updated = engage_GPT_b64_json(questions_json = questions_json, df_example = df_master.loc[0, 'Example'], df_individual = df_individual, GPT_activation = GPT_activation, gpt_model = gpt_model, system_instruction = system_instruction)\n",
    "    \n",
    "    #Remove redundant columns\n",
    "\n",
    "    for column in ['tokens_raw', 'b64_list']:\n",
    "        try:\n",
    "            df_updated.pop(column)\n",
    "        except:\n",
    "            print(f\"No {column} column.\")\n",
    "\n",
    "    return df_updated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0154a7ce-391c-443d-aa56-be334b8c1691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For vision\n",
    "\n",
    "@st.cache_data(show_spinner = False, ttl=300)\n",
    "def run_b64_own(df_master, uploaded_images):\n",
    "\n",
    "    df_master = df_master.fillna('')\n",
    "\n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    df_master['questions_json'] = df_master['Enter your questions for GPT'].apply(GPT_label_dict)\n",
    "\n",
    "    #Obtain bounds and language\n",
    "\n",
    "    file_counter_bound = int(df_master.loc[0, 'Maximum number of files'])\n",
    "\n",
    "    page_bound = int(df_master.loc[0,'Maximum number of pages per file'])\n",
    "\n",
    "    language = df_master.loc[0, 'Language choice']\n",
    "    \n",
    "    #Convert uploaded documents to b64\n",
    "\n",
    "    file_counter = 1 \n",
    "    \n",
    "    #Create files file\n",
    "    Files_file = []\n",
    "\n",
    "    #Convert images to b64, then send to GPT\n",
    "    for uploaded_image in uploaded_images:\n",
    "        if file_counter <= file_counter_bound:\n",
    "            file_triple = image_to_b64_own(uploaded_image, language, page_bound)\n",
    "            Files_file.append(file_triple)\n",
    "            file_counter += 1\n",
    "\n",
    "    #Create and export json file with search output\n",
    "    json_individual = json.dumps(Files_file, indent=2)\n",
    "    \n",
    "    df_individual = pd.read_json(json_individual)\n",
    "\n",
    "    #Instruct GPT\n",
    "\n",
    "    #GPT model\n",
    "\n",
    "    if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "        gpt_model = flagship_model\n",
    "    else:        \n",
    "        gpt_model = basic_model\n",
    "        \n",
    "    #apply GPT_individual to each respondent's file spreadsheet\n",
    "    \n",
    "    GPT_activation = int(df_master.loc[0, 'Use GPT'])\n",
    "\n",
    "    questions_json = df_master.loc[0, 'questions_json']\n",
    "\n",
    "    system_instruction = df_master.loc[0, 'System instruction']\n",
    "\n",
    "    #apply GPT_individual to each respondent's file spreadsheet    \n",
    "    df_updated = engage_GPT_b64_json(questions_json = questions_json, df_example = df_master.loc[0, 'Example'], df_individual = df_individual, GPT_activation = GPT_activation, gpt_model = gpt_model, system_instruction = system_instruction)\n",
    "    \n",
    "    #Remove redundant columns\n",
    "\n",
    "    for column in ['tokens_raw', 'b64_list']:\n",
    "        try:\n",
    "            df_updated.pop(column)\n",
    "        except:\n",
    "            print(f\"No {column} column.\")\n",
    "\n",
    "    return df_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeb9664-aac9-4bda-a6d3-17d0dbb81040",
   "metadata": {},
   "source": [
    "# Batch request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1bc272-7500-4d16-82ca-129e4a26ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch get GPT output\n",
    "\n",
    "@st.cache_data(show_spinner = False, ttl=300)\n",
    "def batch_own(df_master, uploaded_docs, uploaded_images):\n",
    "    df_master = df_master.fillna('')\n",
    "\n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    df_master['questions_json'] = df_master['Enter your questions for GPT'].apply(GPT_label_dict)\n",
    "    \n",
    "    #Create files file\n",
    "    Files_file = []\n",
    "\n",
    "    #Obtain bounds and language\n",
    "\n",
    "    file_counter_bound = int(df_master.loc[0, 'Maximum number of files'])\n",
    "\n",
    "    page_bound = int(df_master.loc[0,'Maximum number of pages per file'])\n",
    "\n",
    "    #st.write(f\"file_counter_bound == {file_counter_bound}, page_bound == {page_bound}\")\n",
    "    \n",
    "    language = df_master.loc[0, 'Language choice']\n",
    "    \n",
    "    #Convert uploaded documents to text\n",
    "\n",
    "    file_counter = 1\n",
    "\n",
    "    #Decide whether to do b64\n",
    "    if bool(df_master.loc[0, 'b64_enabled']) == False:\n",
    "        \n",
    "        for uploaded_doc in uploaded_docs:\n",
    "            if file_counter <= file_counter_bound:\n",
    "                file_triple = doc_to_text(uploaded_doc, language, page_bound)\n",
    "                Files_file.append(file_triple)\n",
    "                file_counter += 1\n",
    "    \n",
    "        #Convert uploaded images to text\n",
    "    \n",
    "        for uploaded_image in uploaded_images:\n",
    "            if file_counter <= file_counter_bound:\n",
    "                file_triple = image_to_text(uploaded_image, language, page_bound)\n",
    "                Files_file.append(file_triple)\n",
    "                file_counter += 1\n",
    "\n",
    "    else: #bool(df_master.loc[0, 'b64']) == True:\n",
    "    \n",
    "        #Convert images to b64, then send to GPT\n",
    "        for uploaded_image in uploaded_images:\n",
    "            if file_counter <= file_counter_bound:\n",
    "                file_triple = image_to_b64_own(uploaded_image, language, page_bound)\n",
    "                Files_file.append(file_triple)\n",
    "                file_counter += 1\n",
    "    \n",
    "    #Create and export json file with search output\n",
    "    json_individual = json.dumps(Files_file, indent=2)\n",
    "    \n",
    "    df_individual = pd.read_json(json_individual)\n",
    "\n",
    "    #GPT model\n",
    "\n",
    "    if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "        gpt_model = flagship_model\n",
    "    else:        \n",
    "        gpt_model = basic_model\n",
    "        \n",
    "    #apply GPT_individual to each respondent's file spreadsheet\n",
    "    \n",
    "    GPT_activation = int(df_master.loc[0, 'Use GPT'])\n",
    "\n",
    "    questions_json = df_master.loc[0, 'questions_json']\n",
    "\n",
    "    system_instruction = df_master.loc[0, 'System instruction']\n",
    "        \n",
    "    #Engage GPT\n",
    "    batch_record_df_individual = gpt_batch_input(questions_json = questions_json, df_example = df_master.loc[0, 'Example'], df_individual = df_individual, GPT_activation = GPT_activation, gpt_model = gpt_model, system_instruction = system_instruction)\n",
    "\n",
    "    #Remove before text before saving to aws\n",
    "    if (pop_judgment() > 0) and ('extracted_text' in df_individual.columns):\n",
    "        df_individual.pop('extracted_text')\n",
    "\n",
    "    return batch_record_df_individual\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46898f82-f37b-4574-bb25-619648c36ca6",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "#Batch function\n",
    "\n",
    "@st.dialog(\"Requesting data\")\n",
    "def own_batch_request_function(df_master, uploaded_docs, uploaded_images):\n",
    "     \n",
    "    if ((st.session_state['df_master'].loc[0, 'Use own account'] == True) and (st.session_state['df_master'].loc[0, 'Use GPT'] == True)):\n",
    "                            \n",
    "        if is_api_key_valid(st.session_state.df_master.loc[0, 'Your GPT API key']) == False:\n",
    "            st.error('Your API key is not valid.')\n",
    "            \n",
    "            st.session_state[\"batch_ready_for_submission\"] = False\n",
    "\n",
    "            st.stop()\n",
    "\n",
    "        else:\n",
    "            \n",
    "            st.session_state[\"batch_ready_for_submission\"] = True\n",
    "    else:\n",
    "        st.session_state[\"batch_ready_for_submission\"] = True\n",
    "\n",
    "    \n",
    "    #Check if valid email address entered\n",
    "    if '@' not in st.session_state['df_master'].loc[0, 'Your email address']:\n",
    "        \n",
    "        st.session_state[\"batch_ready_for_submission\"] = False\n",
    "\n",
    "        st.write('Please enter a valid email address to receive your request data.')\n",
    "        \n",
    "        batch_email_entry = st.text_input(label = \"Your email address (mandatory)\", value =  st.session_state['df_master'].loc[0, 'Your email address'])\n",
    "\n",
    "        if st.button(label = 'CONFIRM your email address', disabled = bool(st.session_state.batch_submitted)):\n",
    "            \n",
    "            st.session_state['df_master'].loc[0, 'Your email address'] = batch_email_entry\n",
    "\n",
    "            if '@' not in st.session_state['df_master'].loc[0, 'Your email address']:\n",
    "            \n",
    "                st.error('You must enter a valid email address to receive your request data.')\n",
    "                st.stop()\n",
    "            else:\n",
    "                st.session_state[\"batch_ready_for_submission\"] = True\n",
    "\n",
    "    if st.session_state[\"batch_ready_for_submission\"] == True:\n",
    "    \n",
    "        with st.spinner(spinner_text):\n",
    "            \n",
    "            try:\n",
    "\n",
    "                #Update df_master\n",
    "                jurisdiction_page = st.session_state.jurisdiction_page\n",
    "\n",
    "                df_master['jurisdiction_page'] = jurisdiction_page\n",
    "                \n",
    "                df_master['submission_time'] = str(datetime.now())\n",
    "\n",
    "                #Activate user's own key or mine\n",
    "                if st.session_state['df_master'].loc[0, 'Use own account'] == True:\n",
    "                    \n",
    "                    API_key = st.session_state.df_master.loc[0, 'Your GPT API key']\n",
    "    \n",
    "                else:\n",
    "                    \n",
    "                    API_key = st.secrets[\"openai\"][\"gpt_api_key\"]\n",
    "\n",
    "                    st.session_state['df_master'].loc[0, 'Maximum number of files'] = st.session_state[\"judgment_counter_max\"]\n",
    "\n",
    "                #Check questions for potential privacy violation\n",
    "                openai.api_key = API_key\n",
    "\n",
    "                if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "                    gpt_model = flagship_model\n",
    "                else:        \n",
    "                    gpt_model = basic_model\n",
    "\n",
    "                questions_checked_dict = GPT_questions_check(df_master.loc[0, 'Enter your questions for GPT'], gpt_model, questions_check_system_instruction)\n",
    "\n",
    "                #Use checked questions\n",
    "                df_master.loc[0, 'Enter your questions for GPT'] = questions_checked_dict['questions_string']\n",
    "                \n",
    "                #Get batch_record, df_individual as a list\n",
    "                batch_record_df_individual = batch_own(df_master, uploaded_docs, uploaded_images)\n",
    "\n",
    "                #print(f\"batch_record_df_individual == {batch_record_df_individual}\")\n",
    "                \n",
    "                df_individual = batch_record_df_individual['df_individual']\n",
    "                \n",
    "                batch_dict = batch_record_df_individual['batch_record'].to_dict()\n",
    "                \n",
    "                batch_id = batch_dict['id']\n",
    "                input_file_id = batch_dict['input_file_id']\n",
    "                status = batch_dict['status']\n",
    "        \n",
    "                #Add batch_record to df_master\n",
    "                df_master['batch_id'] = batch_id\n",
    "                df_master['input_file_id'] = input_file_id\n",
    "                df_master['status'] = status\n",
    "\n",
    "                #print(f\"df_master == {df_master}\")\n",
    "                \n",
    "                #Initiate aws s3\n",
    "                s3_resource = get_aws_s3()\n",
    "                #Get a list of all files on s3\n",
    "                #bucket = s3_resource.Bucket('lawtodata')\n",
    "\n",
    "                #Upload df_individual onto AWS\n",
    "                aws_df_put(s3_resource, df_individual, f'{batch_id}.csv')\n",
    "\n",
    "                #csv_buffer = StringIO()\n",
    "                #df_individual.to_csv(csv_buffer)\n",
    "                #s3_resource.Object('lawtodata', f'{batch_id}.csv').put(Body=csv_buffer.getvalue())\n",
    "                                    \n",
    "                #Get all_df_masters\n",
    "\n",
    "                all_df_masters = aws_df_get(s3_resource, 'all_df_masters.csv')\n",
    "                \n",
    "                #for obj in bucket.objects.all():\n",
    "                    #key = obj.key\n",
    "                    #if key == 'all_df_masters.csv':\n",
    "                        #body = obj.get()['Body'].read()\n",
    "                        #all_df_masters = pd.read_csv(BytesIO(body), index_col=0)\n",
    "                        #break\n",
    "                        \n",
    "                #Add df_master to all_df_masters \n",
    "                all_df_masters = pd.concat([all_df_masters, df_master], ignore_index=True)\n",
    "\n",
    "                #Upload all_df_masters to aws\n",
    "                aws_df_put(s3_resource, all_df_masters, 'all_df_masters.csv')\n",
    "                \n",
    "                #csv_buffer = StringIO()\n",
    "                #all_df_masters.to_csv(csv_buffer)\n",
    "                #s3_resource = boto3.resource('s3',region_name=st.secrets[\"aws\"][\"AWS_DEFAULT_REGION\"], aws_access_key_id=st.secrets[\"aws\"][\"AWS_ACCESS_KEY_ID\"], aws_secret_access_key=st.secrets[\"aws\"][\"AWS_SECRET_ACCESS_KEY\"])\n",
    "                #s3_resource.Object('lawtodata', 'all_df_masters.csv').put(Body=csv_buffer.getvalue())\n",
    "                \n",
    "                #Send me an email to let me know\n",
    "                send_notification_email(ULTIMATE_RECIPIENT_NAME = st.session_state['df_master'].loc[0, 'Your name'], \n",
    "                                        ULTIMATE_RECIPIENT_EMAIL = st.session_state['df_master'].loc[0, 'Your email address'], \n",
    "                                        jurisdiction_page = st.session_state['df_master'].loc[0, 'jurisdiction_page']\n",
    "                                       )\n",
    "\n",
    "                #Change session states\n",
    "                st.session_state[\"batch_submitted\"] = True\n",
    "                st.session_state['need_resetting'] = 1\n",
    "                st.session_state[\"batch_error\"] == False\n",
    "                st.session_state['error_msg'] = ''\n",
    "                \n",
    "                st.rerun()\n",
    "            \n",
    "            except Exception as e:\n",
    "\n",
    "                #Change session states\n",
    "                st.session_state['df_master'].loc[0, 'Maximum number of judgments'] = default_judgment_counter_bound                    \n",
    "                st.session_state[\"batch_submitted\"] = False\n",
    "                st.session_state[\"batch_error\"] = True\n",
    "\n",
    "                st.error(search_error_display)\n",
    "                                \n",
    "                print(traceback.format_exc())\n",
    "\n",
    "                st.session_state['error_msg'] = traceback.format_exc()\n",
    "\n",
    "                st.rerun()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
