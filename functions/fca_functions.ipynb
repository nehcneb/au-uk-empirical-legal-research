{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b8b879",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2312235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminary modules\n",
    "import base64 \n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date\n",
    "from dateutil import parser\n",
    "#from dateutil.relativedelta import *\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import pause\n",
    "import requests\n",
    "import httplib2\n",
    "import urllib\n",
    "from urllib.request import urlretrieve\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import os\n",
    "#import pypdf\n",
    "import io\n",
    "from io import BytesIO\n",
    "import copy\n",
    "\n",
    "#Streamlit\n",
    "import streamlit as st\n",
    "#from streamlit_gsheets import GSheetsConnection\n",
    "from streamlit.components.v1 import html\n",
    "#import streamlit_ext as ste\n",
    "from streamlit_extras.stylable_container import stylable_container\n",
    "\n",
    "#OpenAI\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "#Google\n",
    "#from google.oauth2 import service_account\n",
    "\n",
    "#Excel\n",
    "from pyxlsb import open_workbook as open_xlsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111133cd-06ce-45a6-8556-91199246e001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface == True\n"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from functions.common_functions import own_account_allowed, pop_judgment, convert_df_to_json, convert_df_to_csv, convert_df_to_excel, clear_cache, list_range_check, date_parser, save_input, pdf_judgment\n",
    "#Import variables\n",
    "from functions.common_functions import huggingface, today_in_nums, errors_list, scraper_pause_mean, judgment_text_lower_bound, default_judgment_counter_bound, no_results_msg\n",
    "\n",
    "#Load oalc\n",
    "from functions.oalc_functions import get_judgment_from_oalc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e819e27",
   "metadata": {},
   "source": [
    "# Federal Courts search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f3329d8-1716-4570-8dd0-4b5aa22aaed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.common_functions import link, split_title_mnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cb5735b-cbd1-454a-937d-bdefcdcbcb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define format functions for courts choice, and GPT questions\n",
    "\n",
    "#auxiliary lists and variables\n",
    "\n",
    "fca_courts = {'Federal Court': 'FCA+FCAFC', \n",
    "              'Industrial Relations Court of Australia': 'IRCA', \n",
    "              'Australian Competition Tribunal': 'ACOMPT', \n",
    "              'Copyright Tribunal': 'ACOPYT', \n",
    "              'Defence Force Discipline Appeal Tribunal': 'ADFDAT', \n",
    "              'Federal Police Discipline Tribunal': 'FPDT', \n",
    "              'Trade Practices Tribunal': 'ATPT', \n",
    "              'Supreme Court of Norfolk Island': 'NFSC',\n",
    "             'All': 'FCA+FCAFC+IRCA+ACOMPT+ACOPYT+ADFDAT+FPDT+ATPT+NFSC'}\n",
    "\n",
    "fca_courts_list = list(fca_courts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f676e99-b155-4dee-8d25-3cf805348810",
   "metadata": {},
   "outputs": [],
   "source": [
    "npa_dict = {'All': '', \n",
    "    'Admin., Constitutional, Human Rights': 'administrative', \n",
    "  'Admiralty and Maritime': 'admiralty', \n",
    "  'Commercial and Corporations': 'commercial', \n",
    "  'Employment and Industrial Relations': 'employment', \n",
    "  'Federal Crime and Related Proceedings': 'crime', \n",
    "  'Intellectual Property': 'intellectual', \n",
    "  'Native Title': 'native', \n",
    "  'Taxation': 'taxation',\n",
    "      'Other Federal Jurisdiction': 'other',\n",
    "    }\n",
    "\n",
    "npa_list = list(npa_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4aba2e-2865-431b-9f3f-b688b4f6e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import ActionChains\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from webdriver_manager.core.os_manager import ChromeType\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait as Wait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium_stealth import stealth\n",
    "\n",
    "if 'Users/Ben' not in os.getcwd(): \n",
    "\n",
    "    from pyvirtualdisplay import Display\n",
    "    \n",
    "    display = Display(visible=0, size=(1200, 1600))  \n",
    "    display.start()\n",
    "\n",
    "#For downloading judgments\n",
    "download_dir = os.getcwd() + '/FCA_PDFs'\n",
    "\n",
    "#@st.cache_resource(show_spinner = False, ttl=600)\n",
    "def get_driver():\n",
    "\n",
    "    options = Options()\n",
    "    \n",
    "    #For automation\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option('useAutomationExtension', False)\n",
    "    \n",
    "    #For downloading judgments\n",
    "    options.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": download_dir, #Change default directory for downloads\n",
    "    \"download.prompt_for_download\": False, #To auto download the file\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True #It will not show PDF directly in chrome\n",
    "    })\n",
    "    \n",
    "    browser = webdriver.Chrome(options=options)\n",
    "\n",
    "    browser.implicitly_wait(15)\n",
    "    browser.set_page_load_timeout(30)\n",
    "\n",
    "    stealth(browser,\n",
    "    \n",
    "            languages=[\"en-US\", \"en\"],\n",
    "    \n",
    "            vendor=\"Google Inc.\",\n",
    "    \n",
    "            platform=\"Win32\",\n",
    "    \n",
    "            webgl_vendor=\"Intel Inc.\",\n",
    "    \n",
    "            renderer=\"Intel Iris OpenGL Engine\",\n",
    "    \n",
    "            webdriver=False,\n",
    "    \n",
    "            fix_hairline=True)\n",
    "    \n",
    "    if 'Users/Ben' in os.getcwd():\n",
    "        browser.minimize_window()\n",
    "    \n",
    "    return browser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2a0e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function turning search terms to search results url\n",
    "\n",
    "#@st.cache_data(show_spinner = False)\n",
    "def fca_search(court = '', \n",
    "               case_name_mnc= '', \n",
    "               judge ='', \n",
    "               reported_citation ='', \n",
    "               file_number ='', \n",
    "               npa = '', \n",
    "               with_all_the_words = '', \n",
    "               with_at_least_one_of_the_words = '', \n",
    "               without_the_words = '', \n",
    "               phrase = '', \n",
    "               proximity = '', \n",
    "               on_this_date = '', \n",
    "               after_date = '', \n",
    "               before_date = '', \n",
    "               legislation = '', \n",
    "               cases_cited = '', \n",
    "               catchwords = ''):\n",
    "\n",
    "    #If only searching FCA\n",
    "    #base_url = \"https://search2.fedcourt.gov.au/s/search.html?collection=judgments&sort=date&meta_v_phrase_orsand=judgments%2FJudgments%2Ffca\"\n",
    "\n",
    "    #If allowing users to search which court\n",
    "    #base_url = \"https://search2.fedcourt.gov.au/s/search.html?collection=judgments&sort=date&meta_v_phrase_orsand=judgments%2FJudgments%2F\" + fca_courts[court]\n",
    "\n",
    "    #New base_url as of 20250415\n",
    "    #base_url = \"https://search.judgments.fedcourt.gov.au/s/search.html\"\n",
    "\n",
    "    base_url = 'https://search.judgments.fedcourt.gov.au/s/search.html?collection=fca~sp-judgments-internet&profile=judgments-internet&sort=date&meta_CourtID_orsand=' + fca_courts[court] \n",
    "    \n",
    "    #Tidy up dates for batch mode\n",
    "    if '-' in str(after_date):\n",
    "        after_date = str(after_date).replace('-', '')\n",
    "\n",
    "    if '/' in str(after_date):\n",
    "        after_date = str(after_date).replace('/', '')\n",
    "    \n",
    "    if '-' in str(before_date):\n",
    "        before_date = str(before_date).replace('-', '')\n",
    "\n",
    "    if '/' in str(after_date):\n",
    "        before_date = str(before_date).replace('/', '')\n",
    "    \n",
    "    params = {\n",
    "        #'collection': 'fca~sp-judgments-internet', \n",
    "        #'profile': 'judgments-internet',\n",
    "        #'sort': 'date', \n",
    "        #'meta_CourtID_orsand': fca_courts[court], \n",
    "        'meta_MNC' : case_name_mnc, \n",
    "              'meta_Judge' : judge, \n",
    "              'meta_Reported' : reported_citation, \n",
    "              'meta_FileNumber' : file_number, \n",
    "              'meta_NPA_phrase_orsand' : npa_dict[npa], \n",
    "              'query_sand' : with_all_the_words, \n",
    "              'query_or' : with_at_least_one_of_the_words, \n",
    "              'query_not' : without_the_words, \n",
    "              'query_phrase' : phrase, \n",
    "              'query_prox' : proximity, \n",
    "              'meta_d' : on_this_date, \n",
    "              'meta_d1' : after_date, \n",
    "              'meta_d2' : before_date, \n",
    "              'meta_Legislation' : legislation, \n",
    "              'meta_CasesCited' : cases_cited, \n",
    "              'meta_Catchwords' : catchwords}\n",
    "    \n",
    "    #response = requests.get(base_url, params=params)\n",
    "    #response.raise_for_status()\n",
    "    #Get search url\n",
    "    #results_url = response.url\n",
    "\n",
    "    #Update results_url\n",
    "    params_for_selenium = urllib.parse.urlencode(params, quote_via=urllib.parse.quote)\n",
    "    results_url = base_url + '&' + params_for_selenium\n",
    "    \n",
    "    #st.write(f\"results_url == {results_url}\")\n",
    "    \n",
    "    #Initialise the number of search results\n",
    "    results_count = int(0)\n",
    "\n",
    "    #Initialise default soup\n",
    "    soup = ''\n",
    "    \n",
    "    try:\n",
    "        #soup = BeautifulSoup(response.content, \"lxml\")\n",
    "\n",
    "        browser = get_driver()\n",
    "        \n",
    "        browser.get(results_url)\n",
    "        \n",
    "        #print(f\"soup == {soup}\")\n",
    "\n",
    "        #Wait until number of search results present\n",
    "        loaded = Wait(browser, 15).until(EC.presence_of_element_located((By.XPATH, \"//p[@class='txarial']\")))\n",
    "\n",
    "        soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "        \n",
    "        results_num_raw = soup.find('p', {'class': 'txarial'})\n",
    "\n",
    "        #print(f\"results_num_raw == {results_num_raw}\")\n",
    "\n",
    "        results_num_raw_text = results_num_raw.get_text(strip = True)\n",
    "\n",
    "        #print(f\"results_num_raw_text == {results_num_raw_text}\")\n",
    "        \n",
    "        results_count = results_num_raw_text.split('\\n')[0].split(' ')[-1]\n",
    "        \n",
    "        results_count = results_count.replace(',', '').replace('.', '')\n",
    "\n",
    "        #print(f\"results_count == {results_count}\")\n",
    "\n",
    "        results_count = int(float(results_count))\n",
    "\n",
    "        browser.quit()\n",
    "    \n",
    "    except Exception as e:\n",
    "        \n",
    "        print(f\"Can't get search results due to error: {e}\")\n",
    "\n",
    "    return {'soup': soup, 'results_url': results_url, 'results_count': results_count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6321d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 07:53:01.970 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "#Define function turning search results url to case_infos to judgments\n",
    "\n",
    "#@st.cache_data(show_spinner = False, ttl=600)\n",
    "def fca_search_results_to_judgment_links(_soup, url_search_results, judgment_counter_bound):\n",
    "    \n",
    "    #_soup is from scraping per fca_search\n",
    "    \n",
    "    #Start counter\n",
    "\n",
    "    counter = 0\n",
    "    \n",
    "    # Get case_infos of first 20 results\n",
    "    \n",
    "    case_infos = []\n",
    "\n",
    "    results_list = _soup.find_all('div', attrs={'class' : 'result'})\n",
    "\n",
    "    #print(f'At initial , len(results_list) == {len(results_list)}')\n",
    "    \n",
    "    for result in results_list:\n",
    "        \n",
    "        if counter < judgment_counter_bound:\n",
    "\n",
    "            #Initialise default values\n",
    "            title = ''\n",
    "            case_name = ''\n",
    "            mnc = ''\n",
    "            link_to_case = ''\n",
    "            date = ''\n",
    "            judge = ''\n",
    "            catchwords = ''\n",
    "            subject = ''\n",
    "            \n",
    "            #Get full title\n",
    "            \n",
    "            title = result.h3.get_text(strip = True)\n",
    "\n",
    "            #Get PDF status\n",
    "            pdf_status = False\n",
    "            \n",
    "            if '(pdf' in title.lower():\n",
    "                \n",
    "                pdf_status = True\n",
    "            \n",
    "            #Get case name and mnc\n",
    "            case_name_mnc = split_title_mnc(title)\n",
    "            \n",
    "            case_name = case_name_mnc[0]\n",
    "            \n",
    "            mnc = case_name_mnc[1]\n",
    "            \n",
    "            if '(PDF' in mnc:\n",
    "                mnc = mnc.replace('(PDF', '')\n",
    "            \n",
    "            #Get link to case\n",
    "            link_to_case = result.h3.find('a').get('href')\n",
    "\n",
    "            #Get decision date, subject area, judge\n",
    "            date_area_court_str = str(result.find('p', attrs={'class' : 'meta'}))\n",
    "            date_area_court_raw = str(date_area_court_str).split('<span class=\"divide\"></span>')\n",
    "\n",
    "            date = date_area_court_raw[0].replace('<p class=\"meta\">', '')\n",
    "            \n",
    "            if len(date) > 0:\n",
    "                if date[-1] == ' ':\n",
    "                    date = date[: -1]\n",
    "            \n",
    "            judge = date_area_court_raw[-1].replace('</p>', '')\n",
    "            \n",
    "            subject = result.find('p', attrs={'class' : 'meta'}).text.replace(date, '').replace(judge, '')\n",
    "            \n",
    "            if len(subject) > 0:\n",
    "                if subject[0] == ' ':\n",
    "                    subject = subject[1:]\n",
    "\n",
    "            #Get catchwords\n",
    "            catchwords = ''\n",
    "            try:\n",
    "                catchwords = result.find('p', attrs={'class' : 'summary'}).get_text(strip = True)\n",
    "            except:\n",
    "                print(f\"{case_name}: can't get catchwords\")\n",
    "            \n",
    "            case_info = {'Case name': case_name,\n",
    "                 'Medium neutral citation': mnc,\n",
    "                'Hyperlink to Federal Court Digital Law Library' : link_to_case,\n",
    "                'Judge': judge,\n",
    "                 'Judgment_Dated' : date,  \n",
    "                 'Catchwords' : catchwords,  \n",
    "                 'Subject' : subject,\n",
    "                'Judgment in PDF': pdf_status\n",
    "                        }\n",
    "            case_infos.append(case_info)\n",
    "            counter = counter + 1\n",
    "            #print(counter)\n",
    "\n",
    "    #Go beyond first 20 results\n",
    "\n",
    "    #Auxiliary list for getting more pages of search results\n",
    "    further_page_ending_list = []\n",
    "    for i in range(100):\n",
    "        further_page_ending = 20 + i\n",
    "        if ((str(further_page_ending)[-1] =='1') & (str(further_page_ending)[0] not in ['3', '5', '7', '9', '11'])):\n",
    "            further_page_ending_list.append(str(further_page_ending))\n",
    "    \n",
    "    for ending in further_page_ending_list:\n",
    "        \n",
    "        if counter < judgment_counter_bound:\n",
    "\n",
    "            pause.seconds(np.random.randint(5, 15))\n",
    "\n",
    "            url_next_page = url_search_results + '&start_rank=' + f\"{ending}\"\n",
    "            \n",
    "            #page_judgment_next_page = requests.get(url_next_page)\n",
    "            #soup_judgment_next_page = BeautifulSoup(page_judgment_next_page.content, \"lxml\")\n",
    "\n",
    "            browser = get_driver()\n",
    "            \n",
    "            browser.get(url_next_page)\n",
    "    \n",
    "            #Wait until any search results present\n",
    "            loaded = Wait(browser, 15).until(EC.presence_of_element_located((By.XPATH, \"//div[@class='search-results']\")))\n",
    "            \n",
    "            soup_judgment_next_page = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "\n",
    "            browser.quit()\n",
    "\n",
    "            #print(f\"Searching url_next_page == {url_next_page}\")\n",
    "            \n",
    "            results_list = soup_judgment_next_page.find_all('div', attrs={'class' : 'result'})\n",
    "\n",
    "            #print(f'At page ending {ending}, len(results_list) == {len(results_list)}')\n",
    "\n",
    "            #Check if stll more results\n",
    "            if len(results_list) == 0:\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                for result in results_list:\n",
    "                    if counter < judgment_counter_bound:\n",
    "            \n",
    "                        #Initialise default values\n",
    "                        title = ''\n",
    "                        case_name = ''\n",
    "                        mnc = ''\n",
    "                        link_to_case = ''\n",
    "                        date = ''\n",
    "                        judge = ''\n",
    "                        catchwords = ''\n",
    "                        subject = ''\n",
    "                        \n",
    "                        #Get full title\n",
    "                        \n",
    "                        title = result.h3.get_text(strip = True)\n",
    "            \n",
    "                        #Get case name and mnc\n",
    "                        case_name_mnc = split_title_mnc(title)\n",
    "                        \n",
    "                        case_name = case_name_mnc[0]\n",
    "                        \n",
    "                        mnc = case_name_mnc[1]\n",
    "                        \n",
    "                        if '(PDF' in mnc:\n",
    "                            mnc = mnc.replace('(PDF', '')\n",
    "                        \n",
    "                        #Get link to case\n",
    "                        link_to_case = result.h3.find('a').get('href')\n",
    "            \n",
    "                        #Get decision date, subject area, judge\n",
    "                        date_area_court_str = str(result.find('p', attrs={'class' : 'meta'}))\n",
    "                        date_area_court_raw = str(date_area_court_str).split('<span class=\"divide\"></span>')\n",
    "            \n",
    "                        date = date_area_court_raw[0].replace('<p class=\"meta\">', '')\n",
    "                        \n",
    "                        if len(date) > 0:\n",
    "                            if date[-1] == ' ':\n",
    "                                date = date[: -1]\n",
    "                        \n",
    "                        judge = date_area_court_raw[-1].replace('</p>', '')\n",
    "                        \n",
    "                        subject = result.find('p', attrs={'class' : 'meta'}).text.replace(date, '').replace(judge, '')\n",
    "                        \n",
    "                        if len(subject) > 0:\n",
    "                            if subject[0] == ' ':\n",
    "                                subject = subject[1:]\n",
    "            \n",
    "                        #Get catchwords\n",
    "                        catchwords = ''\n",
    "                        try:\n",
    "                            catchwords = result.find('p', attrs={'class' : 'summary'}).get_text(strip = True)\n",
    "                        except:\n",
    "                            print(f\"{case_name}: can't get catchwords\")\n",
    "                            \n",
    "                        case_info = {'Case name': case_name,\n",
    "                             'Medium neutral citation': mnc,\n",
    "                            'Hyperlink to Federal Court Digital Law Library' : link_to_case,\n",
    "                            'Judge': judge,\n",
    "                             'Judgment_Dated' : date,  \n",
    "                             'Catchwords' : catchwords,  \n",
    "                             'Subject' : subject,  \n",
    "                                    }\n",
    "                        case_infos.append(case_info)\n",
    "                        counter = counter + 1\n",
    "                        #print(counter)\n",
    "\n",
    "                        #print(f'len(case_infos) == {len(case_infos)}')\n",
    "\n",
    "\n",
    "    return case_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "962fad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Meta labels and judgment combined\n",
    "#IN USE\n",
    "fca_metalabels = ['Year', 'Appeal', 'File_Number', 'Judge', 'Judgment_Dated', 'Catchwords', 'Subject', 'Words_Phrases', 'Legislation', 'Cases_Cited', 'Division', 'NPA', 'Sub_NPA', 'Pages', 'All_Parties', 'Jurisdiction', 'Reported', 'Summary', 'Corrigenda', 'Parties', 'Date.published', 'Appeal_to']\n",
    "#'MNC', 'FileName', 'Asset_ID', \n",
    "fca_metalabels_droppable = ['Year', 'Appeal', 'File_Number', 'Judge', 'Judgment_Dated', 'Catchwords', 'Subject', 'Words_Phrases', 'Legislation', 'Cases_Cited', 'Division', 'NPA', 'Sub_NPA', 'Pages', 'All_Parties', 'Jurisdiction', 'Reported', 'Summary', 'Corrigenda', 'Parties', 'Date.published', 'Appeal_to', 'Order']\n",
    "#'FileName', 'Asset_ID', \n",
    "\n",
    "#@st.cache_data(show_spinner = False)\n",
    "def fca_meta_judgment_dict(case_info):\n",
    "    \n",
    "    judgment_dict = copy.deepcopy(case_info)\n",
    "    \n",
    "    judgment_url = case_info['Hyperlink to Federal Court Digital Law Library']\n",
    "\n",
    "    #Make hyperlink clickable    \n",
    "    judgment_dict['Hyperlink to Federal Court Digital Law Library'] = link(judgment_url)\n",
    "\n",
    "    #Get judgment text\n",
    "    judgment_text = ''\n",
    "\n",
    "\n",
    "    #Check if getting taken to a PDF\n",
    "    if 'Judgment in PDF' not in judgment_dict.keys():\n",
    "\n",
    "        judgment_dict.update({'Judgment in PDF': False})\n",
    "    \n",
    "    #Check if not taken to a PDF\n",
    "    if not bool(judgment_dict['Judgment in PDF']):\n",
    "    \n",
    "        try:\n",
    "            \n",
    "            #page = requests.get(judgment_url)\n",
    "            \n",
    "            #soup = BeautifulSoup(page.content, \"lxml\")\n",
    "\n",
    "            browser = get_driver()\n",
    "            browser.get(judgment_url)\n",
    "    \n",
    "            #Wait until judgment present\n",
    "            loaded = Wait(browser, 15).until(EC.presence_of_element_located((By.XPATH, \"//div[@class='judgment_content']\")))\n",
    "\n",
    "            soup = BeautifulSoup(browser.page_source, \"lxml\")\n",
    "            \n",
    "            browser.quit()\n",
    "            \n",
    "            #Attach judgment\n",
    "            try:\n",
    "\n",
    "                #judgment_text_list_raw = soup.find_all(\"div\", {\"class\": \"judgment_content\"})\n",
    "\n",
    "                #print(f\"{judgment_dict['Case name']}: judgment_text_list_raw == {judgment_text_list_raw}\")\n",
    "\n",
    "                #judgment_text_list = [x.get_text(separator=\"\\n\", strip=True) for x in judgment_text_list_raw]\n",
    "\n",
    "                #judgment_text = '\\n'.join(judgment_text_list)\n",
    "\n",
    "                judgment_text = soup.find(\"div\", {\"class\": \"judgment_content\"}).get_text(separator=\"\\n\", strip=True)\n",
    "                \n",
    "                #print(f\"{judgment_dict['Case name']}: judgment_text == {judgment_text}\")\n",
    "                \n",
    "            except:\n",
    "                \n",
    "                judgment_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "            #Attach meta tags\n",
    "            meta_tags = soup.find_all(\"meta\")\n",
    "        \n",
    "            #Attach meta tags\n",
    "            if len(meta_tags)>0:\n",
    "                for tag_index in range(len(meta_tags)):\n",
    "                    meta_name = meta_tags[tag_index].get(\"name\")\n",
    "                    if meta_name in fca_metalabels:\n",
    "                        meta_content = meta_tags[tag_index].get(\"content\")\n",
    "                        judgment_dict.update({meta_name: meta_content})\n",
    "                        \n",
    "        except Exception as e:\n",
    "            \n",
    "            print(f\"{judgment_dict['Case name']}: can't get html judgment or meta due to error {e}.\")\n",
    "            \n",
    "    #Check if gets taken to a PDF\n",
    "    else:\n",
    "        \n",
    "        print(f\"{judgment_dict['Case name']}: trying to get pdf judgment\")\n",
    "        \n",
    "        #Get judgment pdf text\n",
    "        try:\n",
    "            \n",
    "            #judgment_text = pdf_judgment(url_or_path = judgment_url, url_given = True)\n",
    "\n",
    "            browser = get_driver()\n",
    "            browser.get(judgment_url)\n",
    "            \n",
    "            pdf_file = judgment_url.split('/')[-1]    \n",
    "\n",
    "            pdf_file = urllib.parse.unquote(pdf_file)\n",
    "            \n",
    "            pdf_path = f\"{download_dir}/{pdf_file.upper()}.pdf\"\n",
    "\n",
    "            #Limiting waiting time for downloading PDF to 1 min\n",
    "            \n",
    "            waiting_counter = 0\n",
    "            \n",
    "            while ((not os.path.exists(pdf_path)) and (waiting_counter < 10)):\n",
    "                pause.seconds(5)\n",
    "                waiting_counter += 1\n",
    "                            \n",
    "            print(f\"{case_info['Case name']}: Trying to OCR pdf from pdf_path == {pdf_path}\")\n",
    "\n",
    "            judgment_text = pdf_judgment(url_or_path = pdf_path, url_given = False)\n",
    "                                                                \n",
    "            #MUST remove pdf from download folder automatically or manually\n",
    "            os.remove(pdf_path)\n",
    "\n",
    "            browser.quit()\n",
    "            \n",
    "        except Exception as e:\n",
    "            \n",
    "            print(f\"{judgment_dict['Case name']}: can't get pdf judgment due to error {e}.\")\n",
    "\n",
    "    judgment_dict['judgment'] = judgment_text\n",
    "    \n",
    "    return judgment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbd2241e-0ecd-40f7-9d5b-e9269d58f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@st.cache_data(show_spinner = False)\n",
    "def fca_search_url(df_master):\n",
    "    df_master = df_master.fillna('')\n",
    "        \n",
    "    #Conduct search\n",
    "\n",
    "    #st.write(df_master)\n",
    "    \n",
    "    results_url_num = fca_search(court = df_master.loc[0, 'Courts'], \n",
    "                     case_name_mnc = df_master.loc[0, 'Case name or medium neutral citation'],\n",
    "                     judge = df_master.loc[0, 'Judge'], \n",
    "                     reported_citation = df_master.loc[0, 'Reported citation'],\n",
    "                     file_number  = df_master.loc[0, 'File number'],\n",
    "                     npa = df_master.loc[0, 'National practice area'], \n",
    "                     with_all_the_words  = df_master.loc[0, 'With all the words'], \n",
    "                     with_at_least_one_of_the_words = df_master.loc[0, 'With at least one of the words'],\n",
    "                     without_the_words = df_master.loc[0, 'Without the words'],\n",
    "                     phrase  = df_master.loc[0, 'Phrase'], \n",
    "                     proximity = df_master.loc[0, 'Proximity'], \n",
    "                     on_this_date = df_master.loc[0, 'On this date'], \n",
    "                     after_date = df_master.loc[0, 'Decision date is after'], \n",
    "                     before_date = df_master.loc[0, 'Decision date is before'], \n",
    "                     legislation = df_master.loc[0, 'Legislation'], \n",
    "                     cases_cited = df_master.loc[0, 'Cases cited'], \n",
    "                     catchwords = df_master.loc[0, 'Catchwords'] \n",
    "                    )\n",
    "\n",
    "    results_url = results_url_num['results_url']\n",
    "    results_count = results_url_num['results_count']\n",
    "    search_results_soup = results_url_num['soup']\n",
    "\n",
    "\n",
    "    #st.write({'results_url': results_url, 'results_count': results_count, 'soup': search_results_soup})\n",
    "    \n",
    "    return {'results_url': results_url, 'results_count': results_count, 'soup': search_results_soup}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a2d594",
   "metadata": {},
   "source": [
    "# GPT functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da3be60b-5f6b-44fe-9e59-1c51310b8824",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 07:53:03.016 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2025-02-21 07:53:03.017 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2025-02-21 07:53:03.018 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2025-02-21 07:53:03.019 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2025-02-21 07:53:03.020 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from functions.gpt_functions import GPT_label_dict, is_api_key_valid, gpt_input_cost, gpt_output_cost, tokens_cap, max_output, num_tokens_from_string, judgment_prompt_json, GPT_json, engage_GPT_json, gpt_batch_input\n",
    "#Import variables\n",
    "from functions.gpt_functions import basic_model, flagship_model#, role_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f20ba06e-2c4c-4ae6-ac7e-06a8d84d4252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For checking questions and answers\n",
    "from functions.common_functions import check_questions_answers\n",
    "\n",
    "from functions.gpt_functions import questions_check_system_instruction, GPT_questions_check, checked_questions_json, answers_check_system_instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d529e704-08e0-4af6-98f0-b10e29d009ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jurisdiction specific instruction\n",
    "#system_instruction = role_content\n",
    "\n",
    "#intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df1201c6-e12d-40c8-b13a-9632ab67102f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 07:53:03.031 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "#For getting judgments from the Federal Court if unavailable in OALC\n",
    "\n",
    "@st.cache_data(show_spinner = False, ttl=600)\n",
    "def fca_run(df_master):\n",
    "    df_master = df_master.fillna('')\n",
    "\n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    df_master['questions_json'] = df_master['Enter your questions for GPT'].apply(GPT_label_dict)\n",
    "    \n",
    "    #Conduct search    \n",
    "    search_results_soup_url = fca_search(court = df_master.loc[0, 'Courts'], \n",
    "                     case_name_mnc = df_master.loc[0, 'Case name or medium neutral citation'],\n",
    "                     judge = df_master.loc[0, 'Judge'], \n",
    "                     reported_citation = df_master.loc[0, 'Reported citation'],\n",
    "                     file_number  = df_master.loc[0, 'File number'],\n",
    "                     npa = df_master.loc[0, 'National practice area'], \n",
    "                     with_all_the_words  = df_master.loc[0, 'With all the words'], \n",
    "                     with_at_least_one_of_the_words = df_master.loc[0, 'With at least one of the words'],\n",
    "                     without_the_words = df_master.loc[0, 'Without the words'],\n",
    "                     phrase  = df_master.loc[0, 'Phrase'], \n",
    "                     proximity = df_master.loc[0, 'Proximity'], \n",
    "                     on_this_date = df_master.loc[0, 'On this date'], \n",
    "                     after_date = df_master.loc[0, 'Decision date is after'], \n",
    "                     before_date = df_master.loc[0, 'Decision date is before'], \n",
    "                     legislation = df_master.loc[0, 'Legislation'], \n",
    "                     cases_cited = df_master.loc[0, 'Cases cited'], \n",
    "                     catchwords = df_master.loc[0, 'Catchwords'] \n",
    "                    )\n",
    "    \n",
    "    search_results_soup = search_results_soup_url['soup']\n",
    "    \n",
    "    results_url = search_results_soup_url['results_url']\n",
    "\n",
    "    results_count = search_results_soup_url['results_count']\n",
    "    \n",
    "    judgment_counter_bound = int(df_master.loc[0, 'Maximum number of judgments'])\n",
    "\n",
    "    #Get relevant cases\n",
    "    case_infos = fca_search_results_to_judgment_links(search_results_soup, results_url, min(results_count, judgment_counter_bound))\n",
    "\n",
    "    #Create judgments file\n",
    "    judgments_file = []\n",
    "    \n",
    "    if huggingface == False: #If not running on HuggingFace\n",
    "        \n",
    "        for case_info in case_infos:\n",
    "            \n",
    "            judgment_dict = fca_meta_judgment_dict(case_info)\n",
    "\n",
    "            print(f\"{case_info['Case name']} {case_info['Medium neutral citation']}: got judgment from the Federal Court directly\")\n",
    "\n",
    "            pause.seconds(np.random.randint(5, 15))\n",
    "            \n",
    "            judgments_file.append(judgment_dict)\n",
    "    \n",
    "    else: #If running on HuggingFace\n",
    "\n",
    "        #Create a list of mncs for HuggingFace:\n",
    "        mnc_list = []\n",
    "\n",
    "        for case in case_infos:\n",
    "\n",
    "            #Add mnc to list for HuggingFace\n",
    "            mnc_list.append(case['Medium neutral citation'])\n",
    "\n",
    "        #Get judgments from oalc first\n",
    "        mnc_judgment_dict = get_judgment_from_oalc(mnc_list)\n",
    "            \n",
    "        #Append judgment to judgments_file \n",
    "        for case_info in case_infos:\n",
    "            \n",
    "            #Append judgments from oalc first\n",
    "            if case_info['Medium neutral citation'] in mnc_judgment_dict.keys():\n",
    "                \n",
    "                case_info.update({'judgment': mnc_judgment_dict[case_info['Medium neutral citation']]})\n",
    "\n",
    "                #Make judgment_link clickable\n",
    "                clickable_link = link(case_info['Hyperlink to Federal Court Digital Law Library'])\n",
    "                case_info.update({'Hyperlink to Federal Court Digital Law Library': clickable_link})\n",
    "\n",
    "                #Create judgment_dict with oalc judgment text\n",
    "                judgment_dict = copy.deepcopy(case_info)\n",
    "                \n",
    "                print(f\"{case_info['Case name']} {case_info['Medium neutral citation']}: got judgment from OALC\")\n",
    "\n",
    "            else: #Get judgment from FCA if can't get from oalc\n",
    "                \n",
    "                judgment_dict = fca_meta_judgment_dict(case_info)\n",
    "        \n",
    "                print(f\"{case_info['Case name']} {case_info['Medium neutral citation']}: got judgment from the Federal Court directly\")\n",
    "                \n",
    "                pause.seconds(np.random.randint(5, 15))\n",
    "\n",
    "            judgments_file.append(judgment_dict)\n",
    "\n",
    "    #Create and export json file with search results\n",
    "    json_individual = json.dumps(judgments_file, indent=2)\n",
    "\n",
    "#    df_individual = pd.DataFrame(judgments_file)\n",
    "    \n",
    "    df_individual = pd.read_json(json_individual)\n",
    "    \n",
    "    #Instruct GPT\n",
    "    \n",
    "    #GPT model\n",
    "\n",
    "    if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "        gpt_model = flagship_model\n",
    "    else:        \n",
    "        gpt_model = basic_model\n",
    "        \n",
    "    #apply GPT_individual to each respondent's judgment spreadsheet\n",
    "    \n",
    "    GPT_activation = int(df_master.loc[0, 'Use GPT'])\n",
    "\n",
    "    questions_json = df_master.loc[0, 'questions_json']\n",
    "\n",
    "    system_instruction = df_master.loc[0, 'System instruction']\n",
    "    \n",
    "    #Engage GPT\n",
    "    df_updated = engage_GPT_json(questions_json = questions_json, df_example = df_master.loc[0, 'Example'], df_individual = df_individual, GPT_activation = GPT_activation, gpt_model = gpt_model, system_instruction = system_instruction)\n",
    "\n",
    "    #Pop jugdment\n",
    "    if (pop_judgment() > 0) and ('judgment' in df_updated.columns):\n",
    "        df_updated.pop('judgment')\n",
    "\n",
    "    #Drop metadata if not wanted\n",
    "\n",
    "    if int(float(df_master.loc[0, 'Metadata inclusion'])) == 0:\n",
    "        for meta_label in fca_metalabels_droppable:\n",
    "            try:\n",
    "                df_updated.pop(meta_label)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return df_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90e35be4-5b48-4711-bddf-bdbbd566e93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 07:53:03.037 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "#Obtain parameters\n",
    "\n",
    "@st.cache_data(show_spinner = False, ttl=600)\n",
    "def fca_batch(df_master):\n",
    "    df_master = df_master.fillna('')\n",
    "\n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    df_master['questions_json'] = df_master['Enter your questions for GPT'].apply(GPT_label_dict)\n",
    "    \n",
    "    #Conduct search    \n",
    "    search_results_soup_url = fca_search(court = df_master.loc[0, 'Courts'], \n",
    "                     case_name_mnc = df_master.loc[0, 'Case name or medium neutral citation'],\n",
    "                     judge = df_master.loc[0, 'Judge'], \n",
    "                     reported_citation = df_master.loc[0, 'Reported citation'],\n",
    "                     file_number  = df_master.loc[0, 'File number'],\n",
    "                     npa = df_master.loc[0, 'National practice area'], \n",
    "                     with_all_the_words  = df_master.loc[0, 'With all the words'], \n",
    "                     with_at_least_one_of_the_words = df_master.loc[0, 'With at least one of the words'],\n",
    "                     without_the_words = df_master.loc[0, 'Without the words'],\n",
    "                     phrase  = df_master.loc[0, 'Phrase'], \n",
    "                     proximity = df_master.loc[0, 'Proximity'], \n",
    "                     on_this_date = df_master.loc[0, 'On this date'], \n",
    "                     after_date = df_master.loc[0, 'Decision date is after'], \n",
    "                     before_date = df_master.loc[0, 'Decision date is before'], \n",
    "                     legislation = df_master.loc[0, 'Legislation'], \n",
    "                     cases_cited = df_master.loc[0, 'Cases cited'], \n",
    "                     catchwords = df_master.loc[0, 'Catchwords'] \n",
    "                    )\n",
    "    \n",
    "    search_results_soup = search_results_soup_url['soup']\n",
    "    \n",
    "    results_url = search_results_soup_url['results_url']\n",
    "\n",
    "    results_count = search_results_soup_url['results_count']\n",
    "    \n",
    "    judgment_counter_bound = int(df_master.loc[0, 'Maximum number of judgments'])\n",
    "\n",
    "    #Get relevant cases\n",
    "    case_infos = fca_search_results_to_judgment_links(search_results_soup, results_url, min(results_count, judgment_counter_bound))\n",
    "\n",
    "    #Create judgments file\n",
    "    judgments_file = []\n",
    "    \n",
    "    if huggingface == False: #If not running on HuggingFace\n",
    "        \n",
    "        for case_info in case_infos:\n",
    "            \n",
    "            judgment_dict = fca_meta_judgment_dict(case_info)\n",
    "\n",
    "            print(f\"{case_info['Case name']} {case_info['Medium neutral citation']}: got judgment from the Federal Court directly\")\n",
    "\n",
    "            pause.seconds(np.random.randint(5, 15))\n",
    "            \n",
    "            judgments_file.append(judgment_dict)\n",
    "    \n",
    "    else: #If running on HuggingFace\n",
    "\n",
    "        #Create a list of mncs for HuggingFace:\n",
    "        mnc_list = []\n",
    "\n",
    "        for case in case_infos:\n",
    "\n",
    "            #Add mnc to list for HuggingFace\n",
    "            mnc_list.append(case['Medium neutral citation'])\n",
    "\n",
    "        #Get judgments from oalc first\n",
    "        mnc_judgment_dict = get_judgment_from_oalc(mnc_list)\n",
    "            \n",
    "        #Append judgment to judgments_file \n",
    "        for case_info in case_infos:\n",
    "            \n",
    "            #Append judgments from oalc first\n",
    "            if case_info['Medium neutral citation'] in mnc_judgment_dict.keys():\n",
    "                \n",
    "                case_info.update({'judgment': mnc_judgment_dict[case_info['Medium neutral citation']]})\n",
    "\n",
    "                #Make judgment_link clickable\n",
    "                clickable_link = link(case_info['Hyperlink to Federal Court Digital Law Library'])\n",
    "                case_info.update({'Hyperlink to Federal Court Digital Law Library': clickable_link})\n",
    "\n",
    "                #Create judgment_dict with oalc judgment text\n",
    "                judgment_dict = copy.deepcopy(case_info)\n",
    "                \n",
    "                print(f\"{case_info['Case name']} {case_info['Medium neutral citation']}: got judgment from OALC\")\n",
    "\n",
    "            else: #Get judgment from FCA if can't get from oalc\n",
    "                \n",
    "                judgment_dict = fca_meta_judgment_dict(case_info)\n",
    "        \n",
    "                print(f\"{case_info['Case name']} {case_info['Medium neutral citation']}: got judgment from the Federal Court directly\")\n",
    "                \n",
    "                pause.seconds(np.random.randint(5, 15))\n",
    "\n",
    "            judgments_file.append(judgment_dict)\n",
    "\n",
    "    #Create and export json file with search results\n",
    "    json_individual = json.dumps(judgments_file, indent=2)\n",
    "\n",
    "#    df_individual = pd.DataFrame(judgments_file)\n",
    "    \n",
    "    df_individual = pd.read_json(json_individual)\n",
    "                        \n",
    "    #Drop metadata if not wanted\n",
    "\n",
    "    if int(float(df_master.loc[0, 'Metadata inclusion'])) == 0:\n",
    "        for meta_label in fca_metalabels_droppable:\n",
    "            if meta_label in df_individual.columns:\n",
    "                df_individual.pop(meta_label)\n",
    "    \n",
    "    #Instruct GPT\n",
    "    \n",
    "    #GPT model\n",
    "\n",
    "    if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "        gpt_model = flagship_model\n",
    "    else:        \n",
    "        gpt_model = basic_model\n",
    "        \n",
    "    #apply GPT_individual to each respondent's judgment spreadsheet\n",
    "    \n",
    "    GPT_activation = int(df_master.loc[0, 'Use GPT'])\n",
    "\n",
    "    questions_json = df_master.loc[0, 'questions_json']\n",
    "\n",
    "    system_instruction = df_master.loc[0, 'System instruction']\n",
    "    \n",
    "    #Send batch input to gpt\n",
    "    batch_record_df_individual = gpt_batch_input(questions_json = questions_json, df_example = df_master.loc[0, 'Example'], df_individual = df_individual, GPT_activation = GPT_activation, gpt_model = gpt_model, system_instruction = system_instruction)\n",
    "    \n",
    "    return batch_record_df_individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d3ee80-003b-4657-a24e-96134bfe0a45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
