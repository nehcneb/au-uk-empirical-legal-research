{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b8b879",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2312235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminary modules\n",
    "import base64 \n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date\n",
    "from dateutil import parser\n",
    "#from dateutil.relativedelta import *\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import pause\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import httplib2\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "#import pypdf\n",
    "import io\n",
    "from io import BytesIO\n",
    "\n",
    "#Streamlit\n",
    "import streamlit as st\n",
    "#from streamlit_gsheets import GSheetsConnection\n",
    "from streamlit.components.v1 import html\n",
    "#import streamlit_ext as ste\n",
    "from streamlit_extras.stylable_container import stylable_container\n",
    "\n",
    "#OpenAI\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "#Google\n",
    "#from google.oauth2 import service_account\n",
    "\n",
    "#Excel\n",
    "from pyxlsb import open_workbook as open_xlsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111133cd-06ce-45a6-8556-91199246e001",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface == True\n"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from functions.common_functions import own_account_allowed, pop_judgment, convert_df_to_json, convert_df_to_csv, convert_df_to_excel, clear_cache, list_range_check, au_date, save_input, pdf_judgment\n",
    "#Import variables\n",
    "from functions.common_functions import huggingface, today_in_nums, errors_list, scraper_pause_mean, judgment_text_lower_bound, default_judgment_counter_bound, no_results_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e819e27",
   "metadata": {},
   "source": [
    "# Federal Courts search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f3329d8-1716-4570-8dd0-4b5aa22aaed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.common_functions import link, split_title_mnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cb5735b-cbd1-454a-937d-bdefcdcbcb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define format functions for courts choice, and GPT questions\n",
    "\n",
    "#auxiliary lists and variables\n",
    "\n",
    "fca_courts = {'Federal Court': 'fca', \n",
    "              'Industrial Relations Court of Australia': 'irc', \n",
    "              'Australian Competition Tribunal': 'tribunals%2Facompt', \n",
    "              'Copyright Tribunal': 'tribunals%2Facopyt', \n",
    "              'Defence Force Discipline Appeal Tribunal': 'tribunals%2Fadfdat', \n",
    "              'Federal Police Discipline Tribunal': 'tribunals%2Ffpdt', \n",
    "              'Trade Practices Tribunal': 'tribunals%2Fatpt', \n",
    "              'Supreme Court of Norfolk Island': 'nfsc',\n",
    "             'All': ''}\n",
    "\n",
    "fca_courts_list = list(fca_courts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2a0e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function turning search terms to search results url\n",
    "\n",
    "#@st.cache_data(show_spinner = False)\n",
    "def fca_search(court = '', \n",
    "               case_name_mnc= '', \n",
    "               judge ='', \n",
    "               reported_citation ='', \n",
    "               file_number ='', \n",
    "               npa = '', \n",
    "               with_all_the_words = '', \n",
    "               with_at_least_one_of_the_words = '', \n",
    "               without_the_words = '', \n",
    "               phrase = '', \n",
    "               proximity = '', \n",
    "               on_this_date = '', \n",
    "               after_date = '', \n",
    "               before_date = '', \n",
    "               legislation = '', \n",
    "               cases_cited = '', \n",
    "               catchwords = ''):\n",
    "\n",
    "    #If only searching FCA\n",
    "    #base_url = \"https://search2.fedcourt.gov.au/s/search.html?collection=judgments&sort=date&meta_v_phrase_orsand=judgments%2FJudgments%2Ffca\"\n",
    "\n",
    "    #If allowing users to search which court\n",
    "    base_url = \"https://search2.fedcourt.gov.au/s/search.html?collection=judgments&sort=date&meta_v_phrase_orsand=judgments%2FJudgments%2F\" + fca_courts[court]\n",
    "\n",
    "\n",
    "    #Tidy up dates for batch mode\n",
    "    if '-' in str(after_date):\n",
    "        after_date = str(after_date).replace('-', '')\n",
    "\n",
    "    if '/' in str(after_date):\n",
    "        after_date = str(after_date).replace('/', '')\n",
    "    \n",
    "    if '-' in str(before_date):\n",
    "        before_date = str(before_date).replace('-', '')\n",
    "\n",
    "    if '/' in str(after_date):\n",
    "        before_date = str(before_date).replace('/', '')\n",
    "    \n",
    "    params = {'meta_2' : case_name_mnc, \n",
    "              'meta_A' : judge, \n",
    "              'meta_z' : reported_citation, \n",
    "              'meta_3' : file_number, \n",
    "              'meta_n_phrase_orsand' : npa, \n",
    "              'query_sand' : with_all_the_words, \n",
    "              'query_or' : with_at_least_one_of_the_words, \n",
    "              'query_not' : without_the_words, \n",
    "              'query_phrase' : phrase, \n",
    "              'query_prox' : proximity, \n",
    "              'meta_d' : on_this_date, \n",
    "              'meta_d1' : after_date, \n",
    "              'meta_d2' : before_date, \n",
    "              'meta_7' : legislation, \n",
    "              'meta_4' : cases_cited, \n",
    "              'meta_B' : catchwords}\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    # Process the response (e.g., extract relevant information)\n",
    "    # Your code here...\n",
    "\n",
    "    #Get search url\n",
    "    results_url = response.url\n",
    "\n",
    "    #Get the number of search results\n",
    "    results_count = int(0)\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(response.content, \"lxml\")\n",
    "        results_num_raw = soup.find('p', {'class': 'txarial'})\n",
    "        results_num_raw_text = results_num_raw.get_text(strip = True)\n",
    "        results_count = results_num_raw_text.split('\\r\\n')[0].split(' ')[-1]\n",
    "        results_count = results_count.replace(',', '').replace('.', '')\n",
    "        results_count = int(float(results_count))\n",
    "\n",
    "    except:\n",
    "        print(\"Can't get the number of search results\")\n",
    "\n",
    "    #Get soup\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "\n",
    "    return {'soup': soup, 'results_url': results_url, 'results_count': results_count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6321d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 07:53:01.970 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "#Define function turning search results url to case_infos to judgments\n",
    "\n",
    "@st.cache_data(show_spinner = False, ttl=600)\n",
    "def fca_search_results_to_judgment_links(_soup, url_search_results, judgment_counter_bound):\n",
    "    \n",
    "    #_soup is from scraping per fca_search\n",
    "    \n",
    "    #Start counter\n",
    "\n",
    "    counter = 0\n",
    "    \n",
    "    # Get case_infos of first 20 results\n",
    "    \n",
    "    case_infos = []\n",
    "\n",
    "    results_list = _soup.find_all('div', attrs={'class' : 'result'})\n",
    "\n",
    "    #print(f'At initial , len(results_list) == {len(results_list)}')\n",
    "    \n",
    "    for result in results_list:\n",
    "        \n",
    "        if counter < judgment_counter_bound:\n",
    "\n",
    "            #Initialise default values\n",
    "            title = ''\n",
    "            case_name = ''\n",
    "            mnc = ''\n",
    "            link_to_case = ''\n",
    "            date = ''\n",
    "            judge = ''\n",
    "            catchwords = ''\n",
    "            subject = ''\n",
    "            \n",
    "            #Get full title\n",
    "            \n",
    "            title = result.h3.get_text(strip = True)\n",
    "\n",
    "            #Get case name and mnc\n",
    "            case_name_mnc = split_title_mnc(title)\n",
    "            \n",
    "            case_name = case_name_mnc[0]\n",
    "            \n",
    "            mnc = case_name_mnc[1]\n",
    "            \n",
    "            if '(PDF' in mnc:\n",
    "                mnc = mnc.replace('(PDF', '')\n",
    "            \n",
    "            #Get link to case\n",
    "            link_to_case = result.h3.find('a').get('href')\n",
    "\n",
    "            #Get decision date, subject area, judge\n",
    "            date_area_court_str = str(result.find('p', attrs={'class' : 'meta'}))\n",
    "            date_area_court_raw = str(date_area_court_str).split('<span class=\"divide\"></span>')\n",
    "\n",
    "            date = date_area_court_raw[0].replace('<p class=\"meta\">', '')\n",
    "            \n",
    "            if len(date) > 0:\n",
    "                if date[-1] == ' ':\n",
    "                    date = date[: -1]\n",
    "            \n",
    "            judge = date_area_court_raw[-1].replace('</p>', '')\n",
    "            \n",
    "            subject = result.find('p', attrs={'class' : 'meta'}).text.replace(date, '').replace(judge, '')\n",
    "            \n",
    "            if len(subject) > 0:\n",
    "                if subject[0] == ' ':\n",
    "                    subject = subject[1:]\n",
    "\n",
    "            #Get catchwords\n",
    "            catchwords = ''\n",
    "            try:\n",
    "                catchwords = result.find('p', attrs={'class' : 'summary'}).get_text(strip = True)\n",
    "            except:\n",
    "                print(f\"{case_name}: can't get catchwords\")\n",
    "            \n",
    "            case_info = {'Case name': case_name,\n",
    "                 'Medium neutral citation': mnc,\n",
    "                'Hyperlink to Federal Court Digital Law Library' : link_to_case,\n",
    "                'Judge': judge,\n",
    "                 'Judgment_Dated' : date,  \n",
    "                 'Catchwords' : catchwords,  \n",
    "                 'Subject' : subject,  \n",
    "                        }\n",
    "            case_infos.append(case_info)\n",
    "            counter = counter + 1\n",
    "            #print(counter)\n",
    "\n",
    "    #Go beyond first 20 results\n",
    "\n",
    "    #Auxiliary list for getting more pages of search results\n",
    "    further_page_ending_list = []\n",
    "    for i in range(100):\n",
    "        further_page_ending = 20 + i\n",
    "        if ((str(further_page_ending)[-1] =='1') & (str(further_page_ending)[0] not in ['3', '5', '7', '9', '11'])):\n",
    "            further_page_ending_list.append(str(further_page_ending))\n",
    "    \n",
    "    for ending in further_page_ending_list:\n",
    "        \n",
    "        if counter < judgment_counter_bound:\n",
    "\n",
    "            pause.seconds(np.random.randint(5, 15))\n",
    "\n",
    "            url_next_page = url_search_results + '&start_rank=' + f\"{ending}\"\n",
    "            page_judgment_next_page = requests.get(url_next_page)\n",
    "            soup_judgment_next_page = BeautifulSoup(page_judgment_next_page.content, \"lxml\")\n",
    "\n",
    "            #print(f\"Searching url_next_page == {url_next_page}\")\n",
    "            \n",
    "            results_list = soup_judgment_next_page.find_all('div', attrs={'class' : 'result'})\n",
    "\n",
    "            #print(f'At page ending {ending}, len(results_list) == {len(results_list)}')\n",
    "\n",
    "            #Check if stll more results\n",
    "            if len(results_list) == 0:\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                for result in results_list:\n",
    "                    if counter < judgment_counter_bound:\n",
    "            \n",
    "                        #Initialise default values\n",
    "                        title = ''\n",
    "                        case_name = ''\n",
    "                        mnc = ''\n",
    "                        link_to_case = ''\n",
    "                        date = ''\n",
    "                        judge = ''\n",
    "                        catchwords = ''\n",
    "                        subject = ''\n",
    "                        \n",
    "                        #Get full title\n",
    "                        \n",
    "                        title = result.h3.get_text(strip = True)\n",
    "            \n",
    "                        #Get case name and mnc\n",
    "                        case_name_mnc = split_title_mnc(title)\n",
    "                        \n",
    "                        case_name = case_name_mnc[0]\n",
    "                        \n",
    "                        mnc = case_name_mnc[1]\n",
    "                        \n",
    "                        if '(PDF' in mnc:\n",
    "                            mnc = mnc.replace('(PDF', '')\n",
    "                        \n",
    "                        #Get link to case\n",
    "                        link_to_case = result.h3.find('a').get('href')\n",
    "            \n",
    "                        #Get decision date, subject area, judge\n",
    "                        date_area_court_str = str(result.find('p', attrs={'class' : 'meta'}))\n",
    "                        date_area_court_raw = str(date_area_court_str).split('<span class=\"divide\"></span>')\n",
    "            \n",
    "                        date = date_area_court_raw[0].replace('<p class=\"meta\">', '')\n",
    "                        \n",
    "                        if len(date) > 0:\n",
    "                            if date[-1] == ' ':\n",
    "                                date = date[: -1]\n",
    "                        \n",
    "                        judge = date_area_court_raw[-1].replace('</p>', '')\n",
    "                        \n",
    "                        subject = result.find('p', attrs={'class' : 'meta'}).text.replace(date, '').replace(judge, '')\n",
    "                        \n",
    "                        if len(subject) > 0:\n",
    "                            if subject[0] == ' ':\n",
    "                                subject = subject[1:]\n",
    "            \n",
    "                        #Get catchwords\n",
    "                        catchwords = ''\n",
    "                        try:\n",
    "                            catchwords = result.find('p', attrs={'class' : 'summary'}).get_text(strip = True)\n",
    "                        except:\n",
    "                            print(f\"{case_name}: can't get catchwords\")\n",
    "                            \n",
    "                        case_info = {'Case name': case_name,\n",
    "                             'Medium neutral citation': mnc,\n",
    "                            'Hyperlink to Federal Court Digital Law Library' : link_to_case,\n",
    "                            'Judge': judge,\n",
    "                             'Judgment_Dated' : date,  \n",
    "                             'Catchwords' : catchwords,  \n",
    "                             'Subject' : subject,  \n",
    "                                    }\n",
    "                        case_infos.append(case_info)\n",
    "                        counter = counter + 1\n",
    "                        #print(counter)\n",
    "\n",
    "                        #print(f'len(case_infos) == {len(case_infos)}')\n",
    "\n",
    "\n",
    "    return case_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "962fad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Meta labels and judgment combined\n",
    "#IN USE\n",
    "fca_metalabels = ['Year', 'Appeal', 'File_Number', 'Judge', 'Judgment_Dated', 'Catchwords', 'Subject', 'Words_Phrases', 'Legislation', 'Cases_Cited', 'Division', 'NPA', 'Sub_NPA', 'Pages', 'All_Parties', 'Jurisdiction', 'Reported', 'Summary', 'Corrigenda', 'Parties', 'FileName', 'Asset_ID', 'Date.published', 'Appeal_to']\n",
    "#'MNC', \n",
    "fca_metalabels_droppable = ['Year', 'Appeal', 'File_Number', 'Judge', 'Judgment_Dated', 'Catchwords', 'Subject', 'Words_Phrases', 'Legislation', 'Cases_Cited', 'Division', 'NPA', 'Sub_NPA', 'Pages', 'All_Parties', 'Jurisdiction', 'Reported', 'Summary', 'Corrigenda', 'Parties', 'FileName', 'Asset_ID', 'Date.published', 'Appeal_to', 'Order']\n",
    "\n",
    "#@st.cache_data(show_spinner = False)\n",
    "def fca_meta_judgment_dict(case_info):\n",
    "    judgment_dict = {'Case name': '',\n",
    "                 'Medium neutral citation': '',\n",
    "                'Hyperlink to Federal Court Digital Law Library' : '', \n",
    "                #'MNC' : '',  \n",
    "                 'Year' : '',  \n",
    "                 'Appeal' : '',  \n",
    "                 'File_Number' : '',  \n",
    "                 'Judge' : '',  \n",
    "                 'Judgment_Dated' : '',  \n",
    "                 'Catchwords' : '',  \n",
    "                 'Subject' : '',  \n",
    "                 'Words_Phrases' : '',  \n",
    "                 'Legislation' : '',  \n",
    "                 'Cases_Cited' : '',  \n",
    "                 'Division' : '',  \n",
    "                 'NPA' : '',  \n",
    "                'Sub_NPA' : '', \n",
    "                 'Pages' : '',  \n",
    "                 'All_Parties' : '',  \n",
    "                 'Jurisdiction' : '',  \n",
    "                 'Reported' : '',  \n",
    "                 'Summary' : '',  \n",
    "                 'Corrigenda' : '',  \n",
    "                 'Parties' : '',  'FileName' : '',  \n",
    "                 'Asset_ID' : '',  \n",
    "                 'Date.published' : '', \n",
    "                'Appeal_to' : '', \n",
    "                'Order': '',\n",
    "                'judgment' : ''\n",
    "                }\n",
    "\n",
    "    try:\n",
    "    \n",
    "        if 'Case name' in case_info.keys():\n",
    "            judgment_dict['Case name'] = case_info['Case name']\n",
    "    \n",
    "        if 'Medium neutral citation' in case_info.keys():\n",
    "            judgment_dict['Medium neutral citation'] = case_info['Medium neutral citation']\n",
    "    \n",
    "        #Attach hyperlink\n",
    "    \n",
    "        judgment_url = case_info['Hyperlink to Federal Court Digital Law Library']\n",
    "        \n",
    "        judgment_dict['Hyperlink to Federal Court Digital Law Library'] = link(judgment_url)\n",
    "        \n",
    "        page = requests.get(judgment_url)\n",
    "        soup = BeautifulSoup(page.content, \"lxml\")\n",
    "        meta_tags = soup.find_all(\"meta\")\n",
    "    \n",
    "        #Attach meta tags\n",
    "        if len(meta_tags)>0:\n",
    "            for tag_index in range(len(meta_tags)):\n",
    "                meta_name = meta_tags[tag_index].get(\"name\")\n",
    "                if meta_name in fca_metalabels:\n",
    "                    meta_content = meta_tags[tag_index].get(\"content\")\n",
    "                    judgment_dict[meta_name] = meta_content\n",
    "    \n",
    "        #Check if not gets taken to a PDF\n",
    "    \n",
    "        if '.pdf' not in judgment_url.lower():\n",
    "        \n",
    "            judgment_text = ''\n",
    "            order_text = ''\n",
    "        \n",
    "            try:\n",
    "                judgment_raw = ''\n",
    "                judgment_raw = soup.find(\"div\", {\"class\": \"judgment_content\"}).get_text(separator=\"\\n\", strip=True)\n",
    "        \n",
    "                above_reasons_for_judgment = str(re.split(\"REASONS FOR JUDGMENT\", judgment_raw, flags=re.IGNORECASE)[0])\n",
    "        \n",
    "                below_reasons_for_judgment = str(re.split(\"REASONS FOR JUDGMENT\", judgment_raw, flags=re.IGNORECASE)[1:])\n",
    "        \n",
    "                order_text = \"BETWEEEN:\" + str(re.split(\"BETWEEN:\", above_reasons_for_judgment, flags=re.IGNORECASE)[1:])[2:][:-2]\n",
    "        \n",
    "                judgment_text = below_reasons_for_judgment\n",
    "        \n",
    "            except:\n",
    "                try:\n",
    "                    judgment_text = soup.find(\"div\", {\"class\": \"judgment_content\"}).get_text(separator=\"\\n\", strip=True)\n",
    "                except:\n",
    "                    judgment_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "            \n",
    "            judgment_dict['judgment'] = judgment_text\n",
    "            judgment_dict['Order'] = order_text\n",
    "    \n",
    "        #Check if gets taken to a PDF\n",
    "    \n",
    "        else:\n",
    "\n",
    "            #Attach judgment pdf text\n",
    "            try:\n",
    "                judgment_pdf_raw = pdf_judgment(judgment_url)\n",
    "                judgment_dict['judgment'] = judgment_pdf_raw\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{judgment_dict['Case name']}: judgment not scrapped\")\n",
    "        print(e)\n",
    "    \n",
    "    return judgment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a30f606a-d641-4752-8fde-07f5b04d5596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 07:53:01.979 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "#Preliminary function for changing names for any PDF judgments\n",
    "#NOT IN USE\n",
    "\n",
    "@st.cache_data(show_spinner = False)\n",
    "def fca_pdf_name_mnc_list(url_search_results, judgment_counter_bound):\n",
    "                      \n",
    "    #Scrape webpage of search results\n",
    "    page = requests.get(url_search_results)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "    \n",
    "    #Placeholder\n",
    "    name_mnc_list = []\n",
    "\n",
    "    #Start counter\n",
    "    counter = 1\n",
    "    # Get links of first 20 results\n",
    "    #links_raw = soup.find_all(\"a\", href=re.compile(\"fca\")) #If want to search FCA only\n",
    "    links_raw = soup.find_all(\"a\", href=re.compile(\"judgments\"))\n",
    "    \n",
    "    for i in links_raw:\n",
    "        if (('title=' in str(i)) and (counter <=judgment_counter_bound)):\n",
    "            name_mnc_list.append(i['title'])\n",
    "            counter = counter + 1\n",
    "    \n",
    "    #Go beyond first 20 results\n",
    "\n",
    "    #Auxiliary list for getting more pages of search results\n",
    "    further_page_ending_list = []\n",
    "    for i in range(100):\n",
    "        further_page_ending = 20 + i\n",
    "        if ((str(further_page_ending)[-1] =='1') & (str(further_page_ending)[0] not in ['3', '5', '7', '9', '11'])):\n",
    "            further_page_ending_list.append(str(further_page_ending))\n",
    "\n",
    "    for ending in further_page_ending_list:\n",
    "        if counter <=judgment_counter_bound:\n",
    "            \n",
    "            pause.seconds(np.random.randint(5, 15))\n",
    "\n",
    "            url_next_page = url_search_results + '&start_rank=' + f\"{ending}\"\n",
    "            page_judgment_next_page = requests.get(url_next_page)\n",
    "            soup_judgment_next_page = BeautifulSoup(page_judgment_next_page.content, \"lxml\")\n",
    "            #links_next_page_raw = soup_judgment_next_page.find_all(\"a\", href=re.compile(\"fca\")) #If want to search FCA only\n",
    "            links_next_page_raw = soup_judgment_next_page.find_all(\"a\", href=re.compile(\"judgments\"))\n",
    "    \n",
    "            #Check if stll more results\n",
    "            if len(links_next_page_raw) > 0:\n",
    "                for i in links_next_page_raw:\n",
    "                    if (('title=' in str(i)) and (counter <=judgment_counter_bound)):\n",
    "                        name_mnc_list.append(i['title'])\n",
    "                        counter = counter + 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "    return name_mnc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8810eb17-ef74-4083-9801-f8c36395344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for changing names for any PDF judgments\n",
    "#NOT IN USE\n",
    "\n",
    "def fca_pdf_name(name_mnc_list, mnc):\n",
    "    #Placeholder\n",
    "    name = 'Not working properly because judgment in PDF. References to paragraphs likely to pages or wrong.' \n",
    "    \n",
    "    for i in name_mnc_list:\n",
    "        if mnc in i:\n",
    "            name_raw = i.split(' ' + mnc)[0]\n",
    "            name = name_raw.replace('Cached: ', '')\n",
    "            \n",
    "    return name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbd2241e-0ecd-40f7-9d5b-e9269d58f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@st.cache_data(show_spinner = False)\n",
    "def fca_search_url(df_master):\n",
    "    df_master = df_master.fillna('')\n",
    "        \n",
    "    #Conduct search\n",
    "    \n",
    "    results_url_num = fca_search(court = df_master.loc[0, 'Courts'], \n",
    "                     case_name_mnc = df_master.loc[0, 'Case name or medium neutral citation'],\n",
    "                     judge = df_master.loc[0, 'Judge'], \n",
    "                     reported_citation = df_master.loc[0, 'Reported citation'],\n",
    "                     file_number  = df_master.loc[0, 'File number'],\n",
    "                     npa = df_master.loc[0, 'National practice area'], \n",
    "                     with_all_the_words  = df_master.loc[0, 'With all the words'], \n",
    "                     with_at_least_one_of_the_words = df_master.loc[0, 'With at least one of the words'],\n",
    "                     without_the_words = df_master.loc[0, 'Without the words'],\n",
    "                     phrase  = df_master.loc[0, 'Phrase'], \n",
    "                     proximity = df_master.loc[0, 'Proximity'], \n",
    "                     on_this_date = df_master.loc[0, 'On this date'], \n",
    "                     after_date = df_master.loc[0, 'Decision date is after'], \n",
    "                     before_date = df_master.loc[0, 'Decision date is before'], \n",
    "                     legislation = df_master.loc[0, 'Legislation'], \n",
    "                     cases_cited = df_master.loc[0, 'Cases cited'], \n",
    "                     catchwords = df_master.loc[0, 'Catchwords'] \n",
    "                    )\n",
    "\n",
    "    results_url = results_url_num['results_url']\n",
    "    results_count = results_url_num['results_count']\n",
    "    search_results_soup = results_url_num['soup']\n",
    "    \n",
    "    return {'results_url': results_url, 'results_count': results_count, 'soup': search_results_soup}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a2d594",
   "metadata": {},
   "source": [
    "# GPT functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da3be60b-5f6b-44fe-9e59-1c51310b8824",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 07:53:03.016 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2025-02-21 07:53:03.017 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2025-02-21 07:53:03.018 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2025-02-21 07:53:03.019 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2025-02-21 07:53:03.020 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from functions.gpt_functions import split_by_line, GPT_label_dict, is_api_key_valid, gpt_input_cost, gpt_output_cost, tokens_cap, max_output, num_tokens_from_string, judgment_prompt_json, GPT_json, engage_GPT_json, gpt_batch_input\n",
    "#Import variables\n",
    "from functions.gpt_functions import question_characters_bound, role_content#, intro_for_GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f20ba06e-2c4c-4ae6-ac7e-06a8d84d4252",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#For checking questions and answers\n",
    "from functions.common_functions import check_questions_answers\n",
    "\n",
    "from functions.gpt_functions import questions_check_system_instruction, GPT_questions_check, checked_questions_json, answers_check_system_instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d529e704-08e0-4af6-98f0-b10e29d009ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jurisdiction specific instruction\n",
    "system_instruction = role_content\n",
    "\n",
    "intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df1201c6-e12d-40c8-b13a-9632ab67102f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 07:53:03.031 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "#For getting judgments from the Federal Court if unavailable in OALC\n",
    "\n",
    "@st.cache_data(show_spinner = False, ttl=600)\n",
    "def fca_run(df_master):\n",
    "    df_master = df_master.fillna('')\n",
    "\n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    df_master['Enter your questions for GPT'] = df_master['Enter your questions for GPT'][0: question_characters_bound].apply(split_by_line)\n",
    "    df_master['questions_json'] = df_master['Enter your questions for GPT'].apply(GPT_label_dict)\n",
    "    \n",
    "    #Conduct search    \n",
    "    search_results_soup_url = fca_search(court = df_master.loc[0, 'Courts'], \n",
    "                     case_name_mnc = df_master.loc[0, 'Case name or medium neutral citation'],\n",
    "                     judge = df_master.loc[0, 'Judge'], \n",
    "                     reported_citation = df_master.loc[0, 'Reported citation'],\n",
    "                     file_number  = df_master.loc[0, 'File number'],\n",
    "                     npa = df_master.loc[0, 'National practice area'], \n",
    "                     with_all_the_words  = df_master.loc[0, 'With all the words'], \n",
    "                     with_at_least_one_of_the_words = df_master.loc[0, 'With at least one of the words'],\n",
    "                     without_the_words = df_master.loc[0, 'Without the words'],\n",
    "                     phrase  = df_master.loc[0, 'Phrase'], \n",
    "                     proximity = df_master.loc[0, 'Proximity'], \n",
    "                     on_this_date = df_master.loc[0, 'On this date'], \n",
    "                     after_date = df_master.loc[0, 'Decision date is after'], \n",
    "                     before_date = df_master.loc[0, 'Decision date is before'], \n",
    "                     legislation = df_master.loc[0, 'Legislation'], \n",
    "                     cases_cited = df_master.loc[0, 'Cases cited'], \n",
    "                     catchwords = df_master.loc[0, 'Catchwords'] \n",
    "                    )\n",
    "    \n",
    "    search_results_soup = search_results_soup_url['soup']\n",
    "    \n",
    "    results_url = search_results_soup_url['results_url']\n",
    "    \n",
    "    judgments_counter_bound = int(df_master.loc[0, 'Maximum number of judgments'])\n",
    "\n",
    "    #Get relevant cases\n",
    "    case_infos = fca_search_results_to_judgment_links(search_results_soup, results_url, judgments_counter_bound)\n",
    "\n",
    "    #Create judgments file\n",
    "    judgments_file = []\n",
    "    \n",
    "    if huggingface == False: #If not running on HuggingFace\n",
    "        \n",
    "        for case_info in case_infos:\n",
    "            judgment_dict = fca_meta_judgment_dict(case_info)\n",
    "            case_info.update({'judgment': str(judgment_dict)})\n",
    "            \n",
    "            #Make judgment_link clickable\n",
    "            clickable_link = link(case_info['Hyperlink to Federal Court Digital Law Library'])\n",
    "            case_info.update({'Hyperlink to Federal Court Digital Law Library': clickable_link})\n",
    "            \n",
    "            judgments_file.append(case_info)\n",
    "\n",
    "            print(f\"{case_info['Case name']} {case_info['Medium neutral citation']}: got judgment from the Federal Court directly\")\n",
    "            \n",
    "            pause.seconds(np.random.randint(5, 15))\n",
    "\n",
    "    else: #If running on HuggingFace\n",
    "\n",
    "        #Load oalc\n",
    "        from functions.oalc_functions import load_corpus, get_judgment_from_oalc\n",
    "\n",
    "        #Create a list of mncs for HuggingFace:\n",
    "        mnc_list = []\n",
    "\n",
    "        for case in case_infos:\n",
    "\n",
    "            #add search results to json\n",
    "            judgments_file.append(case)\n",
    "\n",
    "            #Add mnc to list for HuggingFace\n",
    "            mnc_list.append(case['Medium neutral citation'])\n",
    "\n",
    "        #Get judgments from oalc first\n",
    "        mnc_judgment_dict = get_judgment_from_oalc(mnc_list)\n",
    "            \n",
    "        #Append judgment to judgments_file \n",
    "        for case_info in judgments_file:\n",
    "            \n",
    "            #Append judgments from oalc first\n",
    "            if case_info['Medium neutral citation'] in mnc_judgment_dict.keys():\n",
    "                \n",
    "                case_info.update({'judgment': mnc_judgment_dict[case_info['Medium neutral citation']]})\n",
    "\n",
    "                print(f\"{case_info['Case name']} {case_info['Medium neutral citation']}: got judgment from OALC\")\n",
    "\n",
    "            else: #Get judgment from FCA if can't get from oalc\n",
    "                judgment_dict_direct = fca_meta_judgment_dict(case_info)\n",
    "                case_info.update({'judgment': str(judgment_dict_direct)})\n",
    "                \n",
    "                print(f\"{case_info['Case name']} {case_info['Medium neutral citation']}: got judgment from the Federal Court directly\")\n",
    "\n",
    "                pause.seconds(np.random.randint(5, 15))\n",
    "\n",
    "            #Make judgment_link clickable\n",
    "            clickable_link = link(case_info['Hyperlink to Federal Court Digital Law Library'])\n",
    "            case_info.update({'Hyperlink to Federal Court Digital Law Library': clickable_link})\n",
    "\n",
    "    #Create and export json file with search results\n",
    "    json_individual = json.dumps(judgments_file, indent=2)\n",
    "\n",
    "#    df_individual = pd.DataFrame(judgments_file)\n",
    "    \n",
    "    df_individual = pd.read_json(json_individual)\n",
    "    \n",
    "    #Instruct GPT\n",
    "    \n",
    "    #GPT model\n",
    "\n",
    "    if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "        gpt_model = \"gpt-4o\"\n",
    "    else:        \n",
    "        gpt_model = \"gpt-4o-mini\"\n",
    "        \n",
    "    #apply GPT_individual to each respondent's judgment spreadsheet\n",
    "    \n",
    "    GPT_activation = int(df_master.loc[0, 'Use GPT'])\n",
    "\n",
    "    questions_json = df_master.loc[0, 'questions_json']\n",
    "            \n",
    "    #Engage GPT\n",
    "    df_updated = engage_GPT_json(questions_json = questions_json, df_example = df_master.loc[0, 'Example'], df_individual = df_individual, GPT_activation = GPT_activation, gpt_model = gpt_model, system_instruction = system_instruction)\n",
    "\n",
    "    #Pop jugdment\n",
    "    if (pop_judgment() > 0) and ('judgment' in df_updated.columns):\n",
    "        df_updated.pop('judgment')\n",
    "\n",
    "    #Drop metadata if not wanted\n",
    "\n",
    "    if int(float(df_master.loc[0, 'Metadata inclusion'])) == 0:\n",
    "        for meta_label in fca_metalabels_droppable:\n",
    "            try:\n",
    "                df_updated.pop(meta_label)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return df_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90e35be4-5b48-4711-bddf-bdbbd566e93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 07:53:03.037 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "#Obtain parameters\n",
    "\n",
    "@st.cache_data(show_spinner = False, ttl=600)\n",
    "def fca_batch(df_master):\n",
    "    df_master = df_master.fillna('')\n",
    "\n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    df_master['Enter your questions for GPT'] = df_master['Enter your questions for GPT'][0: question_characters_bound].apply(split_by_line)\n",
    "    df_master['questions_json'] = df_master['Enter your questions for GPT'].apply(GPT_label_dict)\n",
    "    \n",
    "    #Conduct search    \n",
    "    search_results_soup_url = fca_search(court = df_master.loc[0, 'Courts'], \n",
    "                     case_name_mnc = df_master.loc[0, 'Case name or medium neutral citation'],\n",
    "                     judge = df_master.loc[0, 'Judge'], \n",
    "                     reported_citation = df_master.loc[0, 'Reported citation'],\n",
    "                     file_number  = df_master.loc[0, 'File number'],\n",
    "                     npa = df_master.loc[0, 'National practice area'], \n",
    "                     with_all_the_words  = df_master.loc[0, 'With all the words'], \n",
    "                     with_at_least_one_of_the_words = df_master.loc[0, 'With at least one of the words'],\n",
    "                     without_the_words = df_master.loc[0, 'Without the words'],\n",
    "                     phrase  = df_master.loc[0, 'Phrase'], \n",
    "                     proximity = df_master.loc[0, 'Proximity'], \n",
    "                     on_this_date = df_master.loc[0, 'On this date'], \n",
    "                     after_date = df_master.loc[0, 'Decision date is after'], \n",
    "                     before_date = df_master.loc[0, 'Decision date is before'], \n",
    "                     legislation = df_master.loc[0, 'Legislation'], \n",
    "                     cases_cited = df_master.loc[0, 'Cases cited'], \n",
    "                     catchwords = df_master.loc[0, 'Catchwords'] \n",
    "                    )\n",
    "    \n",
    "    search_results_soup = search_results_soup_url['soup']\n",
    "    \n",
    "    results_url = search_results_soup_url['results_url']\n",
    "    \n",
    "    judgments_counter_bound = int(df_master.loc[0, 'Maximum number of judgments'])\n",
    "\n",
    "    #Get relevant cases\n",
    "    case_infos = fca_search_results_to_judgment_links(search_results_soup, results_url, judgments_counter_bound)\n",
    "\n",
    "    #Create judgments file\n",
    "    judgments_file = []\n",
    "    \n",
    "    if huggingface == False: #If not running on HuggingFace\n",
    "        \n",
    "        for case_info in case_infos:\n",
    "            judgment_dict = fca_meta_judgment_dict(case_info)\n",
    "            case_info.update({'judgment': str(judgment_dict)})\n",
    "            \n",
    "            #Make judgment_link clickable\n",
    "            clickable_link = link(case_info['Hyperlink to Federal Court Digital Law Library'])\n",
    "            case_info.update({'Hyperlink to Federal Court Digital Law Library': clickable_link})\n",
    "            \n",
    "            judgments_file.append(case_info)\n",
    "\n",
    "            print(f\"{case_info['Case name']} {case_info['Medium neutral citation']}: got judgment from the Federal Court directly\")\n",
    "\n",
    "            pause.seconds(np.random.randint(5, 15))\n",
    "\n",
    "    else: #If running on HuggingFace\n",
    "\n",
    "        #Load oalc\n",
    "        from functions.oalc_functions import load_corpus, get_judgment_from_oalc\n",
    "\n",
    "        #Create a list of mncs for HuggingFace:\n",
    "        mnc_list = []\n",
    "\n",
    "        for case in case_infos:\n",
    "\n",
    "            #add search results to json\n",
    "            judgments_file.append(case)\n",
    "\n",
    "            #Add mnc to list for HuggingFace\n",
    "            mnc_list.append(case['Medium neutral citation'])\n",
    "\n",
    "        #Get judgments from oalc first\n",
    "        mnc_judgment_dict = get_judgment_from_oalc(mnc_list)\n",
    "            \n",
    "        #Append judgment to judgments_file \n",
    "        for case_info in judgments_file:\n",
    "            \n",
    "            #Append judgments from oalc first\n",
    "            if case_info['Medium neutral citation'] in mnc_judgment_dict.keys():\n",
    "                \n",
    "                case_info.update({'judgment': mnc_judgment_dict[case_info['Medium neutral citation']]})\n",
    "\n",
    "                print(f\"{case_info['Case name']} {case_info['Medium neutral citation']}: got judgment from OALC\")\n",
    "            \n",
    "            else: #Get judgment from FCA if can't get from oalc\n",
    "                judgment_dict_direct = fca_meta_judgment_dict(case_info)\n",
    "                case_info.update({'judgment': str(judgment_dict_direct)})\n",
    "                \n",
    "                print(f\"{case_info['Case name']} {case_info['Medium neutral citation']}: got judgment from the Federal Court directly\")\n",
    "\n",
    "                pause.seconds(np.random.randint(5, 15))\n",
    "\n",
    "            #Make judgment_link clickable\n",
    "            clickable_link = link(case_info['Hyperlink to Federal Court Digital Law Library'])\n",
    "            case_info.update({'Hyperlink to Federal Court Digital Law Library': clickable_link})\n",
    "\n",
    "    #Create and export json file with search results\n",
    "    json_individual = json.dumps(judgments_file, indent=2)\n",
    "\n",
    "#    df_individual = pd.DataFrame(judgments_file)\n",
    "    \n",
    "    df_individual = pd.read_json(json_individual)\n",
    "                        \n",
    "    #Drop metadata if not wanted\n",
    "\n",
    "    if int(float(df_master.loc[0, 'Metadata inclusion'])) == 0:\n",
    "        for meta_label in fca_metalabels_droppable:\n",
    "            if meta_label in df_individual.columns:\n",
    "                df_individual.pop(meta_label)\n",
    "    \n",
    "    #Instruct GPT\n",
    "    \n",
    "    #GPT model\n",
    "\n",
    "    if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "        gpt_model = \"gpt-4o\"\n",
    "    else:        \n",
    "        gpt_model = \"gpt-4o-mini\"\n",
    "        \n",
    "    #apply GPT_individual to each respondent's judgment spreadsheet\n",
    "    \n",
    "    GPT_activation = int(df_master.loc[0, 'Use GPT'])\n",
    "\n",
    "    questions_json = df_master.loc[0, 'questions_json']\n",
    "    \n",
    "    #Send batch input to gpt\n",
    "    batch_record_df_individual = gpt_batch_input(questions_json = questions_json, df_example = df_master.loc[0, 'Example'], df_individual = df_individual, GPT_activation = GPT_activation, gpt_model = gpt_model, system_instruction = system_instruction)\n",
    "    \n",
    "    return batch_record_df_individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d3ee80-003b-4657-a24e-96134bfe0a45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
