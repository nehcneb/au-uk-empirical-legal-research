{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d06a2290-eb1f-4bc8-bad5-3716a5c9b53b",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7509e9b7-7d67-4cb3-a833-cfbf9c9204ee",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Preliminary modules\n",
    "#import base64 \n",
    "import json\n",
    "import pandas as pd\n",
    "#import shutil\n",
    "import numpy as np\n",
    "#import re\n",
    "#import datetime\n",
    "#from datetime import date\n",
    "#from datetime import datetime\n",
    "import sys\n",
    "import pause\n",
    "import os\n",
    "import io\n",
    "from io import BytesIO\n",
    "from io import StringIO\n",
    "#from dateutil import parser\n",
    "#from dateutil.relativedelta import *\n",
    "#from datetime import timedelta\n",
    "#from PIL import Image\n",
    "#import math\n",
    "#from math import ceil\n",
    "#import matplotlib.pyplot as plt\n",
    "#import ast\n",
    "#import copy\n",
    "\n",
    "#OpenAI\n",
    "import openai\n",
    "#import tiktoken\n",
    "\n",
    "#aws\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "#Streamlit\n",
    "import streamlit as st\n",
    "#from streamlit_gsheets import GSheetsConnection\n",
    "#from streamlit.components.v1 import html\n",
    "#import streamlit_ext as ste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b2cc0ff-c707-4b9c-977e-6aa76b516dc6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 08:19:14.592 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2024-09-21 08:19:14.599 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2024-09-21 08:19:14.600 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2024-09-21 08:19:14.600 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2024-09-21 08:19:14.601 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2024-09-21 08:19:14.602 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions for GPT are capped at 2000 characters.\n",
      "\n",
      "By default, questions and answers are checked for potential privacy violation.\n"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from functions.gpt_functions import gpt_batch_input_submit, split_by_line, GPT_label_dict, is_api_key_valid, gpt_input_cost, gpt_output_cost, tokens_cap, max_output, num_tokens_from_string, judgment_prompt_json, GPT_json, engage_GPT_json, gpt_run\n",
    "\n",
    "#For checking questions and answers\n",
    "from functions.common_functions import check_questions_answers\n",
    "\n",
    "from functions.gpt_functions import questions_check_system_instruction, GPT_questions_check, checked_questions_json, answers_check_system_instruction, GPT_answers_check\n",
    "\n",
    "if check_questions_answers() > 0:\n",
    "    print(f'By default, questions and answers are checked for potential privacy violation.')\n",
    "else:\n",
    "    print(f'By default, questions and answers are NOT checked for potential privacy violation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a719995-5364-4424-b3a4-0dd91e966cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 08:19:14.624 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "#Title of webpage\n",
    "st.set_page_config(\n",
    "   page_title=\"LawtoData: An Empirical Legal Research Kickstarter\",\n",
    "   page_icon=\"ðŸ§Š\",\n",
    "   layout=\"centered\",\n",
    "   initial_sidebar_state=\"collapsed\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb9d1f7-7606-43ef-b7fe-c87b38ce4c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 08:19:14.628 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-21 08:19:14.628 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n",
      "2024-09-21 08:19:14.629 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-21 08:19:14.629 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "#Initialise\n",
    "if 'page_from' not in st.session_state:\n",
    "    st.session_state['page_from'] = 'pages/BATCH_GET.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63652ebe-231d-4f89-8175-4d07a8cd87b3",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 08:19:14.632 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-21 08:19:14.749 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-21 08:19:14.750 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-21 08:19:14.750 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-21 08:19:14.750 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-21 08:19:14.750 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.title(\":blue[LawtoData]\")\n",
    "\n",
    "st.subheader(\"An Empirical Legal Research Kickstarter\")\n",
    "\n",
    "st.markdown(\"\"\"*LawtoData* is an [open-source](https://github.com/nehcneb/au-uk-empirical-legal-research) web app designed to help kickstart empirical projects involving judgments. It automates the most costly and time-consuming aspects of empirical research.\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "006a6a38-8bf2-4fe1-a30e-ea846be64bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Ben/Library/CloudStorage/Dropbox/Python/GitHub/au-uk-empirical-legal-research-unlimited\n"
     ]
    }
   ],
   "source": [
    "#Generate current directory, just to check whether running on Github Actions or locally\n",
    "current_dir = ''\n",
    "try:\n",
    "    current_dir = os.getcwd()\n",
    "    print(current_dir)\n",
    "except Exception as e:\n",
    "    print(f\"current_dir not generated.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1833a5bf-0deb-46e0-ad5c-f0af1e63d91c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AWS_SECRET_ACCESS_KEY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     SENDER \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39msecrets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memail_notifications\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memail_sender\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     19\u001b[0m     RECIPIENT \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39msecrets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memail_notifications\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memail_receiver_work\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 21\u001b[0m s3_resource \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mresource(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m'\u001b[39m,region_name\u001b[38;5;241m=\u001b[39mAWS_DEFAULT_REGION, aws_access_key_id\u001b[38;5;241m=\u001b[39mAWS_ACCESS_KEY_ID, aws_secret_access_key\u001b[38;5;241m=\u001b[39m\u001b[43mAWS_SECRET_ACCESS_KEY\u001b[49m)\n\u001b[1;32m     22\u001b[0m ses \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mses\u001b[39m\u001b[38;5;124m'\u001b[39m,region_name\u001b[38;5;241m=\u001b[39mAWS_DEFAULT_REGION, aws_access_key_id\u001b[38;5;241m=\u001b[39mAWS_ACCESS_KEY_ID, aws_secret_access_key\u001b[38;5;241m=\u001b[39mAWS_SECRET_ACCESS_KEY)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#ses is based on the following upon substitutiong 'ses' for 's3', https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html#guide-credentials\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AWS_SECRET_ACCESS_KEY' is not defined"
     ]
    }
   ],
   "source": [
    "#Initiate aws s3 and ses\n",
    "\n",
    "#If using Github Actions\n",
    "if 'Users/Ben' not in current_dir:\n",
    "    AWS_DEFAULT_REGION = os.environ['AWS_DEFAULT_REGION']\n",
    "    AWS_ACCESS_KEY_ID = os.environ['AWS_ACCESS_KEY_ID']\n",
    "    AWS_SECRET_ACCESS_KEY = os.environ['AWS_SECRET_ACCESS_KEY']\n",
    "    \n",
    "    SENDER = os.environ['EMAIL_SENDER']\n",
    "    RECIPIENT = os.environ['EMAIL_RECEIVER_WORK']\n",
    "\n",
    "else:#If using on streamlit\n",
    "\n",
    "    AWS_DEFAULT_REGION=st.secrets[\"aws\"][\"AWS_DEFAULT_REGION\"]\n",
    "    AWS_ACCESS_KEY_ID=st.secrets[\"aws\"][\"AWS_ACCESS_KEY_ID\"]\n",
    "    AWS_SECRET_ACCESS_KEY=st.secrets[\"aws\"][\"AWS_SECRET_ACCESS_KEY\"]\n",
    "    \n",
    "    SENDER = st.secrets[\"email_notifications\"][\"email_sender\"]\n",
    "    RECIPIENT = st.secrets[\"email_notifications\"][\"email_receiver_work\"]\n",
    "\n",
    "s3_resource = boto3.resource('s3',region_name=AWS_DEFAULT_REGION, aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "ses = boto3.client('ses',region_name=AWS_DEFAULT_REGION, aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "#ses is based on the following upon substitutiong 'ses' for 's3', https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html#guide-credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91c9b9a-8233-4a50-8fed-b19b12dac0ca",
   "metadata": {},
   "source": [
    "# Get all_df_masters and all df_individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6426fd-9e96-4d7c-a691-281a3538d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.subheader(\"Load records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3faaa8-2efd-4de3-ab83-21f27686cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a list of all files on s3\n",
    "bucket = s3_resource.Bucket('lawtodata')\n",
    "\n",
    "aws_objects = []\n",
    "\n",
    "for obj in bucket.objects.all():\n",
    "    key = obj.key\n",
    "    body = obj.get()['Body'].read()\n",
    "    key_body = {'key': key, 'body': body}\n",
    "    aws_objects.append(key_body)\n",
    "\n",
    "#Get all_df_masters\n",
    "\n",
    "for key_body in aws_objects:\n",
    "    if key_body['key'] == 'all_df_masters.csv':\n",
    "        all_df_masters_current = pd.read_csv(BytesIO(key_body['body']), index_col=0)\n",
    "        st.success(f\"Succesfully loaded {key_body['key']}.\")\n",
    "        break\n",
    "\n",
    "#Work on new copy of all_df_masters\n",
    "\n",
    "all_df_masters = all_df_masters_current.copy()\n",
    "        \n",
    "#Alternative download file example\n",
    "#NOT IN USE\n",
    "#s3 = boto3.client('s3',region_name=st.secrets[\"aws\"][\"AWS_DEFAULT_REGION\"], aws_access_key_id=st.secrets[\"aws\"][\"AWS_ACCESS_KEY_ID\"], aws_secret_access_key=st.secrets[\"aws\"][\"AWS_SECRET_ACCESS_KEY\"])\n",
    "#s3.download_file('BUCKET_NAME', 'OBJECT_NAME', 'FILE_NAME')\n",
    "#see https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-example-download-file.html\n",
    "#'OBJECT_NAME' = 'FILE_NAME'\n",
    "#eg s3.download_file('lawtodata', 'myfile.csv', 'myfile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bc4ecf-80a7-4faf-b0dd-28d4171931c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain google spreadsheet for all df_masters      \n",
    "#conn_all_df_masters = st.connection(\"gsheets_all_df_masters\", type=GSheetsConnection, ttl=0)\n",
    "#all_df_masters = conn_all_df_masters.read()\n",
    "#all_df_masters = all_df_masters.fillna('')\n",
    "#all_df_masters = all_df_masters[all_df_masters[\"submission_time\"]!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c8c51-d4b4-4d71-8458-277ce25d9af8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Tidy up all_df_masters\n",
    "\n",
    "boolean_columns = [\"Metadata inclusion\", 'Use GPT', 'Use own account', 'Use flagship version of GPT']\n",
    "\n",
    "for column in boolean_columns:\n",
    "    if column in all_df_masters.columns:\n",
    "        all_df_masters[column] = all_df_masters[column].replace({'True': 1, 'False':0, 'TRUE': 1, 'FALSE': 0})\n",
    "\n",
    "#all_df_masters.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fc3b5c-ca95-402a-bb72-fe7a18bf8393",
   "metadata": {},
   "source": [
    "# Submit updated all_df_masters to GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20e3721-7500-4c78-a1e5-b0a5b7e66bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.subheader(\"Scrape judgments and submit batches to GPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb5e200-f494-4ca9-b92d-bbae55a0008b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#requests counter\n",
    "\n",
    "batch_request_total = 0\n",
    "\n",
    "for index in all_df_masters.index:\n",
    "\n",
    "    current_status = str(all_df_masters.loc[index, 'status'])\n",
    "\n",
    "    if current_status == 'to_process':\n",
    "        batch_request_total += 1\n",
    "\n",
    "if batch_request_total == 0:\n",
    "    st.warning('No requests need to be submitted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1d48ea-b76e-4654-a2f8-be0ff4fe702d",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generate batch input, submit to GPT and keep record online\n",
    "\n",
    "#all_df_masters = all_df_masters[all_df_masters[\"status\"]=='to_process']\n",
    "\n",
    "batch_request_counter = 0\n",
    "\n",
    "gpt_batch_input_list = []\n",
    "\n",
    "for index in all_df_masters.index:\n",
    "\n",
    "    current_status = str(all_df_masters.loc[index, 'status'])\n",
    "\n",
    "    if current_status == 'to_process':\n",
    "    \n",
    "        api_key = all_df_masters.loc[index, 'Your GPT API key']\n",
    "    \n",
    "        openai.api_key = api_key\n",
    "        \n",
    "        df_dict = all_df_masters.loc[index].to_dict()\n",
    "    \n",
    "        df_master = pd.DataFrame.from_dict([df_dict], orient='columns')\n",
    "    \n",
    "        jurisdiction_page = df_master.loc[0, 'jurisdiction_page']\n",
    "    \n",
    "        gpt_batch_input = gpt_batch_input_submit(jurisdiction_page, df_master)\n",
    "\n",
    "        gpt_batch_input_list.append(gpt_batch_input)\n",
    "        \n",
    "        #Get batch record\n",
    "        batch_record = gpt_batch_input['batch_record']\n",
    "        \n",
    "        batch_dict = batch_record.to_dict()\n",
    "        batch_id = batch_dict['id']\n",
    "        input_file_id = batch_dict['input_file_id']\n",
    "        status = batch_dict['status']\n",
    "\n",
    "        #Update df_masters\n",
    "        all_df_masters.loc[index, 'batch_id'] = batch_id\n",
    "        all_df_masters.loc[index, 'input_file_id'] = input_file_id\n",
    "        all_df_masters.loc[index, 'status'] = status\n",
    "\n",
    "        #Update counter\n",
    "        batch_request_counter += 1\n",
    "\n",
    "        st.success(f'{batch_id} submitted to GPT. Done {batch_request_counter}/{batch_request_total}.')\n",
    "\n",
    "    #Keep batching record on AWS\n",
    "    #Upload all_df_masters to aws\n",
    "    #csv_buffer = StringIO()\n",
    "    #all_df_masters.to_csv(csv_buffer)\n",
    "    #s3_resource.Object('lawtodata', 'all_df_masters.csv').put(Body=csv_buffer.getvalue())\n",
    "\n",
    "    #Keep batch record on google sheet\n",
    "    #conn_all_df_masters.update(worksheet=\"Sheet1\", data=all_df_masters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b5cad4-c475-4693-9c50-0f7fbd5defbb",
   "metadata": {},
   "source": [
    "# Upload each submitted df_individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a7b565-50f3-4ac5-a2ad-ca195cc59c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.subheader(\"Save scaped judgment data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549127c0-cee4-468d-8005-2d6f153d97bb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if len(gpt_batch_input_list) == 0:\n",
    "    \n",
    "    st.warning('No judgment data have been scraped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f418c-63ed-420d-903b-cdcd28f9c48b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Obtain all_df_individuals from google sheets\n",
    "#conn_all_df_individuals = st.connection(\"gsheets_record_all_df_individuals\", type=GSheetsConnection, ttl=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55bd62b-13e7-49a0-941a-7b8fa23cdd7f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Obtain all_df_individuals from aws\n",
    "#Based on https://stackoverflow.com/questions/38154040/save-dataframe-to-csv-directly-to-s3-python\n",
    "\n",
    "save_counter = 0\n",
    "\n",
    "for gpt_batch_input in gpt_batch_input_list:\n",
    "\n",
    "    #Get batch record\n",
    "    batch_record = gpt_batch_input['batch_record']\n",
    "    \n",
    "    batch_dict = batch_record.to_dict()\n",
    "    batch_id = batch_dict['id']\n",
    "    #Sheet will be named by batch id\n",
    "    \n",
    "    df_individual = gpt_batch_input['df_individual']\n",
    "\n",
    "    #Upload df_individual onto AWS\n",
    "    csv_buffer = StringIO()\n",
    "    df_individual.to_csv(csv_buffer)\n",
    "    s3_resource.Object('lawtodata', f'{batch_id}.csv').put(Body=csv_buffer.getvalue())\n",
    "\n",
    "    save_counter += 1\n",
    "\n",
    "    #Keep all_df_individuals on google sheets\n",
    "    #conn_all_df_individuals.create(worksheet=batch_id, data=df_individual)\n",
    "    \n",
    "    st.success(f\"{batch_id} saved online. Done {save_counter}/{len(gpt_batch_input_list)}.\")\n",
    "\n",
    "#Alternative Uploading file example\n",
    "#NOT IN USE\n",
    "#s3 = boto3.client('s3')\n",
    "#with open(\"FILE_NAME\", \"rb\") as f:\n",
    "    #s3.upload_fileobj(f, \"BUCKET_NAME\", \"OBJECT_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d0da9-065b-459b-8a31-cc1a5607d666",
   "metadata": {},
   "source": [
    "# Retrive output from GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27335b6a-0143-4e7e-bbce-cd114e89f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.subheader(\"Retrive GPT output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95887ded-2038-4afc-84b3-daa3dcf4c72c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Get max number of batches to retrieve\n",
    "\n",
    "max_retrieve_counter = 0\n",
    "\n",
    "for index in all_df_masters.index:\n",
    "\n",
    "    current_status = all_df_masters.loc[index, \"status\"]\n",
    "\n",
    "    if current_status == 'validating':\n",
    "\n",
    "        max_retrieve_counter += 1\n",
    "\n",
    "if max_retrieve_counter == 0:\n",
    "    st.warning('No batches are pending retrival.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db76431-33cd-4562-9eb0-6d79740ff64d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df_batch_id_response_list = []\n",
    "\n",
    "retrieve_counter = 0\n",
    "\n",
    "for index in all_df_masters.index:\n",
    "\n",
    "    current_status = all_df_masters.loc[index, \"status\"]\n",
    "\n",
    "    if current_status == 'validating':\n",
    "    \n",
    "        api_key = all_df_masters.loc[index, 'Your GPT API key']\n",
    "        \n",
    "        openai.api_key = api_key\n",
    "    \n",
    "        batch_id = all_df_masters.loc[index, 'batch_id']\n",
    "    \n",
    "        gpt_model = \"gpt-4o-mini\"\n",
    "        if all_df_masters.loc[index, 'Use flagship version of GPT'] == True:\n",
    "            gpt_model = \"gpt-4o-2024-08-06\"\n",
    "        else:        \n",
    "            gpt_model = \"gpt-4o-mini\"\n",
    "            \n",
    "        #Get batch record\n",
    "        batch_record = openai.batches.retrieve(batch_id)\n",
    "    \n",
    "        output_file_id = ''\n",
    "    \n",
    "        try:\n",
    "    \n",
    "            output_file_id = batch_record.output_file_id\n",
    "    \n",
    "        except:\n",
    "    \n",
    "            st.warning(f\"{batch_id}: output_file_id not yet available.\")\n",
    "            \n",
    "        status = batch_record.status\n",
    "\n",
    "        #Print any status change\n",
    "        st.info(f\"{batch_id}: status == {status}.\")\n",
    "        print(f\"{batch_id}: status == {status}.\")\n",
    "        \n",
    "        if status == 'completed':\n",
    "            \n",
    "            batch_response = openai.files.content(output_file_id)\n",
    "    \n",
    "            df_batch_response = pd.read_json(batch_response.text, lines=True)\n",
    "    \n",
    "            batch_id_response = {'batch_id': batch_id, 'df_batch_response': df_batch_response, 'gpt_model': gpt_model}\n",
    "    \n",
    "            #Apppend gpt batch id and responses to list for adding to df_individual later\n",
    "            \n",
    "            df_batch_id_response_list.append(batch_id_response)\n",
    "\n",
    "            #Update status etc and remove api key on all_df_masters\n",
    "            all_df_masters.loc[index, 'status'] = status\n",
    "            all_df_masters.loc[index, 'output_file_id'] = output_file_id\n",
    "            all_df_masters.loc[index, 'Your GPT API key'] = ''\n",
    "    \n",
    "            #Update all_df_masters on AWS\n",
    "            #csv_buffer = StringIO()\n",
    "            #all_df_masters.to_csv(csv_buffer)\n",
    "            #s3_resource.Object('lawtodata', 'all_df_masters.csv').put(Body=csv_buffer.getvalue())\n",
    "\n",
    "            #Update counter \n",
    "            retrieve_counter += 1\n",
    "    \n",
    "            #Update google sheet for all_df_masters\n",
    "            #conn_all_df_masters.update(worksheet=\"Sheet1\", data=all_df_masters)\n",
    "\n",
    "            st.success(f\"{batch_id}: status == {status}. Done {retrieve_counter}/{max_retrieve_counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b0100b-7d0e-4db2-a929-4338e712e893",
   "metadata": {},
   "source": [
    "# Append retrieved output to df_individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76167500-06af-4fa5-b2b0-0bc72424ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.subheader(\"Append GPT output to judgment data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dea4b9-f5f8-44cf-b5fb-93c73b04097b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if len(df_batch_id_response_list) == 0:\n",
    "    \n",
    "    st.warning('No GPT output needs to be appended.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf2349-5a32-4edf-842f-68ab04822b2b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Get a list of all files on s3\n",
    "bucket = s3_resource.Bucket('lawtodata')\n",
    "\n",
    "aws_objects = []\n",
    "\n",
    "for obj in bucket.objects.all():\n",
    "    key = obj.key\n",
    "    body = obj.get()['Body'].read()\n",
    "    key_body = {'key': key, 'body': body}\n",
    "    aws_objects.append(key_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c15f4-862c-4be0-a770-943a199eb658",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Attach add gpt response to df_individual\n",
    "\n",
    "append_counter = 0\n",
    "\n",
    "for df_batch_response in df_batch_id_response_list:\n",
    "\n",
    "    batch_id = df_batch_response['batch_id']\n",
    "\n",
    "    #Get df_individual from aws\n",
    "    for key_body in aws_objects:\n",
    "        if key_body['key'] == f'{batch_id}.csv':\n",
    "            df_individual = pd.read_csv(BytesIO(key_body['body']), index_col=0)\n",
    "            st.success(f\"Succesfully loaded {key_body['key']} as df_individual.\")\n",
    "            break\n",
    "    \n",
    "    #Get df_individual from google sheets\n",
    "    #conn_all_df_individuals = st.connection(\"gsheets_record_all_df_individuals\", type=GSheetsConnection, ttl=0)\n",
    "    #df_individual = conn_all_df_individuals.read(worksheet = batch_id)\n",
    "\n",
    "    #Append gpt output to individual\n",
    "    df_batch_response = df_batch_response['df_batch_response']\n",
    "\n",
    "    for gpt_index in df_batch_response.index:\n",
    "\n",
    "        #Get custom id of GPT case-specific response\n",
    "        \n",
    "        custom_id = df_batch_response.loc[gpt_index, 'custom_id']\n",
    "\n",
    "        #Link GPT case-specific response to row in df_individual\n",
    "        \n",
    "        judgment_index_list = df_individual.index[df_individual['custom_id']==custom_id].tolist()\n",
    "\n",
    "        if len(judgment_index_list) > 0:\n",
    "            \n",
    "            judgment_index = judgment_index_list[0]\n",
    "\n",
    "            #Get gpt specific answers\n",
    "            \n",
    "            answers_dict = json.loads(df_batch_response.loc[gpt_index, 'response']['body']['choices'][0]['message']['content'])\n",
    "\n",
    "            input_tokens = df_batch_response.loc[gpt_index, 'response']['body']['usage']['prompt_tokens']\n",
    "\n",
    "            output_tokens = df_batch_response.loc[gpt_index, 'response']['body']['usage']['completion_tokens']\n",
    "\n",
    "            #Check GPT answers\n",
    "            if check_questions_answers() > 0:\n",
    "                \n",
    "                try:\n",
    "\n",
    "                    #Get checked answers and tokens \n",
    "                    redacted_output = GPT_answers_check(answers_dict, gpt_model, answers_check_system_instruction)\n",
    "            \n",
    "                    redacted_answers_dict = redacted_output[0]\n",
    "            \n",
    "                    redacted_answers_output_tokens = redacted_output[1]\n",
    "            \n",
    "                    redacted_answers_prompt_tokens = redacted_output[2]\n",
    "\n",
    "                    #Update to reflect checked answers\n",
    "\n",
    "                    answers_dict = redacted_answers_dict\n",
    "                    \n",
    "                    input_tokens += redacted_answers_prompt_tokens\n",
    "\n",
    "                    output_tokens += redacted_answers_output_tokens\n",
    "                        \n",
    "                except Exception as e:\n",
    "        \n",
    "                    print('Answers check failed.')\n",
    "        \n",
    "                    print(e)\n",
    "\n",
    "            #Attach GPT answers to df_individual\n",
    "            #for gpt_question in answers_dict.keys():\n",
    "\n",
    "                #heading = 'GPT question: ' + gpt_question\n",
    "                \n",
    "                #answer = answers_dict[gpt_question]\n",
    "\n",
    "                #df_individual.loc[judgment_index, heading] = answer\n",
    "\n",
    "            #df_individual.loc[judgment_index, 'GPT cost estimate (USD excl GST)'] = input_tokens*gpt_input_cost(gpt_model)/2 + output_tokens*gpt_output_cost(gpt_model)/2\n",
    "\n",
    "            #Add costs column\n",
    "            df_individual.loc[judgment_index, 'GPT cost estimate (USD excl GST)'] = input_tokens*gpt_input_cost(gpt_model)/2 + output_tokens*gpt_output_cost(gpt_model)/2\n",
    "\n",
    "        \t#Create GPT question headings, append answers to individual spreadsheets, and remove template answers\n",
    "\n",
    "            #answers_list = [answers_dict]\n",
    "    \n",
    "            #if isinstance(answers_dict, list):\n",
    "                #answers_list = answers_dict\n",
    "            \n",
    "            #for answers_dict in answers_list:\n",
    "            \n",
    "            for answer_index in answers_dict.keys():\n",
    "    \n",
    "                #Check any question override\n",
    "                if 'Say \"n/a\" only' in str(answer_index):\n",
    "                    answer_header = 'GPT question: ' + 'Not answered due to potential privacy violation'\n",
    "                else:\n",
    "                    answer_header = 'GPT question: ' + answer_index\n",
    "    \n",
    "                try:\n",
    "                \n",
    "                    df_individual.loc[judgment_index, answer_header] = answers_dict[answer_index]\n",
    "    \n",
    "                except:\n",
    "    \n",
    "                    df_individual.loc[judgment_index, answer_header] = str(answers_dict[answer_index])                \n",
    "    \n",
    "        #Remove judgment column\n",
    "        \n",
    "        if 'judgment' in df_individual.columns:\n",
    "            df_individual.pop('judgment')\n",
    "\n",
    "        #Update df_individual on AWS\n",
    "        csv_buffer = StringIO()\n",
    "        df_individual.to_csv(csv_buffer)\n",
    "        s3_resource.Object('lawtodata', f'{batch_id}.csv').put(Body=csv_buffer.getvalue())\n",
    "        \n",
    "        #Update df_individual on google sheet\n",
    "        #conn_all_df_individuals.update(worksheet=batch_id, data=df_individual)                \n",
    "\n",
    "    append_counter += 1\n",
    "    \n",
    "    st.success(f\"{batch_id} GPT output appended to df_individual and saved online. Done {append_counter}/{len(df_batch_id_response_list)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d59135-0578-40a2-ae1c-fd22bda22c78",
   "metadata": {},
   "source": [
    "# Sending emails via AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eab4dc-407f-41ee-988d-97056ee11b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.subheader(\"Send notification emails\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67ae3f7-7c2e-492f-ac21-3c8dc42c6b9e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Define send email function\n",
    "\n",
    "def send_email(ULTIMATE_RECIPIENT_NAME, ULTIMATE_RECIPIENT_EMAIL, ACCESS_LINK, BATCH_CODE):\n",
    "    #Based on the following upon substituting various arguments, https://docs.aws.amazon.com/ses/latest/dg/send-an-email-using-sdk-programmatically.html\n",
    "    \n",
    "    # Replace sender@example.com with your \"From\" address.\n",
    "    # This address must be verified with Amazon SES.\n",
    "    #SENDER = \"name <email>\"\n",
    "\n",
    "    # The subject line for the email.\n",
    "    SUBJECT = f\"{ULTIMATE_RECIPIENT_EMAIL}\"\n",
    "    \n",
    "    # The email body for recipients with non-HTML email clients.\n",
    "\n",
    "    #BODY_TEXT is not in used\n",
    "    BODY_TEXT = (\n",
    "    \n",
    "    f\"Dear {ULTIMATE_RECIPIENT_NAME}\\r\\n\\r\\n\"\n",
    "    \n",
    "    \"Thank you for using LawtoData. You can now view and download your requested data. To do so, please click on the following link:\\r\\n\"\n",
    "    f\"{ACCESS_LINK}\\r\\n\\r\\n\"\n",
    "    \n",
    "    f\"Your access code is {BATCH_CODE}\\r\\n\\r\\n\"\n",
    "    \n",
    "    \"Lawtodata is partially funded by a University of Sydney Research Accelerator (SOAR) Prize. Please kindly acknowledge this if you use your requested data to produce any research output. \\r\\n\\r\\n\"\n",
    "    \n",
    "    #There is no need to acknowledge me. \n",
    "    \n",
    "    \"Kind regards\\r\\n\\r\\n\"\n",
    "    \n",
    "    \"Ben\\r\\n\\r\\n\"\n",
    "    \n",
    "    \"Ben Chen | Senior Research Fellow and Senior Lecturer\\r\\n\"\n",
    "    \"The University of Sydney Law School\\r\\n\"\n",
    "    \" \\r\\n\"\n",
    "    \"Email: ben.chen@sydney.edu.au | Phone: + 61 2 8627 6887 (by appointment)\\r\\n\"\n",
    "    \"Webpage: https://www.sydney.edu.au/law/about/our-people/academic-staff/ben-chen.html\\r\\n\"\n",
    "    \"Address: Room 431, New Law Building (F10), Eastern Ave, The University of Sydney, NSW 2006\\r\\n\"\n",
    "    )\n",
    "\n",
    "    #<h1>LawtoData: an Empirical Legal Research Kickstarter</h1>\n",
    "\n",
    "    \n",
    "    # The HTML body of the email.\n",
    "    BODY_HTML = f\"\"\"<html>\n",
    "    <head></head>\n",
    "    <body>\n",
    "    <p>\n",
    "    Dear {ULTIMATE_RECIPIENT_NAME}\n",
    "    </p>\n",
    "    <p>\n",
    "    Thank you for using <em>LawtoData</em>. You can now view and download your requested data. To do so, please click on the following link:\n",
    "    </p>\n",
    "    <p>\n",
    "    {ACCESS_LINK}\n",
    "    </p>\n",
    "    <p>\n",
    "    Your access code is {BATCH_CODE}\n",
    "    </p>     \n",
    "    <p>\n",
    "    <em>LawtoData</em> is partially funded by a University of Sydney Research Accelerator (SOAR) Prize. Please kindly acknowledge this if you use your requested data to produce any research output.\n",
    "    </p>    \n",
    "    <p>\n",
    "    Please don't hesitate to reach out if I could be of assistance.\n",
    "    </p> \n",
    "    <p>\n",
    "    Kind regards\n",
    "    </p> \n",
    "    <p>\n",
    "    Ben\n",
    "    </p>   \n",
    "    <p>\n",
    "    <b>Ben Chen</b> | Senior Research Fellow and Senior Lecturer\n",
    "    <p>\n",
    "    The University of Sydney Law School\n",
    "    </p>\n",
    "    <p>\n",
    "    Email: ben.chen@sydney.edu.au | Phone: + 61 2 8627 6887 (by appointment)\n",
    "    </p>\n",
    "    <p>\n",
    "    Webpage: https://www.sydney.edu.au/law/about/our-people/academic-staff/ben-chen.html\n",
    "    </p>\n",
    "    <p>\n",
    "    Address: Room 431, New Law Building (F10), Eastern Ave, The University of Sydney, NSW 2006\n",
    "    </p> \n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"              \n",
    "    \n",
    "    # The character encoding for the email.\n",
    "    CHARSET = \"UTF-8\"\n",
    "    \n",
    "    # Create a new SES resource and specify a region.\n",
    "    #client = boto3.client('ses',region_name=AWS_REGION)\n",
    "    \n",
    "    # Try to send the email.\n",
    "    try:\n",
    "        #Provide the contents of the email.\n",
    "        response = ses.send_email(\n",
    "            Destination={\n",
    "                'ToAddresses': [\n",
    "                    RECIPIENT,\n",
    "                ],\n",
    "                #'CcAddresses': [\n",
    "                    #CC_RECIPIENT,\n",
    "                #]\n",
    "            },\n",
    "            Message={\n",
    "                'Body': {\n",
    "                    'Html': {\n",
    "                        'Charset': CHARSET,\n",
    "                        'Data': BODY_HTML,\n",
    "                    },\n",
    "                    #'Text': {\n",
    "                        #'Charset': CHARSET,\n",
    "                        #'Data': BODY_TEXT,\n",
    "                    #},\n",
    "                },\n",
    "                'Subject': {\n",
    "                    'Charset': CHARSET,\n",
    "                    'Data': SUBJECT,\n",
    "                },\n",
    "            },\n",
    "            Source=SENDER,\n",
    "            # If you are not using a configuration set, comment or delete the\n",
    "            # following line\n",
    "            #ConfigurationSetName=CONFIGURATION_SET,\n",
    "        )\n",
    "    # Display an error if something goes wrong.\t\n",
    "    \n",
    "    except ClientError as e:\n",
    "        st.error(e.response['Error']['Message'])\n",
    "        print(e.response['Error']['Message'])\n",
    "        \n",
    "    else:\n",
    "        st.success(f\"Email sent! Message ID: {response['MessageId']}.\")        \n",
    "        print(f\"Email sent! Message ID: {response['MessageId']}.\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b78858-0342-4787-8bb2-153ed344ca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get number of notification emails to send\n",
    "all_df_masters.fillna('')\n",
    "\n",
    "emails_counter_total = 0\n",
    "\n",
    "for index in all_df_masters.index:\n",
    "    \n",
    "    sent_to_user = all_df_masters.loc[index, 'sent_to_user']\n",
    "\n",
    "    status = all_df_masters.loc[index, 'status']\n",
    "\n",
    "    if ((status == 'completed') and (sent_to_user not in [True, 1, 'yes', 'Yes', '1'])):\n",
    "        emails_counter_total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94494630-e0f2-43dc-b0c4-f4afe40d1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send emails\n",
    "#all_df_masters.fillna('')\n",
    "\n",
    "email_sent_counter = 0\n",
    "\n",
    "for index in all_df_masters.index:\n",
    "    \n",
    "    sent_to_user = all_df_masters.loc[index, 'sent_to_user']\n",
    "\n",
    "    status = all_df_masters.loc[index, 'status']\n",
    "\n",
    "    if ((status == 'completed') and (sent_to_user not in [True, 1, 'yes', 'Yes', '1'])):\n",
    "        \n",
    "        batch_id = str(all_df_masters.loc[index, 'batch_id'])\n",
    "        name = str(all_df_masters.loc[index, 'Your name'])\n",
    "        email = str(all_df_masters.loc[index, 'Your email address'])\n",
    "\n",
    "        link = 'https://lawtodata.streamlit.app/BATCH'\n",
    "\n",
    "        try:\n",
    "            send_email(ULTIMATE_RECIPIENT_NAME = name, \n",
    "                       ULTIMATE_RECIPIENT_EMAIL = email, \n",
    "                       ACCESS_LINK = link , \n",
    "                       BATCH_CODE = batch_id\n",
    "                      )\n",
    "\n",
    "            all_df_masters.loc[index, 'sent_to_user'] = 1\n",
    "\n",
    "            email_sent_counter += 1\n",
    "            \n",
    "            st.success(f'{batch_id} for user {name} at {email} successfully emailed. Done {email_sent_counter}/{emails_counter_total}.')\n",
    "            print(f'{batch_id} for user {name} at {email} successfully emailed. Done {email_sent_counter}/{emails_counter_total}.')\n",
    "\n",
    "        except Exception as e:\n",
    "            st.error(f\"{batch_id} not emailed to user {name} at {email}.\")\n",
    "            print(f\"{batch_id} not emailed to user {name} at {email}.\")\n",
    "\n",
    "            st.error(f\"{e}\")\n",
    "            print(f\"{e}\")\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df855f-3dc0-4cd1-a072-525de85a7cac",
   "metadata": {},
   "source": [
    "# Finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f21ae0-bfef-4ae0-8357-e57c8ace9a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.subheader(\"Finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa7850-b0d7-4455-9a07-64df7bcb35f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload all_df_masters to aws if needed\n",
    "\n",
    "all_df_masters_needs_update = False\n",
    "\n",
    "for index in all_df_masters.index:\n",
    "    if all_df_masters.loc[index, 'status'] != all_df_masters_current.loc[index, 'status']:\n",
    "        all_df_masters_needs_update = True\n",
    "        break\n",
    "\n",
    "if all_df_masters_needs_update == True:\n",
    "    \n",
    "    csv_buffer = StringIO()\n",
    "    all_df_masters.to_csv(csv_buffer)\n",
    "    s3_resource.Object('lawtodata', 'all_df_masters.csv').put(Body=csv_buffer.getvalue())\n",
    "\n",
    "    st.success(f\"Updated all_df_masters.csv online.\" )\n",
    "    print(f\"Updated all_df_masters.csv online.\" )\n",
    "\n",
    "else:\n",
    "    st.warning(f\"No need to update all_df_masters.csv online.\" )\n",
    "    print(f\"No need to update all_df_masters.csv online.\" )\n",
    "\n",
    "#Update google sheet for all_df_masters\n",
    "#conn_all_df_masters.update(worksheet=\"Sheet1\", data=all_df_masters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
