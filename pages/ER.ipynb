{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b8b879",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2312235",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Preliminary modules\n",
    "import base64 \n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date\n",
    "from dateutil import parser\n",
    "from dateutil.relativedelta import *\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import pause\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import httplib2\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import urllib.request\n",
    "import PyPDF2\n",
    "import io\n",
    "from io import BytesIO\n",
    "\n",
    "#Streamlit\n",
    "import streamlit as st\n",
    "from streamlit_gsheets import GSheetsConnection\n",
    "from streamlit.components.v1 import html\n",
    "import streamlit_ext as ste\n",
    "from streamlit_extras.stylable_container import stylable_container\n",
    "\n",
    "#OpenAI\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "#Google\n",
    "#from google.oauth2 import service_account\n",
    "\n",
    "#Excel\n",
    "from pyxlsb import open_workbook as open_xlsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36b77045-bdcc-4ee1-a9a4-b7dd03b6bfd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'common_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Import functions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcommon_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m own_account_allowed, convert_df_to_json, convert_df_to_csv, convert_df_to_excel, mnc_cleaner \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Import variables\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcommon_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m today_in_nums, errors_list, scraper_pause_mean, judgment_text_lower_bound\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'common_functions'"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from common_functions import own_account_allowed, convert_df_to_json, convert_df_to_csv, convert_df_to_excel, mnc_cleaner, save_input\n",
    "#Import variables\n",
    "from common_functions import today_in_nums, errors_list, scraper_pause_mean, judgment_text_lower_bound, default_judgment_counter_bound\n",
    "\n",
    "if own_account_allowed() > 0:\n",
    "    print(f'By default, users are allowed to use their own account')\n",
    "else:\n",
    "    print(f'By default, users are NOT allowed to use their own account')\n",
    "\n",
    "print(f\"The pause between judgment scraping is {scraper_pause_mean} second.\\n\")\n",
    "\n",
    "print(f\"The lower bound on lenth of judgment text to process is {judgment_text_lower_bound} tokens.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e819e27",
   "metadata": {},
   "source": [
    "# English Reports search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "151727b0-e03c-4ab1-be18-4fd113d064af",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'common_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcommon_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m link\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'common_functions'"
     ]
    }
   ],
   "source": [
    "from common_functions import link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f841231-e0ad-4647-b5c2-244594a4cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create dataframe\n",
    "def er_create_df():\n",
    "\n",
    "    #submission time\n",
    "    timestamp = datetime.now()\n",
    "\n",
    "    #Personal info entries\n",
    "\n",
    "    name = ''\n",
    "    \n",
    "    email = ''\n",
    "\n",
    "    gpt_api_key = ''\n",
    "\n",
    "    try:\n",
    "        name = name_entry\n",
    "    except:\n",
    "        print('Name not entered')\n",
    "    \n",
    "    try:\n",
    "        email = email_entry\n",
    "    except:\n",
    "        print('Email not entered')\n",
    "\n",
    "    try:\n",
    "        gpt_api_key = gpt_api_key_entry\n",
    "    except:\n",
    "        print('API key not entered')\n",
    "\n",
    "    #Own account status\n",
    "    own_account = st.session_state.own_account\n",
    "    \n",
    "    #Judgment counter bound\n",
    "    judgments_counter_bound = st.session_state.judgments_counter_bound\n",
    "\n",
    "    #GPT enhancement\n",
    "    gpt_enhancement = st.session_state.gpt_enhancement_entry\n",
    "\n",
    "    #GPT choice and entry\n",
    "    try:\n",
    "        gpt_activation_status = gpt_activation_entry\n",
    "    except:\n",
    "        gpt_activation_status = False\n",
    "    \n",
    "    gpt_questions = ''\n",
    "    \n",
    "    try:\n",
    "        gpt_questions = gpt_questions_entry[0: question_characters_bound]\n",
    "    \n",
    "    except:\n",
    "        print('GPT questions not entered.')\n",
    "    \n",
    "    new_row = {'Processed': '',\n",
    "           'Timestamp': timestamp,\n",
    "           'Your name': name, \n",
    "           'Your email address': email, \n",
    "           'Your GPT API key': gpt_api_key, \n",
    "            'Enter search query': query_entry,\n",
    "           'Find (method)': method_entry, \n",
    "          'Metadata inclusion': True, #Placeholder even though no metadata collected\n",
    "           'Maximum number of judgments': judgments_counter_bound, \n",
    "           'Enter your questions for GPT': gpt_questions, \n",
    "            'Use GPT': gpt_activation_status,\n",
    "              'Use own account': own_account,\n",
    "            'Use flagship version of GPT' : gpt_enhancement\n",
    "          }\n",
    "\n",
    "    df_master_new = pd.DataFrame(new_row, index = [0])\n",
    "        \n",
    "    return df_master_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f2f670-342c-44ab-a87b-58aa2b023b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of search methods\n",
    "\n",
    "er_methods_list = ['using autosearch', 'this Boolean query', 'any of these words', 'all of these words', 'this phrase', 'this case name']\n",
    "er_method_types = ['auto', 'boolean', 'any', 'all', 'phrase', 'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2a0e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function turning search terms to search results url\n",
    "\n",
    "def er_search(query= '', \n",
    "              method = ''\n",
    "             ):\n",
    "    base_url = \"http://www.commonlii.org/cgi-bin/sinosrch.cgi?\" #+ method\n",
    "\n",
    "    method_index = er_methods_list.index(method)\n",
    "    method_type = er_method_types[method_index]\n",
    "\n",
    "    query_text = query\n",
    "\n",
    "    params = {'meta' : '/commonlii', \n",
    "              'mask_path' : '+uk/cases/EngR+', \n",
    "              'method' : method_type,\n",
    "              'query' : query_text\n",
    "             }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    return response.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6321d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function turning search results url to case_link_pairs to judgments\n",
    "def er_search_results_to_case_link_pairs(url_search_results, judgment_counter_bound):\n",
    "    #Scrape webpage of search results\n",
    "    page = requests.get(url_search_results)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "    hrefs = soup.find_all('a', href=True)\n",
    "    case_link_pairs = []\n",
    "\n",
    "    #number of search results\n",
    "    docs_found_string = str(soup.find_all('span', {'class' : 'ndocs'})).split('Documents found:')[1].split('<')[0].replace(' ', '')\n",
    "    docs_found = int(docs_found_string)\n",
    "    \n",
    "    #Start counter\n",
    "    counter = 1\n",
    "    \n",
    "    for link in hrefs:\n",
    "        if ((counter <= judgment_counter_bound) and (' ER ' in str(link)) and ('cases' in str(link))):\n",
    "#        if ((counter <= judgment_counter_bound) and ('commonlii' in str(link)) and ('cases/EngR' in str(link)) and ('LawCite' not in str(link))):\n",
    "            case = link.get_text()\n",
    "            link_direct = link.get('href')\n",
    "            sub_link = link_direct.replace('.html', '.pdf').split('cases')[1].split('.pdf')[0]\n",
    "            pdf_link = 'http://www.commonlii.org/uk/cases' + sub_link + '.pdf'\n",
    "            dict_object = { 'case':case, 'link_direct': pdf_link}\n",
    "            case_link_pairs.append(dict_object)\n",
    "            counter = counter + 1\n",
    "        \n",
    "    for ending in range(20, docs_found, 20):\n",
    "        if counter <= min(judgment_counter_bound, docs_found):\n",
    "            url_next_page = url_search_results + ';offset=' + f\"{ending}\"\n",
    "            page_judgment_next_page = requests.get(url_next_page)\n",
    "            soup_judgment_next_page = BeautifulSoup(page_judgment_next_page.content, \"lxml\")\n",
    "            \n",
    "            hrefs_next_page = soup_judgment_next_page.find_all('a', href=True)\n",
    "            for extra_link in hrefs_next_page:\n",
    "                if ((counter <= judgment_counter_bound) and (' ER ' in str(extra_link)) and ('cases' in str(extra_link))):\n",
    "#                if ((counter <= judgment_counter_bound) and ('commonlii' in str(extra_link)) and ('cases/EngR' in str(extra_link)) and ('LawCite' not in str(extra_link))):\n",
    "                    case = extra_link.get_text()\n",
    "                    extra_link_direct = extra_link.get('href')\n",
    "                    sub_extra_link = extra_link_direct.replace('.html', '.pdf').split('cases')[1].split('.pdf')[0]\n",
    "                    pdf_extra_link = 'http://www.commonlii.org/uk/cases' + sub_extra_link + '.pdf'\n",
    "                    dict_object = { 'case':case, 'link_direct': pdf_extra_link}\n",
    "                    case_link_pairs.append(dict_object)\n",
    "                    counter = counter + 1\n",
    "\n",
    "            pause.seconds(np.random.randint(5, 15))\n",
    "            \n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return case_link_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a72562c-8dce-4d9e-a2f4-c7a14a5877a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert case-link pairs to judgment text\n",
    "\n",
    "def er_judgment_text(case_link_pair):\n",
    "    url = case_link_pair['link_direct']\n",
    "    headers = {'User-Agent': 'whatever'}\n",
    "    r = requests.get(url, headers=headers)\n",
    "    remote_file_bytes = io.BytesIO(r.content)\n",
    "    pdfdoc_remote = PyPDF2.PdfReader(remote_file_bytes)\n",
    "    \n",
    "    text_list = []\n",
    "    \n",
    "    for page in pdfdoc_remote.pages:\n",
    "        text_list.append(page.extract_text())\n",
    "    \n",
    "    return str(text_list)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86a85223-64ca-4d1b-998c-1c2769c5a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Meta labels and judgment combined\n",
    "\n",
    "def er_meta_judgment_dict(case_link_pair):\n",
    "    \n",
    "    judgment_dict = {'Case name': '',\n",
    "                     'Medium neutral citation' : '', \n",
    "                     'English Reports': '', \n",
    "                     'Nominate Reports': '', \n",
    "                     'Hyperlink to CommonLII': '', \n",
    "                     'Year' : '', \n",
    "                     'judgment': ''\n",
    "                    }\n",
    "\n",
    "    case_name = case_link_pair['case']\n",
    "    year = case_link_pair['link_direct'].split('EngR/')[-1][0:4]\n",
    "    case_num = case_link_pair['link_direct'].split('/')[-1].replace('.pdf', '')\n",
    "    mnc = '[' + year + ']' + ' EngR ' + case_num\n",
    "\n",
    "    er_cite = ''\n",
    "    nr_cite = ''\n",
    "        \n",
    "    try:\n",
    "        case_name = case_link_pair['case'].split('[')[0][:-1]\n",
    "        nr_cite = case_link_pair['case'].split(';')[1][1:]\n",
    "        er_cite = case_link_pair['case'].split(';')[2][1:]\n",
    "    except:\n",
    "        pass\n",
    "                \n",
    "    judgment_dict['Case name'] = case_name\n",
    "    judgment_dict['Medium neutral citation'] = mnc\n",
    "    judgment_dict['English Reports'] = er_cite\n",
    "    judgment_dict['Nominate Reports'] = nr_cite\n",
    "    judgment_dict['Year'] = year\n",
    "    judgment_dict['Hyperlink to CommonLII'] = link(case_link_pair['link_direct'])\n",
    "    judgment_dict['judgment'] = er_judgment_text(case_link_pair)\n",
    "\n",
    "#    pause.seconds(np.random.randint(5, 15))\n",
    "    \n",
    "    #try:\n",
    "     #   er_judgment_text = str(soup.find_all('content'))\n",
    "    #except:\n",
    "      #  er_judgment_text= soup.get_text(strip=True)\n",
    "        \n",
    "    return judgment_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a2d594",
   "metadata": {},
   "source": [
    "# GPT functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c24172f8-acaf-4be0-ac0c-dd0dfc6737a9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gpt_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Import functions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_by_line, GPT_label_dict, is_api_key_valid, gpt_input_cost, gpt_output_cost, tokens_cap, max_output, num_tokens_from_string, judgment_prompt_json, GPT_json, engage_GPT_json  \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Import variables\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m question_characters_bound, default_judgment_counter_bound\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gpt_functions'"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from gpt_functions import split_by_line, GPT_label_dict, is_api_key_valid, gpt_input_cost, gpt_output_cost, tokens_cap, max_output, num_tokens_from_string, judgment_prompt_json, GPT_json, engage_GPT_json  \n",
    "#Import variables\n",
    "from gpt_functions import question_characters_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c25483b-ed35-41f6-882d-6b84c6045b99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'question_characters_bound' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestions for GPT are capped at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mquestion_characters_bound\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m characters.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe default number of judgments to scrape per request is capped at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdefault_judgment_counter_bound\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'question_characters_bound' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Questions for GPT are capped at {question_characters_bound} characters.\\n\")\n",
    "print(f\"The default number of judgments to scrape per request is capped at {default_judgment_counter_bound}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecf1d9d-6f84-4ec2-976f-73a4fc752235",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#For checking questions and answers\n",
    "from common_functions import check_questions_answers\n",
    "\n",
    "from gpt_functions import questions_check_system_instruction, GPT_questions_check, checked_questions_json, answers_check_system_instruction\n",
    "\n",
    "if check_questions_answers() > 0:\n",
    "    print(f'By default, questions and answers are checked for potential privacy violation.')\n",
    "else:\n",
    "    print(f'By default, questions and answers are NOT checked for potential privacy violation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2292028-93db-41f6-b886-0fcee71ad703",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Jurisdiction specific instruction\n",
    "\n",
    "role_content_er = 'You are a legal research assistant helping an academic researcher to answer questions about a public judgment. You will be provided with the judgment and metadata in JSON form. Please answer questions based only on information contained in the judgment and metadata. Where your answer comes from a part of the judgment or metadata, include a reference to that part of the judgment or metadata. If you cannot answer the questions based on the judgment or metadata, do not make up information, but instead write \"answer not found\". The \"judgment\" field of the JSON given to you sometimes contains judgments for multiple cases. If you detect multiple judgments in the \"judgment\" field, please provide answers only for the specific case identified in the \"Case name\" field of the JSON given to you.'\n",
    "\n",
    "system_instruction = role_content_er\n",
    "\n",
    "intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a03e8eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-04 19:00:37.524 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'default_judgment_counter_bound' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#Upperbound on number of judgments to scrape\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjudgments_counter_bound\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m st\u001b[38;5;241m.\u001b[39msession_state:\n\u001b[0;32m---> 13\u001b[0m     st\u001b[38;5;241m.\u001b[39msession_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjudgments_counter_bound\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdefault_judgment_counter_bound\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'default_judgment_counter_bound' is not defined"
     ]
    }
   ],
   "source": [
    "#Initialize default GPT settings\n",
    "\n",
    "if 'gpt_model' not in st.session_state:\n",
    "    st.session_state['gpt_model'] = \"gpt-4o-mini\"\n",
    "    \n",
    "#Initialize API key\n",
    "if 'gpt_api_key' not in st.session_state:\n",
    "\n",
    "    st.session_state['gpt_api_key'] = st.secrets[\"openai\"][\"gpt_api_key\"]\n",
    "\n",
    "#Upperbound on number of judgments to scrape\n",
    "if 'judgments_counter_bound' not in st.session_state:\n",
    "    st.session_state['judgments_counter_bound'] = default_judgment_counter_bound\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83981e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain parameters\n",
    "\n",
    "def er_run(df_master):\n",
    "    df_master = df_master.fillna('')\n",
    "\n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    df_master['Enter your questions for GPT'] = df_master['Enter your questions for GPT'][0: question_characters_bound].apply(split_by_line)\n",
    "    df_master['questions_json'] = df_master['Enter your questions for GPT'].apply(GPT_label_dict)\n",
    "    \n",
    "    #Create judgments file\n",
    "    judgments_file = []\n",
    "    \n",
    "    #Conduct search\n",
    "\n",
    "    url_search_results = er_search(query= df_master.loc[0, 'Enter search query'], \n",
    "                                   method = df_master.loc[0, 'Find (method)']\n",
    "                                  )\n",
    "        \n",
    "    judgments_counter_bound = int(df_master.loc[0, 'Maximum number of judgments'])\n",
    "\n",
    "    case_link_pairs = er_search_results_to_case_link_pairs(url_search_results, judgments_counter_bound)\n",
    "\n",
    "    for case_link_pair in case_link_pairs:\n",
    "\n",
    "        judgment_dict = er_meta_judgment_dict(case_link_pair)\n",
    "        judgments_file.append(judgment_dict)\n",
    "        pause.seconds(np.random.randint(5, 15))\n",
    "    \n",
    "    #Create and export json file with search results\n",
    "    json_individual = json.dumps(judgments_file, indent=2)\n",
    "    \n",
    "    df_individual = pd.read_json(json_individual)\n",
    "    \n",
    "    #Instruct GPT\n",
    "\n",
    "    GPT_activation = int(df_master.loc[0, 'Use GPT'])\n",
    "    \n",
    "    #GPT model\n",
    "\n",
    "    if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "        gpt_model = \"gpt-4o\"\n",
    "    else:        \n",
    "        gpt_model = \"gpt-4o-mini\"\n",
    "        \n",
    "    #apply GPT_individual to each respondent's judgment spreadsheet\n",
    "    \n",
    "\n",
    "    questions_json = df_master.loc[0, 'questions_json']\n",
    "            \n",
    "    #Engage GPT\n",
    "    df_updated = engage_GPT_json(questions_json, df_individual, GPT_activation, gpt_model, system_instruction)\n",
    "\n",
    "    df_updated.pop('judgment')\n",
    "    \n",
    "    return df_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dacc31ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def er_search_url(df_master):\n",
    "\n",
    "    df_master = df_master.fillna('')\n",
    "    \n",
    "    #Conduct search\n",
    "    \n",
    "    url = er_search(query= df_master.loc[0, 'Enter search query'],\n",
    "                    method= df_master.loc[0, 'Find (method)']\n",
    "                   )\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafb4142-30d3-4dfa-a2b1-0317592f5704",
   "metadata": {},
   "source": [
    "# For vision, ER only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f92c3f2-04b6-4a0f-aac6-d55c7aad15ed",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pdf2image\n",
    "from PIL import Image\n",
    "import math\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "021bbaaf-a520-4113-a22b-c6206ff44680",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gpt_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Import functions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_image_dims, calculate_image_token_cost\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gpt_functions'"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from gpt_functions import get_image_dims, calculate_image_token_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9ed9ee2-0dea-4f6a-840b-bb5ed83eca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert case-link pairs to judgment text\n",
    "\n",
    "def er_judgment_tokens_b64(case_link_pair):\n",
    "\n",
    "    output_b64 = {'judgment':[], 'tokens_raw': 0}\n",
    "    \n",
    "    url = case_link_pair['link_direct']\n",
    "    headers = {'User-Agent': 'whatever'}\n",
    "    r = requests.get(url, headers=headers)\n",
    "    bytes_data = io.BytesIO(r.content)\n",
    "    \n",
    "    images = pdf2image.convert_from_bytes(bytes_data.read(), timeout=30, fmt=\"jpeg\")\n",
    "    \n",
    "    for image in images[ : len(images)]:\n",
    "\n",
    "        output = BytesIO()\n",
    "        image.save(output, format='JPEG')\n",
    "        im_data = output.getvalue()\n",
    "        \n",
    "        image_data = base64.b64encode(im_data)\n",
    "        if not isinstance(image_data, str):\n",
    "            # Python 3, decode from bytes to string\n",
    "            image_data = image_data.decode()\n",
    "        data_url = 'data:image/jpg;base64,' + image_data\n",
    "\n",
    "        #b64 = base64.b64encode(image_raw).decode('utf-8')\n",
    "\n",
    "        b64_to_attach = data_url\n",
    "        #b64_to_attach = f\"data:image/png;base64,{b64}\"\n",
    "\n",
    "        output_b64['judgment'].append(b64_to_attach)\n",
    "    \n",
    "    for image_b64 in output_b64['judgment']:\n",
    "\n",
    "        output_b64['tokens_raw'] = output_b64['tokens_raw'] + calculate_image_token_cost(image_b64, detail=\"auto\")\n",
    "    \n",
    "    return output_b64\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a95d7b4-0db7-4e5f-84e4-a6e937420034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Meta labels and judgment combined\n",
    "\n",
    "def er_meta_judgment_dict_b64(case_link_pair):\n",
    "    \n",
    "    judgment_dict = {'Case name': '',\n",
    "                     'Medium neutral citation' : '', \n",
    "                     'English Reports': '', \n",
    "                     'Nominate Reports': '', \n",
    "                     'Hyperlink to CommonLII': '', \n",
    "                     'Year' : '', \n",
    "                     'judgment': '', \n",
    "                     'tokens_raw': 0\n",
    "                    }\n",
    "\n",
    "    case_name = case_link_pair['case']\n",
    "    year = case_link_pair['link_direct'].split('EngR/')[-1][0:4]\n",
    "    case_num = case_link_pair['link_direct'].split('/')[-1].replace('.pdf', '')\n",
    "    mnc = '[' + year + ']' + ' EngR ' + case_num\n",
    "\n",
    "    er_cite = ''\n",
    "    nr_cite = ''\n",
    "        \n",
    "    try:\n",
    "        case_name = case_link_pair['case'].split('[')[0][:-1]\n",
    "        nr_cite = case_link_pair['case'].split(';')[1][1:]\n",
    "        er_cite = case_link_pair['case'].split(';')[2][1:]\n",
    "    except:\n",
    "        pass\n",
    "                \n",
    "    judgment_dict['Case name'] = case_name\n",
    "    judgment_dict['Medium neutral citation'] = mnc\n",
    "    judgment_dict['English Reports'] = er_cite\n",
    "    judgment_dict['Nominate Reports'] = nr_cite\n",
    "    judgment_dict['Year'] = year\n",
    "    judgment_dict['Hyperlink to CommonLII'] = link(case_link_pair['link_direct'])\n",
    "    judgment_dict['judgment'] = er_judgment_tokens_b64(case_link_pair)['judgment']\n",
    "    judgment_dict['tokens_raw'] = er_judgment_tokens_b64(case_link_pair)['tokens_raw']\n",
    "\n",
    "#    pause.seconds(np.random.randint(5, 15))\n",
    "    \n",
    "    #try:\n",
    "     #   er_judgment_text = str(soup.find_all('content'))\n",
    "    #except:\n",
    "      #  er_judgment_text= soup.get_text(strip=True)\n",
    "        \n",
    "    return judgment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2332d698-126d-4069-8ee3-286af5ab7be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT answer function for answers in json form, YES TOKENS\n",
    "#For gpt-4o vision\n",
    "\n",
    "def er_GPT_b64_json(questions_json, judgment_json, gpt_model, system_instruction):\n",
    "    #'question_json' variable is a json of questions to GPT\n",
    "\n",
    "    #file_for_GPT = [{\"role\": \"user\", \"content\": file_prompt(file_triple, gpt_model) + 'you will be given questions to answer in JSON form.'}]\n",
    "\n",
    "    #Add images to messages to GPT\n",
    "    image_content_value = [{\"type\": \"text\", \"text\": 'Based on the following images:'}]\n",
    "\n",
    "    for image_b64 in judgment_json['judgment']:\n",
    "        image_message_to_attach = {\"type\": \"image_url\", \"image_url\": {\"url\": image_b64,}}\n",
    "        image_content_value.append(image_message_to_attach)\n",
    "\n",
    "    image_content = [{\"role\": \"user\", \n",
    "                      \"content\": image_content_value\n",
    "                     }\n",
    "                  ]\n",
    "\n",
    "    #Create metadata content\n",
    "\n",
    "    metadata_content = [{\"role\": \"user\", \"content\": ''}]\n",
    "\n",
    "    metadata_json_raw = judgment_json\n",
    "\n",
    "    for key in ['Hyperlink to CommonLII', 'judgment', 'tokens_raw']:\n",
    "        try:\n",
    "            metadata_json_raw.pop(key)\n",
    "        except:\n",
    "            print(f'Unable to remove {key} from metadata_json_raw')\n",
    "\n",
    "    metadata_json = metadata_json_raw\n",
    "\n",
    "    if 'judgment' not in metadata_json.keys():\n",
    "        metadata_content = [{\"role\": \"user\", \"content\": 'Based on the following metadata:' + str(metadata_json)}]\n",
    "\n",
    "    #Create json direction content\n",
    "\n",
    "    json_direction = [{\"role\": \"user\", \"content\": 'You will be given questions to answer in JSON form.'}]\n",
    "\n",
    "    file_for_GPT = image_content + metadata_content + json_direction\n",
    "    \n",
    "    #Create answer format\n",
    "    \n",
    "    q_keys = [*questions_json]\n",
    "    \n",
    "    answers_json = {}\n",
    "    \n",
    "    for q_index in q_keys:\n",
    "        answers_json.update({q_index: 'Your answer to the question with index ' + q_index + '. State specific page numbers or sections of the judgment.'})\n",
    "    \n",
    "    #Create questions, which include the answer format\n",
    "    \n",
    "    question_for_GPT = [{\"role\": \"user\", \"content\": json.dumps(questions_json) + ' Give responses in the following JSON form: ' + json.dumps(answers_json)}]\n",
    "    \n",
    "    #Create messages in one prompt for GPT\n",
    "    #ER specific intro\n",
    "\n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction}]\n",
    "\n",
    "    messages_for_GPT = intro_for_GPT + file_for_GPT + question_for_GPT\n",
    "    \n",
    "#   return messages_for_GPT\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    try:\n",
    "        #completion = client.chat.completions.create(\n",
    "        completion = openai.chat.completions.create(\n",
    "            model=gpt_model,\n",
    "            messages=messages_for_GPT, \n",
    "            response_format={\"type\": \"json_object\"}, \n",
    "            temperature = 0.2, \n",
    "            top_p = 0.2\n",
    "        )\n",
    "        \n",
    "#        return completion.choices[0].message.content #This gives answers as a string containing a dictionary\n",
    "        \n",
    "        #To obtain a json directly, use below\n",
    "        answers_dict = json.loads(completion.choices[0].message.content)\n",
    "        \n",
    "        #Obtain tokens\n",
    "        output_tokens = completion.usage.completion_tokens\n",
    "        \n",
    "        prompt_tokens = completion.usage.prompt_tokens\n",
    "        \n",
    "        #return [answers_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "        #Check answers\n",
    "\n",
    "        if check_questions_answers() > 0:\n",
    "            \n",
    "            try:\n",
    "                redacted_output = GPT_answers_check(answers_dict, gpt_model, answers_check_system_instruction)\n",
    "        \n",
    "                redacted_answers_dict = redacted_output[0]\n",
    "        \n",
    "                redacted_answers_output_tokens = redacted_output[1]\n",
    "        \n",
    "                redacted_answers_prompt_tokens = redacted_output[2]\n",
    "        \n",
    "                return [redacted_answers_dict, output_tokens + redacted_answers_output_tokens, prompt_tokens + redacted_answers_prompt_tokens]\n",
    "\n",
    "                print('Answers checked.')\n",
    "                \n",
    "            except Exception as e:\n",
    "    \n",
    "                print('Answers check failed.')\n",
    "    \n",
    "                print(e)\n",
    "    \n",
    "                return [answers_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "        else:\n",
    "\n",
    "            print('Answers not checked.')\n",
    "            \n",
    "            return [answers_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "    except Exception as error:\n",
    "        \n",
    "        print('GPT failed to produce answers.')\n",
    "\n",
    "        for q_index in q_keys:\n",
    "            answers_json[q_index] = error\n",
    "        \n",
    "        return [answers_json, 0, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d494f30b-28e6-40fd-bfcb-47e184e2d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT function for each respondent's dataframe, index by judgment then question, with input and output tokens given by GPT itself\n",
    "#For gpt-4o vision\n",
    "\n",
    "#The following function DOES NOT check for existence of questions for GPT\n",
    "    # To so check, active line marked as #*\n",
    "def er_engage_GPT_b64_json(questions_json, df_individual, GPT_activation, gpt_model, system_instruction):\n",
    "    # Variable questions_json refers to the json of questions\n",
    "    # Variable df_individual refers to each respondent's df\n",
    "    # Variable activation refers to status of GPT activation (real or test)\n",
    "    # The output is a new JSON for the relevant respondent with new columns re:\n",
    "        # f\"Length of first 10 pages in tokens (up to {tokens_cap(gpt_model)} given to GPT)\"\n",
    "        # 'GPT cost estimate (USD excl GST)'\n",
    "        # 'GPT time estimate (seconds)'\n",
    "        # GPT questions/answers\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "\n",
    "    #Make a copy of questions for making headings later\n",
    "    unchecked_questions_json = questions_json.copy()\n",
    "\n",
    "    #Check questions for privacy violation\n",
    "\n",
    "    if check_questions_answers() > 0:\n",
    "    \n",
    "        try:\n",
    "    \n",
    "            labels_output = GPT_questions_check(questions_json, gpt_model, questions_check_system_instruction)\n",
    "    \n",
    "            labels_output_tokens = labels_output[1]\n",
    "    \n",
    "            labels_prompt_tokens = labels_output[2]\n",
    "        \n",
    "            questions_json = checked_questions_json(questions_json, labels_output)\n",
    "\n",
    "            print('Questions checked.')\n",
    "    \n",
    "        except Exception as e:\n",
    "            \n",
    "            print('Questions check failed.')\n",
    "            \n",
    "            print(e)\n",
    "    \n",
    "            labels_output_tokens = 0\n",
    "            \n",
    "            labels_prompt_tokens = 0\n",
    "\n",
    "    else:\n",
    "\n",
    "        print('Questions not checked.')\n",
    "        \n",
    "        labels_output_tokens = 0\n",
    "        \n",
    "        labels_prompt_tokens = 0\n",
    "    \n",
    "    question_keys = [*questions_json]\n",
    "\n",
    "    for judgment_index in df_individual.index:\n",
    "        \n",
    "        judgment_json = df_individual.to_dict('index')[judgment_index]\n",
    "        \n",
    "        #Calculate and append number of tokens of judgment, regardless of whether given to GPT\n",
    "        #judgment_json['tokens_raw'] = num_tokens_from_string(str(judgment_json), \"cl100k_base\")\n",
    "\n",
    "        df_individual.loc[judgment_index, f\"Tokens (up to {tokens_cap(gpt_model)} given to GPT)\"] = judgment_json['tokens_raw']       \n",
    "\n",
    "        #Indicate whether judgment truncated\n",
    "        \n",
    "        df_individual.loc[judgment_index, \"judgment truncated (if given to GPT)?\"] = ''       \n",
    "        \n",
    "        if judgment_json['tokens_raw'] <= tokens_cap(gpt_model):\n",
    "            \n",
    "            df_individual.loc[judgment_index, \"judgment truncated (if given to GPT)?\"] = 'No'\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            df_individual.loc[judgment_index, \"judgment truncated (if given to GPT)?\"] = 'Yes'\n",
    "\n",
    "        #Create columns for respondent's GPT cost, time\n",
    "        df_individual.loc[judgment_index, 'GPT cost estimate (USD excl GST)'] = ''\n",
    "        df_individual.loc[judgment_index, 'GPT time estimate (seconds)'] = ''\n",
    "                \n",
    "        #Calculate GPT start time\n",
    "\n",
    "        GPT_start_time = datetime.now()\n",
    "\n",
    "        #Depending on activation status, apply GPT_json function to each judgment, gives answers as a string containing a dictionary\n",
    "\n",
    "        if int(GPT_activation) > 0:\n",
    "            GPT_judgment_json = er_GPT_b64_json(questions_json, judgment_json, gpt_model, system_instruction) #Gives [answers as a JSON, output tokens, input tokens]\n",
    "            answers_dict = GPT_judgment_json[0]\n",
    "\n",
    "            #Calculate and append GPT finish time and time difference to individual df\n",
    "            GPT_finish_time = datetime.now()\n",
    "            \n",
    "            GPT_time_difference = GPT_finish_time - GPT_start_time\n",
    "    \n",
    "            df_individual.loc[judgment_index, 'GPT time estimate (seconds)'] = GPT_time_difference.total_seconds()\n",
    "        \n",
    "        else:\n",
    "            answers_dict = {}    \n",
    "            for q_index in question_keys:\n",
    "                #Increases judgment index by 2 to ensure consistency with Excel spreadsheet\n",
    "                answer = 'Placeholder answer for ' + ' judgment ' + str(int(judgment_index) + 2) + ' ' + str(q_index)\n",
    "                answers_dict.update({q_index: answer})\n",
    "            \n",
    "            #Own calculation of GPT costs for Placeholder answer fors\n",
    "\n",
    "            #Calculate capped judgment tokens\n",
    "\n",
    "            judgment_capped_tokens = min(judgment_json['tokens_raw'], tokens_cap(gpt_model))\n",
    "\n",
    "            #Calculate questions tokens and cost\n",
    "\n",
    "            questions_tokens = num_tokens_from_string(json.dumps(questions_json), \"cl100k_base\")\n",
    "\n",
    "            #Calculate metadata tokens\n",
    "\n",
    "            metadata_tokens = 0\n",
    "            \n",
    "            metadata_json_for_counting = judgment_json\n",
    "\n",
    "            for key in ['Hyperlink to CommonLII', 'judgment', 'tokens_raw']:\n",
    "                try:\n",
    "                    metadata_json_for_counting.pop(key)\n",
    "                except:\n",
    "                    print(f'Unable to remove {key} from metadata_json_for_counting')        \n",
    "\n",
    "            if 'judgment' not in metadata_json_for_counting.keys():\n",
    "                metadata_tokens = metadata_tokens + num_tokens_from_string(str(metadata_json_for_counting), \"cl100k_base\")\n",
    "\n",
    "            #Calculate other instructions' tokens\n",
    "\n",
    "            other_instructions = system_instruction + 'you will be given questions to answer in JSON form.' + ' Give responses in the following JSON form: '\n",
    "\n",
    "            other_tokens = num_tokens_from_string(other_instructions, \"cl100k_base\") + len(question_keys)*num_tokens_from_string(\"GPT question x:  Your answer to the question with index GPT question x. State specific page numbers or sections of the judgment.\", \"cl100k_base\")\n",
    "\n",
    "            #Calculate number of tokens of answers\n",
    "            answers_tokens = num_tokens_from_string(str(answers_dict), \"cl100k_base\")\n",
    "\n",
    "            input_tokens = judgment_capped_tokens + questions_tokens + metadata_tokens + other_tokens\n",
    "            \n",
    "            GPT_judgment_json = [answers_dict, answers_tokens, input_tokens]\n",
    "\n",
    "        #Create GPT question headings and append answers to individual spreadsheets\n",
    "\n",
    "        for question_index in question_keys:\n",
    "            #If not checking questions\n",
    "            #question_heading = question_index + ': ' + questions_json[question_index]\n",
    "\n",
    "            #If checking questions\n",
    "            question_heading = question_index + ': ' + unchecked_questions_json[question_index]\n",
    "\n",
    "            df_individual.loc[judgment_index, question_heading] = answers_dict[question_index]\n",
    "\n",
    "        #Calculate GPT costs\n",
    "\n",
    "        #Calculate GPT costs\n",
    "\n",
    "        #If check for questions\n",
    "        GPT_cost = (GPT_judgment_json[1] + labels_output_tokens/len(df_individual))*gpt_output_cost(gpt_model) + (GPT_judgment_json[2] + labels_prompt_tokens/len(df_individual))*gpt_input_cost(gpt_model)\n",
    "\n",
    "        #If no check for questions\n",
    "        #GPT_cost = GPT_judgment_json[1]*gpt_output_cost(gpt_model) + GPT_judgment_json[2]*gpt_input_cost(gpt_model)\n",
    "\n",
    "        #Calculate and append GPT cost to individual df\n",
    "        df_individual.loc[judgment_index, 'GPT cost estimate (USD excl GST)'] = GPT_cost\n",
    "    \n",
    "    return df_individual\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3046c1e-5d6f-463a-92e4-84897573bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For gpt-4o vision\n",
    "\n",
    "def er_run_b64(df_master):\n",
    "\n",
    "    df_master = df_master.fillna('')\n",
    "\n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    df_master['Enter your questions for GPT'] = df_master['Enter your questions for GPT'][0: question_characters_bound].apply(split_by_line)\n",
    "    df_master['questions_json'] = df_master['Enter your questions for GPT'].apply(GPT_label_dict)\n",
    "    \n",
    "    #Create judgments file\n",
    "    judgments_file = []\n",
    "    \n",
    "    #Conduct search\n",
    "\n",
    "    url_search_results = er_search(query= df_master.loc[0, 'Enter search query'], \n",
    "                                   method = df_master.loc[0, 'Find (method)']\n",
    "                                  )\n",
    "        \n",
    "    judgments_counter_bound = int(df_master.loc[0, 'Maximum number of judgments'])\n",
    "\n",
    "    case_link_pairs = er_search_results_to_case_link_pairs(url_search_results, judgments_counter_bound)\n",
    "\n",
    "    for case_link_pair in case_link_pairs:\n",
    "\n",
    "        judgment_dict = er_meta_judgment_dict_b64(case_link_pair)\n",
    "        judgments_file.append(judgment_dict)\n",
    "        pause.seconds(np.random.randint(5, 15))\n",
    "    \n",
    "    #Create and export json file with search results\n",
    "    json_individual = json.dumps(judgments_file, indent=2)\n",
    "    \n",
    "    df_individual = pd.read_json(json_individual)\n",
    "    \n",
    "    #Instruct GPT\n",
    "\n",
    "    GPT_activation = int(df_master.loc[0, 'Use GPT'])\n",
    "    \n",
    "    #GPT model\n",
    "\n",
    "    if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "        gpt_model = \"gpt-4o\"\n",
    "    else:        \n",
    "        gpt_model = \"gpt-4o-mini\"\n",
    "        \n",
    "    #apply GPT_individual to each respondent's judgment spreadsheet\n",
    "    \n",
    "    questions_json = df_master.loc[0, 'questions_json']\n",
    "            \n",
    "    #apply GPT_individual to each respondent's judgment spreadsheet\n",
    "\n",
    "    df_updated = er_engage_GPT_b64_json(questions_json, df_individual, GPT_activation, gpt_model, system_instruction)\n",
    "\n",
    "    #Remove redundant columns\n",
    "\n",
    "    for column in ['tokens_raw', 'judgment']:\n",
    "        try:\n",
    "            df_updated.pop(column)\n",
    "        except:\n",
    "            print(f\"No {column} column.\")\n",
    "\n",
    "    return df_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d5dcd6",
   "metadata": {},
   "source": [
    "# Streamlit form, functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d39542ce-8064-49ad-89da-907107f6bfe3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'common_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Import functions and variables\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcommon_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m open_page, clear_cache_except_validation_df_master, tips\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'common_functions'"
     ]
    }
   ],
   "source": [
    "#Import functions and variables\n",
    "from common_functions import open_page, clear_cache_except_validation_df_master, tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85681973-6bb0-44b9-a537-ae6a2ac75709",
   "metadata": {},
   "source": [
    "## Initialize session states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca910eae-4fa4-4113-8805-3c054f17ffae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'default_judgment_counter_bound' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m st\u001b[38;5;241m.\u001b[39msession_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf_master\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYour GPT API key\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     21\u001b[0m st\u001b[38;5;241m.\u001b[39msession_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf_master\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetadata inclusion\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m st\u001b[38;5;241m.\u001b[39msession_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf_master\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaximum number of judgments\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdefault_judgment_counter_bound\u001b[49m\n\u001b[1;32m     23\u001b[0m st\u001b[38;5;241m.\u001b[39msession_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf_master\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnter your questions for GPT\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     24\u001b[0m st\u001b[38;5;241m.\u001b[39msession_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf_master\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUse GPT\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'default_judgment_counter_bound' is not defined"
     ]
    }
   ],
   "source": [
    "#Initialize default values\n",
    "\n",
    "if 'gpt_enhancement_entry' not in st.session_state:\n",
    "    st.session_state['gpt_enhancement_entry'] = False\n",
    "\n",
    "if 'gpt_api_key_validity' not in st.session_state:\n",
    "    st.session_state['gpt_api_key_validity'] = False\n",
    "\n",
    "if 'own_account' not in st.session_state:\n",
    "    st.session_state['own_account'] = False\n",
    "\n",
    "if 'need_resetting' not in st.session_state:\n",
    "        \n",
    "    st.session_state['need_resetting'] = 0\n",
    "\n",
    "if 'df_master' not in st.session_state:\n",
    "\n",
    "    #Generally applicable\n",
    "    st.session_state['df_master'] = pd.DataFrame([])\n",
    "    st.session_state['df_master'].loc[0, 'Your name'] = ''\n",
    "    st.session_state['df_master'].loc[0, 'Your email address'] = ''\n",
    "    st.session_state['df_master'].loc[0, 'Your GPT API key'] = ''\n",
    "    st.session_state['df_master'].loc[0, 'Metadata inclusion'] = True\n",
    "    st.session_state['df_master'].loc[0, 'Maximum number of judgments'] = default_judgment_counter_bound\n",
    "    st.session_state['df_master'].loc[0, 'Enter your questions for GPT'] = ''\n",
    "    st.session_state['df_master'].loc[0, 'Use GPT'] = False\n",
    "    st.session_state['df_master'].loc[0, 'Use own account'] = False\n",
    "    st.session_state['df_master'].loc[0, 'Use flagship version of GPT'] = False\n",
    "\n",
    "    #Jurisdiction specific\n",
    "    st.session_state.df_master.loc[0, 'Enter search query'] = None\n",
    "    st.session_state.df_master.loc[0, 'Find (method)'] = 'this Boolean query'\n",
    "\n",
    "    #Generally applicable\n",
    "    st.session_state['df_master'] = st.session_state['df_master'].replace({np.nan: None})\n",
    "\n",
    "if 'df_individual_output' not in st.session_state:\n",
    "\n",
    "    st.session_state['df_individual_output'] = pd.DataFrame([])\n",
    "\n",
    "#Disable toggles\n",
    "if 'disable_input' not in st.session_state:\n",
    "    st.session_state[\"disable_input\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77140a95-68ef-47f4-acec-c8765cf14c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If landing page is not home\n",
    "if 'page_from' not in st.session_state:\n",
    "    st.session_state['page_from'] = 'Home.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5945ecc8-f9bc-4fd4-8d43-01dbae99b91a",
   "metadata": {},
   "source": [
    "## Form before AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c683d9af",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-04 19:01:06.169 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/Ben/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Find (method)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Find (method)'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 23\u001b[0m\n\u001b[1;32m     18\u001b[0m     st\u001b[38;5;241m.\u001b[39msubheader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour search terms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m     st\u001b[38;5;241m.\u001b[39mmarkdown(\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mFor search tips, please visit [CommonLII](http://www.commonlii.org/form/search1.html?mask=uk/cases/EngR). This section mimics their search function.\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m     method_entry \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39mselectbox(label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFind\u001b[39m\u001b[38;5;124m'\u001b[39m, options \u001b[38;5;241m=\u001b[39m er_methods_list, index\u001b[38;5;241m=\u001b[39m er_methods_list\u001b[38;5;241m.\u001b[39mindex(\u001b[43mst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf_master\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFind (method)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[1;32m     25\u001b[0m     query_entry \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39mtext_input(label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnter search query\u001b[39m\u001b[38;5;124m'\u001b[39m, value \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39msession_state\u001b[38;5;241m.\u001b[39mdf_master\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnter search query\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     27\u001b[0m     st\u001b[38;5;241m.\u001b[39mmarkdown(\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou can preview the judgments returned by your search terms on CommonLII after you have entered some search terms.\u001b[39m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124mYou may have to unblock a popped up window, refresh this page, and re-enter your search terms.\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1066\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1064\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(com\u001b[38;5;241m.\u001b[39mapply_if_callable(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m-> 1066\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple(key)\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1069\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3917\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   3914\u001b[0m     series \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ixs(col, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[index]\n\u001b[0;32m-> 3917\u001b[0m series \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_item_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3918\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_engine\n\u001b[1;32m   3920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[1;32m   3921\u001b[0m     \u001b[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[1;32m   3922\u001b[0m     \u001b[38;5;66;03m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[1;32m   3923\u001b[0m     \u001b[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:4282\u001b[0m, in \u001b[0;36mDataFrame._get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   4277\u001b[0m res \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(item)\n\u001b[1;32m   4278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4279\u001b[0m     \u001b[38;5;66;03m# All places that call _get_item_cache have unique columns,\u001b[39;00m\n\u001b[1;32m   4280\u001b[0m     \u001b[38;5;66;03m#  pending resolution of GH#33047\u001b[39;00m\n\u001b[0;32m-> 4282\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4283\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ixs(loc, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   4285\u001b[0m     cache[item] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Find (method)'"
     ]
    }
   ],
   "source": [
    "if st.session_state.page_from != \"pages/ER.py\": #Need to add in order to avoid GPT page from showing form of previous page\n",
    "    \n",
    "    #Create form\n",
    "    \n",
    "    return_button = st.button('RETURN to first page')\n",
    "    \n",
    "    st.header(f\"You have selected to study :blue[the English Reports].\")\n",
    "    \n",
    "    #Search terms\n",
    "    \n",
    "    #    st.header(\"Judgment Search Criteria\")\n",
    "    \n",
    "    st.markdown(\"\"\"**:green[Please enter your search terms.]** This program will collect (ie scrape) the first 10 judgments returned by your search terms.\n",
    "\"\"\")\n",
    "    \n",
    "    st.caption('During the pilot stage, the number of judgments to scrape is capped. Please reach out to Ben Chen at ben.chen@sydney.edu.au should you wish to cover more judgments.')\n",
    "\n",
    "    reset_button = st.button(label='RESET', type = 'primary')\n",
    "\n",
    "    st.subheader(\"Your search terms\")\n",
    "    \n",
    "    st.markdown(\"\"\"For search tips, please visit [CommonLII](http://www.commonlii.org/form/search1.html?mask=uk/cases/EngR). This section mimics their search function.\n",
    "\"\"\")\n",
    "    \n",
    "    method_entry = st.selectbox(label = 'Find', options = er_methods_list, index= er_methods_list.index(st.session_state.df_master.loc[0, 'Find (method)']))\n",
    "    \n",
    "    query_entry = st.text_input(label = 'Enter search query', value = st.session_state.df_master.loc[0, 'Enter search query'])\n",
    "        \n",
    "    st.markdown(\"\"\"You can preview the judgments returned by your search terms on CommonLII after you have entered some search terms.\n",
    "\n",
    "You may have to unblock a popped up window, refresh this page, and re-enter your search terms.\n",
    "\"\"\")\n",
    "    \n",
    "    preview_button = st.button(label = 'PREVIEW on CommonLII (in a popped up window)', type = 'primary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a445523-7f0c-412c-8b11-873d27732fad",
   "metadata": {},
   "source": [
    "## Buttons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5428b07e-fe2e-4854-88a1-d216fd143912",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    #Buttons\n",
    "    \n",
    "    #col1, col2, col3, col4 = st.columns(4, gap = 'small')\n",
    "    \n",
    "    #with col1:\n",
    "    \n",
    "        #reset_button = st.button(label='RESET', type = 'primary')\n",
    "    \n",
    "    #with col4:\n",
    "    with stylable_container(\n",
    "        \"green\",\n",
    "        css_styles=\"\"\"\n",
    "        button {\n",
    "            background-color: #00FF00;\n",
    "            color: black;\n",
    "        }\"\"\",\n",
    "    ):\n",
    "        next_button = st.button(label='NEXT')\n",
    "    \n",
    "    keep_button = st.button('SAVE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f71a3",
   "metadata": {},
   "source": [
    "# Save and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81d89e4a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preview_button' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mpreview_button\u001b[49m:\n\u001b[1;32m      3\u001b[0m     df_master \u001b[38;5;241m=\u001b[39m er_create_df()\n\u001b[1;32m      5\u001b[0m     judgments_url \u001b[38;5;241m=\u001b[39m er_search_url(df_master)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preview_button' is not defined"
     ]
    }
   ],
   "source": [
    "    if preview_button:\n",
    "        \n",
    "        df_master = er_create_df()\n",
    "    \n",
    "        judgments_url = er_search_url(df_master)\n",
    "    \n",
    "        open_page(judgments_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4873fffb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "    if keep_button:\n",
    "    \n",
    "        all_search_terms = str(query_entry)\n",
    "            \n",
    "        if all_search_terms.replace('None', '') == \"\":\n",
    "    \n",
    "            st.warning('You must enter some search terms.')\n",
    "    \n",
    "        else:\n",
    "                                \n",
    "            df_master = er_create_df()\n",
    "    \n",
    "            save_input(df_master)\n",
    "        \n",
    "            responses_output_name = str(df_master.loc[0, 'Your name']) + '_' + str(today_in_nums) + '_responses'\n",
    "        \n",
    "            #Produce a file to download\n",
    "        \n",
    "            csv = convert_df_to_csv(df_master)\n",
    "            \n",
    "            ste.download_button(\n",
    "                label=\"Download as a CSV (for use in Excel etc)\", \n",
    "                data = csv,\n",
    "                file_name=responses_output_name + '.csv', \n",
    "                mime= \"text/csv\", \n",
    "        #            key='download-csv'\n",
    "            )\n",
    "    \n",
    "            xlsx = convert_df_to_excel(df_master)\n",
    "            \n",
    "            ste.download_button(label='Download as an Excel spreadsheet (XLSX)',\n",
    "                                data=xlsx,\n",
    "                                file_name=responses_output_name + '.xlsx', \n",
    "                                mime='application/vnd.ms-excel',\n",
    "                               )\n",
    "            \n",
    "            json = convert_df_to_json(df_master)\n",
    "            \n",
    "            ste.download_button(\n",
    "                label=\"Download as a JSON\", \n",
    "                data = json,\n",
    "                file_name= responses_output_name + '.json', \n",
    "                mime= \"application/json\", \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "995962d0-2a30-4327-9a0e-55b2f3e27b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if return_button:\n",
    "\n",
    "        df_master = er_create_df()\n",
    "\n",
    "        save_input(df_master)\n",
    "\n",
    "        st.session_state[\"page_from\"] = 'pages/ER.py'\n",
    "    \n",
    "        st.switch_page(\"Home.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59ab063d-6b65-41a1-b8b1-1ba2f18f6fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if reset_button:\n",
    "        st.session_state.pop('df_master')\n",
    "\n",
    "        #clear_cache()\n",
    "        st.rerun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ada792d4-736f-448f-b06d-6d51a47be627",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if next_button:\n",
    "    \n",
    "        all_search_terms = str(query_entry)\n",
    "        \n",
    "        if all_search_terms.replace('None', '') == \"\":\n",
    "    \n",
    "            st.warning('You must enter some search terms.')\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            df_master = er_create_df()\n",
    "    \n",
    "            save_input(df_master)\n",
    "                        \n",
    "            st.session_state[\"page_from\"] = 'pages/ER.py'\n",
    "            \n",
    "            st.switch_page('pages/GPT.py')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
