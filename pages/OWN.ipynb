{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b8b879",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2312235",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Preliminary modules\n",
    "import base64 \n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "from dateutil.relativedelta import *\n",
    "from datetime import timedelta\n",
    "import sys\n",
    "import pause\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "from math import ceil\n",
    "\n",
    "#Conversion to text\n",
    "import fitz\n",
    "#from io import StringIO\n",
    "from io import BytesIO\n",
    "import pdf2image\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import mammoth\n",
    "\n",
    "#Streamlit\n",
    "import streamlit as st\n",
    "from streamlit_gsheets import GSheetsConnection\n",
    "from streamlit.components.v1 import html\n",
    "import streamlit_ext as ste\n",
    "\n",
    "#OpenAI\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "#Google\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "#Excel\n",
    "from pyxlsb import open_workbook as open_xlsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "651ed485-489f-4c0c-9412-06d15e64dfe0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By default, users are allowed to use their own account\n"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from common_functions import own_account_allowed, convert_df_to_json, convert_df_to_csv, convert_df_to_excel, mnc_cleaner \n",
    "#Import variables\n",
    "from common_functions import today_in_nums, errors_list, scraper_pause_mean\n",
    "\n",
    "if own_account_allowed() > 0:\n",
    "    print(f'By default, users are allowed to use their own account')\n",
    "else:\n",
    "    print(f'By default, users are NOT allowed to use their own account')\n",
    "\n",
    "print(f\"The pause between judgment scraping is {scraper_pause_mean} second.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c94f73e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Title of webpage\n",
    "st.set_page_config(\n",
    "   page_title=\"Empirical Legal Research Kickstarter\",\n",
    "   page_icon=\"ðŸ§Š\",\n",
    "   layout=\"centered\",\n",
    "   initial_sidebar_state=\"collapsed\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e819e27",
   "metadata": {},
   "source": [
    "# Functions for Own Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f841231-e0ad-4647-b5c2-244594a4cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create dataframe\n",
    "#@st.cache_data\n",
    "def create_df():\n",
    "\n",
    "    #submission time\n",
    "    timestamp = datetime.now()\n",
    "\n",
    "        #Personal info entries\n",
    "\n",
    "    name = ''\n",
    "    \n",
    "    email = ''\n",
    "\n",
    "    gpt_api_key = ''\n",
    "\n",
    "    try:\n",
    "        name = name_entry\n",
    "    except:\n",
    "        print('Name not entered')\n",
    "    \n",
    "    try:\n",
    "        email = email_entry\n",
    "    except:\n",
    "        print('Email not entered')\n",
    "\n",
    "    try:\n",
    "        gpt_api_key = gpt_api_key_entry\n",
    "        #This is the user's entered API key whether valid or invalid, not necessarily the one used to produce outputs\n",
    "    except:\n",
    "        print('API key not entered')\n",
    "\n",
    "    #Own account status\n",
    "    own_account = st.session_state.own_account\n",
    "    \n",
    "    #file counter bound\n",
    "    file_counter_bound = st.session_state.file_counter_bound\n",
    "\n",
    "    #Page counter bound\n",
    "\n",
    "    page_bound = st.session_state.page_bound\n",
    "    \n",
    "    #GPT enhancement\n",
    "    gpt_enhancement = st.session_state.gpt_enhancement_entry\n",
    "\n",
    "    #GPT choice and entry\n",
    "    gpt_activation_status = gpt_activation_entry\n",
    "        \n",
    "    gpt_questions = gpt_questions_entry[0: question_characters_bound]\n",
    "\n",
    "    #Get uploaded file names\n",
    "\n",
    "    file_names_list = []\n",
    "\n",
    "    for uploaded_doc in uploaded_docs:\n",
    "        file_names_list.append(uploaded_doc.name)\n",
    "\n",
    "    for uploaded_image in uploaded_images:\n",
    "        file_names_list.append(uploaded_image.name)\n",
    "\n",
    "    #Language choice\n",
    "\n",
    "    language = language_entry\n",
    "    \n",
    "    new_row = {'Processed': '',\n",
    "           'Timestamp': timestamp,\n",
    "           'Your name': name, \n",
    "           'Your email address': email, \n",
    "           'Your GPT API key': gpt_api_key, \n",
    "            'Your uploaded files' : str(file_names_list), \n",
    "           'Language choice': language, \n",
    "           'Maximum number of files': file_counter_bound, \n",
    "          'Maximum number of pages per file': page_bound, \n",
    "            'Use GPT': gpt_activation_status, \n",
    "           'Enter your questions for GPT': gpt_questions, \n",
    "            'Use own account': own_account,\n",
    "            'Use flagship version of GPT': gpt_enhancement\n",
    "          }\n",
    "\n",
    "    df_master_new = pd.DataFrame(new_row, index = [0])\n",
    "    \n",
    "#    df_master_new.to_json(current_dir + '/df_master.json', orient = 'split', compression = 'infer')\n",
    "#    df_master_new.to_excel(current_dir + '/df_master.xlsx', index=False)\n",
    "\n",
    "#    if len(df_master_new) > 0:\n",
    "        \n",
    "    return df_master_new\n",
    "\n",
    "#    else:\n",
    "#        return 'Error: spreadsheet of reponses NOT generated.' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e330af89-11fb-42ec-88fb-ef40580028eb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#File types and languages for processing\n",
    "doc_types = [\"pdf\", \"txt\", 'docx', \"xps\", \"epub\", \"mobi\", 'cs', 'xml', 'json'] #\"fb2\", \"cbz\", \"svg\",\n",
    "image_types = [\"pdf\", \"jpg\", \"jpeg\", \"png\", \"bmp\", \"gif\", \"tiff\"] #, \"pnm\", \"pgm\", \"pbm\", \"ppm\", \"pam\", \"jxr\", \"jpx\", \"jp2\", \"psd\"]\n",
    "languages_dict = {'English': 'eng', \n",
    "                  'English, Middle (1100-1500)': 'enm', \n",
    "                  'Chinese - Simplified': 'chi_sim', \n",
    "                  'Chinese - Traditional': 'chi_tra', \n",
    "                  'French': 'fra', \n",
    "                  'German' : 'deu',\n",
    "                  'Greek, Modern (1453-)': 'ell', \n",
    "                  'Greek, Ancient (-1453)': 'grc', \n",
    "                  'Hebrew' : 'heb', \n",
    "                  'Hindi' : 'hin', \n",
    "                  'Hungarian': 'hun', \n",
    "                  'Indonesian': 'ind', \n",
    "                  'Italian': 'ita', \n",
    "                  'Italian - Old': 'ita_old', \n",
    "                  'Japanese': 'jpn', \n",
    "                  'Korean': 'kor', \n",
    "                  'Malay': 'msa', \n",
    "                  'Panjabi; Punjabi': 'pan', \n",
    "                  'Polish': 'pol', \n",
    "                  'Portuguese': 'por', \n",
    "                  'Russian': 'rus', \n",
    "                  'Spanish; Castilian': 'spa', \n",
    "                  'Spanish; Castilian - Old': 'spa_old', \n",
    "                  'Swedish': 'swe', \n",
    "                  'Thai': 'tha', \n",
    "                  'Turkish': 'tur', \n",
    "                  'Uighur; Uyghur': 'uig', \n",
    "                  'Ukrainian': 'ukr', \n",
    "                  'Vietnamese': 'vie', \n",
    "                  'Yiddish': 'yid'\n",
    "                 }\n",
    "languages_list = list(languages_dict.keys())\n",
    "\n",
    "#languages_words = ', '.join(languages_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "035a5c8b-f936-4100-9ddf-b7547e4c5d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define format functions for GPT questions    \n",
    "\n",
    "#Create function to split a string into a list by line\n",
    "def split_by_line(x):\n",
    "    y = x.split('\\n')\n",
    "    for i in y:\n",
    "        if len(i) == 0:\n",
    "            y.remove(i)\n",
    "    return y\n",
    "\n",
    "#Create function to split a list into a dictionary for list items longer than 10 characters\n",
    "\n",
    "#Apply split_by_line() before the following function\n",
    "def GPT_label_dict(x_list):\n",
    "    GPT_dict = {}\n",
    "    for i in x_list:\n",
    "        if len(i) > 10:\n",
    "            GPT_index = x_list.index(i) + 1\n",
    "            i_label = 'GPT question ' + f'{GPT_index}'\n",
    "            GPT_dict.update({i_label: i})\n",
    "    return GPT_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b4492d7-d779-44f2-bf47-aafce3baa108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert each uploaded file to file name, text\n",
    "#@st.cache_data\n",
    "def doc_to_text(uploaded_doc, language, page_bound):\n",
    "    file_triple = {'File name' : '', 'Language choice': language, 'Page length': '', 'Extracted text': '', \n",
    "#                  'Page 2': '' #Test page\n",
    "                  }\n",
    "    \n",
    "    #Get file name\n",
    "    file_triple['File name']=uploaded_doc.name\n",
    "    \n",
    "    #Get file data\n",
    "    bytes_data = uploaded_doc.getvalue()\n",
    "\n",
    "    #Get file extension\n",
    "    extension = file_triple['File name'].split('.')[-1].lower()\n",
    "\n",
    "    #Create list of pages\n",
    "    text_list = []\n",
    "\n",
    "    #Word format\n",
    "    if extension == 'docx':\n",
    "        doc_string = mammoth.convert_to_html(BytesIO(bytes_data)).value\n",
    "        text_list.append(doc_string)\n",
    "\n",
    "        file_triple['Page length'] = 1\n",
    "        \n",
    "    else:\n",
    "        #text formats\n",
    "        if extension in ['txt', 'cs', 'xml', 'json']:\n",
    "            doc = fitz.open(stream=bytes_data, filetype=\"txt\")\n",
    "\n",
    "        #Other formats\n",
    "        else:\n",
    "            doc = fitz.open(stream=bytes_data)\n",
    "\n",
    "        max_doc_number=min(len(doc), page_bound)\n",
    "        \n",
    "        for page_index in list(range(0, max_doc_number)):\n",
    "            page = doc.load_page(page_index)\n",
    "            text_page = page.get_text() \n",
    "            text_list.append(text_page)\n",
    "\n",
    "        #Length of pages\n",
    "        file_triple['Page length'] = len(doc)\n",
    "\n",
    "    file_triple['Extracted text'] = str(text_list)\n",
    "\n",
    "    #Test page\n",
    "#    file_triple['Page 2'] = doc.load_page(1).get_text()\n",
    "    \n",
    "    return file_triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9087b68-2c2b-4240-b04a-a02bf3580a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for images to text\n",
    "#@st.cache_data\n",
    "def image_to_text(uploaded_image, language, page_bound):\n",
    "    file_triple = {'File name' : '', 'Language choice': language, 'Page length': '', 'Extracted text': '', \n",
    "#                  'Page 2': '' #Test page\n",
    "                  }\n",
    "\n",
    "    #Get file name\n",
    "    file_triple['File name']=uploaded_image.name\n",
    "\n",
    "    #Get file data\n",
    "    bytes_data = uploaded_image.read()\n",
    "\n",
    "    #Get file extension\n",
    "    extension = file_triple['File name'].split('.')[-1].lower()\n",
    "\n",
    "    #Obtain images from uploaded file\n",
    "    if extension == 'pdf':\n",
    "        try:\n",
    "            images = pdf2image.convert_from_bytes(bytes_data, timeout=30)\n",
    "        except PDFPopplerTimeoutError as pdf2image_timeout_error:\n",
    "            print(f\"pdf2image error: {pdf2image_timeout_error}.\")\n",
    "\n",
    "    else:\n",
    "        images = []\n",
    "        image_raw = Image.open(BytesIO(bytes_data))\n",
    "        images.append(image_raw)\n",
    "        \n",
    "    #Extract text from images\n",
    "    text_list = []\n",
    "    \n",
    "    max_images_number=min(len(images), page_bound)\n",
    "\n",
    "    for image in images[ : max_images_number]:\n",
    "        try:\n",
    "            text_page = pytesseract.image_to_string(image, lang=languages_dict[language], timeout=30)\n",
    "            text_list.append(text_page)\n",
    "            \n",
    "        except RuntimeError as pytesseract_timeout_error:\n",
    "            print(f\"pytesseract error: {pytesseract_timeout_error}.\")\n",
    "\n",
    "    file_triple['Extracted text'] = str(text_list)\n",
    "\n",
    "    #Length of pages\n",
    "    file_triple['Page length'] = len(images)\n",
    "\n",
    "    #Test page\n",
    "#    file_triple['Page 2'] = pytesseract.image_to_string(images[1], lang=languages_dict[language], timeout=30)\n",
    "        \n",
    "    return file_triple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a2d594",
   "metadata": {},
   "source": [
    "# GPT functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bfae1df-142c-4583-ab1f-fd94500499b1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gpt_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Import functions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_by_line, GPT_label_dict, is_api_key_valid, gpt_input_cost, gpt_output_cost, tokens_cap, num_tokens_from_string  \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Import variables\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m question_characters_bound, default_judgment_counter_bound\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gpt_functions'"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from gpt_functions import split_by_line, GPT_label_dict, is_api_key_valid, gpt_input_cost, gpt_output_cost, tokens_cap, max_output, num_tokens_from_string  \n",
    "#Import variables\n",
    "from gpt_functions import question_characters_bound, default_judgment_counter_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab06b309-e9be-495e-96fd-1073b25291c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Questions for GPT are capped at {question_characters_bound} characters.\\n\")\n",
    "print(f\"The default number of judgments to scrape per request is capped at {default_judgment_counter_bound}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e8eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize default GPT settings\n",
    "\n",
    "if 'gpt_model' not in st.session_state:\n",
    "    st.session_state['gpt_model'] = \"gpt-4o-mini\"\n",
    "    \n",
    "#Initialize API key\n",
    "if 'gpt_api_key' not in st.session_state:\n",
    "\n",
    "    st.session_state['gpt_api_key'] = st.secrets[\"openai\"][\"gpt_api_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef483929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_prompt(file_triple, gpt_model):\n",
    "                \n",
    "    file_content = 'Based on the following document:  \"\"\"'+ file_triple['Extracted text'] + '\"\"\"'\n",
    "\n",
    "    file_content_tokens = num_tokens_from_string(file_content, \"cl100k_base\")\n",
    "    \n",
    "    if file_content_tokens <= tokens_cap(gpt_model):\n",
    "        \n",
    "        return file_content\n",
    "\n",
    "    else:\n",
    "                \n",
    "        file_chars_capped = int(tokens_cap(gpt_model)*4)\n",
    "        \n",
    "        #Keep first x characters rather than cut out the middle\n",
    "        file_string_trimmed = file_triple['Extracted text'][ : int(file_chars_capped)]\n",
    "\n",
    "        #If want to cut out the middle instead\n",
    "#        file_string_trimmed = file_triple['Extracted text'][ :int(file_chars_capped/2)] + file_triple['Extracted text'][-int(file_chars_capped/2): ]\n",
    "        \n",
    "        file_content_capped = 'Based on the following document:  \"\"\"'+ file_string_trimmed + '\"\"\"'\n",
    "        \n",
    "        return file_content_capped\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f7af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define system role content for GPT\n",
    "role_content_own = 'You are a legal research assistant helping an academic researcher to answer questions about a file. The file may be a document or an image. You will be provided with the file. Please answer questions based only on information contained in the file. Where your answer comes from specific pages, paragraphs or sections of the file, provide the page or pagraph numbers, or section names as part of your answer. If you cannot answer the questions based on the file, do not make up information, but instead write \"answer not found\".'\n",
    "\n",
    "system_instruction = role_content_own\n",
    "\n",
    "intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d95be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT answer function for answers in json form, YES TOKENS\n",
    "#IN USE\n",
    "\n",
    "def GPT_json_own(questions_json, file_triple, gpt_model, system_instruction):\n",
    "    #'question_json' variable is a json of questions to GPT\n",
    "\n",
    "    file_for_GPT = [{\"role\": \"user\", \"content\": file_prompt(file_triple, gpt_model)}]\n",
    "\n",
    "    json_direction = [{\"role\": \"user\", \"content\": 'You will be given questions to answer in JSON form.'}]\n",
    "\n",
    "    #Create answer format\n",
    "    \n",
    "    q_keys = [*questions_json]\n",
    "    \n",
    "    answers_json = {}\n",
    "    \n",
    "    for q_index in q_keys:\n",
    "        answers_json.update({q_index: 'Your answer to the question with index ' + q_index + '. State specific page numbers or sections of the file.'})\n",
    "    \n",
    "    #Create questions, which include the answer format\n",
    "    \n",
    "    question_for_GPT = [{\"role\": \"user\", \"content\": json.dumps(questions_json) + ' Give responses in the following JSON form: ' + json.dumps(answers_json)}]\n",
    "    \n",
    "    #Create messages in one prompt for GPT\n",
    "    language_content = f\"The file is written in {file_triple['Language choice']}.\"\n",
    "\n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction + language_content}] \n",
    "\n",
    "    messages_for_GPT = intro_for_GPT + file_for_GPT + json_direction + question_for_GPT\n",
    "    \n",
    "#   return messages_for_GPT\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    try:\n",
    "        #completion = client.chat.completions.create(\n",
    "        completion = openai.chat.completions.create(\n",
    "            model=gpt_model,\n",
    "            messages=messages_for_GPT, \n",
    "            response_format={\"type\": \"json_object\"}, \n",
    "            max_tokens = max_output(gpt_model, messages_for_GPT), \n",
    "            temperature = 0.2, \n",
    "            top_p = 0.2\n",
    "        )\n",
    "        \n",
    "#        return completion.choices[0].message.content #This gives answers as a string containing a dictionary\n",
    "        \n",
    "        #To obtain a json directly, use below\n",
    "        answers_dict = json.loads(completion.choices[0].message.content)\n",
    "        \n",
    "        #Obtain tokens\n",
    "        output_tokens = completion.usage.completion_tokens\n",
    "        \n",
    "        prompt_tokens = completion.usage.prompt_tokens\n",
    "        \n",
    "        return [answers_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "    except Exception as error:\n",
    "        \n",
    "        for q_index in q_keys:\n",
    "            answers_json[q_index] = error\n",
    "        \n",
    "        return [answers_json, 0, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80714830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT function for each respondent's dataframe, index by file then question, with input and output tokens given by GPT itself\n",
    "#IN USE\n",
    "\n",
    "#The following function DOES NOT check for existence of questions for GPT\n",
    "    # To so check, active line marked as #*\n",
    "def engage_GPT_json_own(questions_json, df_individual, GPT_activation, gpt_model, system_instruction):\n",
    "    # Variable questions_json refers to the json of questions\n",
    "    # Variable df_individual refers to each respondent's df\n",
    "    # Variable activation refers to status of GPT activation (real or test)\n",
    "    # The output is a new JSON for the relevant respondent with new columns re:\n",
    "        # f\"Length of first 10 pages in tokens (up to {tokens_cap(gpt_model)} given to GPT)\"\n",
    "        # 'GPT cost estimate (USD excl GST)'\n",
    "        # 'GPT time estimate (seconds)'\n",
    "        # GPT questions/answers\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    question_keys = [*questions_json]\n",
    "    \n",
    "    for file_index in df_individual.index:\n",
    "        \n",
    "        file_triple = df_individual.to_dict('index')[file_index]\n",
    "        \n",
    "        #Calculate and append number of tokens of file, regardless of whether given to GPT\n",
    "        file_tokens = num_tokens_from_string(str(file_triple), \"cl100k_base\")\n",
    "        df_individual.loc[file_index, f\"Length of first {st.session_state.page_bound} pages in tokens (up to {tokens_cap(gpt_model)} given to GPT)\"] = file_tokens       \n",
    "\n",
    "        #Indicate whether file truncated\n",
    "        \n",
    "        df_individual.loc[file_index, \"File truncated (if given to GPT)?\"] = ''       \n",
    "        \n",
    "        if file_tokens <= tokens_cap(gpt_model):\n",
    "            \n",
    "            df_individual.loc[file_index, \"File truncated (if given to GPT)?\"] = 'No'\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            df_individual.loc[file_index, \"File truncated (if given to GPT)?\"] = 'Yes'\n",
    "\n",
    "        #Create columns for respondent's GPT cost, time\n",
    "        df_individual.loc[file_index, 'GPT cost estimate (USD excl GST)'] = ''\n",
    "        df_individual.loc[file_index, 'GPT time estimate (seconds)'] = ''\n",
    "                \n",
    "        #Calculate GPT start time\n",
    "\n",
    "        GPT_start_time = datetime.now()\n",
    "\n",
    "        #Depending on activation status, apply GPT_json function to each file, gives answers as a string containing a dictionary\n",
    "\n",
    "        if int(GPT_activation) > 0:\n",
    "            GPT_file_triple = GPT_json_own(questions_json, file_triple, gpt_model) #Gives [answers as a JSON, output tokens, input tokens]\n",
    "            answers_dict = GPT_file_triple[0]\n",
    "\n",
    "            #Calculate and append GPT finish time and time difference to individual df\n",
    "            GPT_finish_time = datetime.now()\n",
    "            \n",
    "            GPT_time_difference = GPT_finish_time - GPT_start_time\n",
    "    \n",
    "            df_individual.loc[file_index, 'GPT time estimate (seconds)'] = GPT_time_difference.total_seconds()\n",
    "\n",
    "        else:\n",
    "            answers_dict = {}    \n",
    "            for q_index in question_keys:\n",
    "                #Increases file index by 2 to ensure consistency with Excel spreadsheet\n",
    "                answer = 'Placeholder answer for ' + ' file ' + str(int(file_index) + 2) + ' ' + str(q_index)\n",
    "                answers_dict.update({q_index: answer})\n",
    "            \n",
    "            #Own calculation of GPT costs for Placeholder answer fors\n",
    "\n",
    "            #Calculate capped file tokens\n",
    "\n",
    "            file_capped_tokens = num_tokens_from_string(file_prompt(file_triple, gpt_model), \"cl100k_base\")\n",
    "\n",
    "            #Calculate questions tokens and cost\n",
    "\n",
    "            questions_tokens = num_tokens_from_string(json.dumps(questions_json), \"cl100k_base\")\n",
    "\n",
    "            #Calculate other instructions' tokens\n",
    "\n",
    "            other_instructions = system_instruction + 'The file is written in some language' + 'you will be given questions to answer in JSON form.' + ' Give responses in the following JSON form: '\n",
    "\n",
    "            other_tokens = num_tokens_from_string(other_instructions, \"cl100k_base\") + len(question_keys)*num_tokens_from_string(\"GPT question x:  Your answer to the question with index GPT question x. State specific page numbers or sections of the file.\", \"cl100k_base\")\n",
    "\n",
    "            #Calculate number of tokens of answers\n",
    "            answers_tokens = num_tokens_from_string(str(answers_dict), \"cl100k_base\")\n",
    "\n",
    "            input_tokens = file_capped_tokens + questions_tokens + other_tokens\n",
    "            \n",
    "            GPT_file_triple = [answers_dict, answers_tokens, input_tokens]\n",
    "\n",
    "        #Create GPT question headings and append answers to individual spreadsheets\n",
    "\n",
    "        for question_index in question_keys:\n",
    "            question_heading = question_index + ': ' + questions_json[question_index]\n",
    "            df_individual.loc[file_index, question_heading] = answers_dict[question_index]\n",
    "\n",
    "        #Calculate GPT costs\n",
    "\n",
    "        GPT_cost = GPT_file_triple[1]*gpt_output_cost(gpt_model) + GPT_file_triple[2]*gpt_input_cost(gpt_model)\n",
    "\n",
    "        #Calculate and append GPT cost to individual df\n",
    "        df_individual.loc[file_index, 'GPT cost estimate (USD excl GST)'] = GPT_cost\n",
    "    \n",
    "    return df_individual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83981e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain parameters\n",
    "\n",
    "def run(df_master, uploaded_docs, uploaded_images):\n",
    "    df_master = df_master.fillna('')\n",
    "\n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    df_master['Enter your questions for GPT'] = df_master['Enter your questions for GPT'][0: question_characters_bound].apply(split_by_line)\n",
    "    df_master['questions_json'] = df_master['Enter your questions for GPT'].apply(GPT_label_dict)\n",
    "    \n",
    "    #Create files file\n",
    "    Files_file = []\n",
    "\n",
    "    #Obtain bounds and language\n",
    "\n",
    "    file_counter_bound = int(df_master.loc[0, 'Maximum number of files'])\n",
    "\n",
    "    page_bound = int(df_master.loc[0,'Maximum number of pages per file'])\n",
    "\n",
    "    language = df_master.loc[0, 'Language choice']\n",
    "    \n",
    "    #Convert uploaded documents to text\n",
    "\n",
    "    file_counter = 1 \n",
    "\n",
    "    for uploaded_doc in uploaded_docs:\n",
    "        if file_counter <= file_counter_bound:\n",
    "            file_triple = doc_to_text(uploaded_doc, language, page_bound)\n",
    "            Files_file.append(file_triple)\n",
    "            file_counter += 1\n",
    "\n",
    "    #Convert uploaded images to text\n",
    "\n",
    "    for uploaded_image in uploaded_images:\n",
    "        if file_counter <= file_counter_bound:\n",
    "            file_triple = image_to_text(uploaded_image, language, page_bound)\n",
    "            Files_file.append(file_triple)\n",
    "            file_counter += 1\n",
    "    \n",
    "    #Create and export json file with search results\n",
    "    json_individual = json.dumps(Files_file, indent=2)\n",
    "    \n",
    "    df_individual = pd.read_json(json_individual)\n",
    "    \n",
    "    #GPT model\n",
    "\n",
    "    #GPT model\n",
    "\n",
    "    if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "        gpt_model = \"gpt-4o\"\n",
    "    else:        \n",
    "        gpt_model = \"gpt-4o-mini\"\n",
    "        \n",
    "    #apply GPT_individual to each respondent's judgment spreadsheet\n",
    "    \n",
    "    GPT_activation = int(df_master.loc[0, 'Use GPT'])\n",
    "\n",
    "    questions_json = df_master.loc[0, 'questions_json']\n",
    "        \n",
    "    #Engage GPT\n",
    "    df_updated = engage_GPT_json_own(questions_json, df_individual, GPT_activation, gpt_model, system_instruction)\n",
    "\n",
    "    try:\n",
    "        df_updated.pop('Extracted text')\n",
    "    except:\n",
    "        print(\"No 'Extracted text' columnn.\")\n",
    "    \n",
    "    return df_updated\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caa7ba6-7f30-40a5-a98d-16a956d1f5f6",
   "metadata": {},
   "source": [
    "# For gpt-4o vision, own file only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb2c4cf-70cf-48cb-856b-1df1b044be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import functions\n",
    "from gpt_functions import get_image_dims, calculate_image_token_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed9ee2-0dea-4f6a-840b-bb5ed83eca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def to_base64(uploaded_file):\n",
    "    #file_buffer = uploaded_file.read()\n",
    "    #b64 = base64.b64encode(file_buffer).decode()\n",
    "    #return f\"data:image/png;base64,{b64}\"\n",
    "\n",
    "#def to_base64(uploaded_file):\n",
    "    #file_buffer = uploaded_file.read()\n",
    "    #b64 = base64.b64encode(file_buffer).decode('utf-8')\n",
    "    #return b64\n",
    "\n",
    "def image_to_b64_own(uploaded_image, language, page_bound):\n",
    "    file_triple = {'File name' : '', 'Language choice': language, 'b64_list': [], 'Dimensions (width, height)' : [],\n",
    "                   'Page length': '', 'tokens_raw': 0, \n",
    "#                 'Image ID': '', 'Page length': '', 'Page 2': '' #Test page\n",
    "                  }\n",
    "\n",
    "    file_triple['File name']=uploaded_image.name\n",
    "\n",
    "    #Get file extension\n",
    "    extension = file_triple['File name'].split('.')[-1].lower()\n",
    "\n",
    "    bytes_data = uploaded_image.read()\n",
    "\n",
    "    if extension == 'pdf':\n",
    "        \n",
    "        images = pdf2image.convert_from_bytes(bytes_data, timeout=30, fmt=\"jpeg\")\n",
    "\n",
    "        file_triple['Page length'] = len(images)\n",
    "\n",
    "        #Get page bound\n",
    "        max_images_number=min(len(images), page_bound)\n",
    "\n",
    "        for image in images[ : max_images_number]:\n",
    "\n",
    "            output = BytesIO()\n",
    "            image.save(output, format='JPEG')\n",
    "            im_data = output.getvalue()\n",
    "            \n",
    "            image_data = base64.b64encode(im_data)\n",
    "            if not isinstance(image_data, str):\n",
    "                # Python 3, decode from bytes to string\n",
    "                image_data = image_data.decode()\n",
    "            data_url = 'data:image/jpg;base64,' + image_data\n",
    "\n",
    "            #b64 = base64.b64encode(image_raw).decode('utf-8')\n",
    "\n",
    "            b64_to_attach = data_url\n",
    "            #b64_to_attach = f\"data:image/png;base64,{b64}\"\n",
    "\n",
    "        file_triple['b64_list'].append(b64_to_attach)\n",
    "            \n",
    "        #except PDFPopplerTimeoutError as pdf2image_timeout_error:\n",
    "            #print(f\"pdf2image error: {pdf2image_timeout_error}.\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        file_triple['Page length'] = 1\n",
    "    \n",
    "        b64 = base64.b64encode(bytes_data).decode('utf-8')\n",
    "    \n",
    "        b64_to_attach = f\"data:image/{extension};base64,{b64}\"\n",
    "        \n",
    "        #file_triple['b64_list'] = [b64_to_attach]\n",
    "        file_triple['b64_list'].append(b64_to_attach)\n",
    "        \n",
    "\n",
    "        #Get tokens\n",
    "    \n",
    "        #file_triple['tokens_raw'] = calculate_image_token_cost(b64_to_attach, detail=\"auto\")\n",
    "        \n",
    "    for image_b64 in file_triple['b64_list']:\n",
    "\n",
    "        #Get dimensions\n",
    "        try:\n",
    "\n",
    "            file_triple['Dimensions (width, height)'].append(get_image_dims(b64_to_attach))\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot obtain dimensions for {file_triple['File name']}, p {file_triple['b64_list'].index(image_b64)}.\")\n",
    "            print(e)\n",
    "        \n",
    "        file_triple['tokens_raw'] = file_triple['tokens_raw'] + calculate_image_token_cost(image_b64, detail=\"auto\")\n",
    "            \n",
    "    return file_triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2332d698-126d-4069-8ee3-286af5ab7be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT answer function for answers in json form, YES TOKENS\n",
    "#For gpt-4o vision\n",
    "\n",
    "def GPT_b64_json_own(questions_json, file_triple, gpt_model, system_instruction):\n",
    "    #'question_json' variable is a json of questions to GPT\n",
    "\n",
    "    #file_for_GPT = [{\"role\": \"user\", \"content\": file_prompt(file_triple, gpt_model) + 'you will be given questions to answer in JSON form.'}]\n",
    "\n",
    "    #Add images to messages to GPT\n",
    "    image_content_value = [{\"type\": \"text\", \"text\": 'Based on the following images:'}]\n",
    "\n",
    "    for image_b64 in file_triple['b64_list']:\n",
    "        image_message_to_attach = {\"type\": \"image_url\", \"image_url\": {\"url\": image_b64,}}\n",
    "        image_content_value.append(image_message_to_attach)\n",
    "\n",
    "    image_content = [{\"role\": \"user\", \n",
    "                      \"content\": image_content_value\n",
    "                     }\n",
    "                  ]\n",
    "\n",
    "    json_direction = [{\"role\": \"user\", \"content\": 'You will be given questions to answer in JSON form.'}]\n",
    "\n",
    "    file_for_GPT = image_content + json_direction\n",
    "    \n",
    "    #Create answer format\n",
    "    \n",
    "    q_keys = [*questions_json]\n",
    "    \n",
    "    answers_json = {}\n",
    "    \n",
    "    for q_index in q_keys:\n",
    "        answers_json.update({q_index: 'Your answer to the question with index ' + q_index + '. State specific page numbers or sections of the file.'})\n",
    "    \n",
    "    #Create questions, which include the answer format\n",
    "    \n",
    "    question_for_GPT = [{\"role\": \"user\", \"content\": json.dumps(questions_json) + ' Give responses in the following JSON form: ' + json.dumps(answers_json)}]\n",
    "    \n",
    "    #Create messages in one prompt for GPT\n",
    "    language_content = f\"The file is written in {file_triple['Language choice']}.\"\n",
    "\n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction + language_content}] \n",
    "\n",
    "    messages_for_GPT = intro_for_GPT + file_for_GPT + question_for_GPT\n",
    "    \n",
    "#   return messages_for_GPT\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    try:\n",
    "        #completion = client.chat.completions.create(\n",
    "        completion = openai.chat.completions.create(\n",
    "            model=gpt_model,\n",
    "            messages=messages_for_GPT, \n",
    "            response_format={\"type\": \"json_object\"}, \n",
    "            temperature = 0.2, \n",
    "            top_p = 0.2\n",
    "        )\n",
    "        \n",
    "#        return completion.choices[0].message.content #This gives answers as a string containing a dictionary\n",
    "        \n",
    "        #To obtain a json directly, use below\n",
    "        answers_dict = json.loads(completion.choices[0].message.content)\n",
    "        \n",
    "        #Obtain tokens\n",
    "        output_tokens = completion.usage.completion_tokens\n",
    "        \n",
    "        prompt_tokens = completion.usage.prompt_tokens\n",
    "        \n",
    "        return [answers_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "    except Exception as error:\n",
    "        \n",
    "        for q_index in q_keys:\n",
    "            answers_json[q_index] = error\n",
    "        \n",
    "        return [answers_json, 0, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d494f30b-28e6-40fd-bfcb-47e184e2d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT function for each respondent's dataframe, index by file then question, with input and output tokens given by GPT itself\n",
    "#For gpt-4o vision\n",
    "\n",
    "#The following function DOES NOT check for existence of questions for GPT\n",
    "    # To so check, active line marked as #*\n",
    "def engage_GPT_b64_json_own(questions_json, df_individual, GPT_activation, gpt_model, system_instruction):\n",
    "    # Variable questions_json refers to the json of questions\n",
    "    # Variable df_individual refers to each respondent's df\n",
    "    # Variable activation refers to status of GPT activation (real or test)\n",
    "    # The output is a new JSON for the relevant respondent with new columns re:\n",
    "        # f\"Length of first 10 pages in tokens (up to {tokens_cap(gpt_model)} given to GPT)\"\n",
    "        # 'GPT cost estimate (USD excl GST)'\n",
    "        # 'GPT time estimate (seconds)'\n",
    "        # GPT questions/answers\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    #openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    question_keys = [*questions_json]\n",
    "    \n",
    "    for file_index in df_individual.index:\n",
    "        \n",
    "        file_triple = df_individual.to_dict('index')[file_index]\n",
    "        \n",
    "        #Calculate and append number of tokens of file, regardless of whether given to GPT\n",
    "        #file_triple['tokens_raw'] = num_tokens_from_string(str(file_triple), \"cl100k_base\")\n",
    "        df_individual.loc[file_index, f\"Tokens (up to {tokens_cap(gpt_model)} given to GPT)\"] = file_triple['tokens_raw']       \n",
    "\n",
    "        #Indicate whether file truncated\n",
    "        \n",
    "        df_individual.loc[file_index, \"File truncated (if given to GPT)?\"] = ''       \n",
    "        \n",
    "        if file_triple['tokens_raw'] <= tokens_cap(gpt_model):\n",
    "            \n",
    "            df_individual.loc[file_index, \"File truncated (if given to GPT)?\"] = 'No'\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            df_individual.loc[file_index, \"File truncated (if given to GPT)?\"] = 'Yes'\n",
    "\n",
    "        #Create columns for respondent's GPT cost, time\n",
    "        df_individual.loc[file_index, 'GPT cost estimate (USD excl GST)'] = ''\n",
    "        df_individual.loc[file_index, 'GPT time estimate (seconds)'] = ''\n",
    "                \n",
    "        #Calculate GPT start time\n",
    "\n",
    "        GPT_start_time = datetime.now()\n",
    "\n",
    "        #Depending on activation status, apply GPT_json function to each file, gives answers as a string containing a dictionary\n",
    "\n",
    "        if int(GPT_activation) > 0:\n",
    "            GPT_file_triple = GPT_b64_json_own(questions_json, file_triple, gpt_model, system_instruction) #Gives [answers as a JSON, output tokens, input tokens]\n",
    "            answers_dict = GPT_file_triple[0]\n",
    "\n",
    "            #Calculate and append GPT finish time and time difference to individual df\n",
    "            GPT_finish_time = datetime.now()\n",
    "            \n",
    "            GPT_time_difference = GPT_finish_time - GPT_start_time\n",
    "    \n",
    "            df_individual.loc[file_index, 'GPT time estimate (seconds)'] = GPT_time_difference.total_seconds()\n",
    "        \n",
    "        else:\n",
    "            answers_dict = {}    \n",
    "            for q_index in question_keys:\n",
    "                #Increases file index by 2 to ensure consistency with Excel spreadsheet\n",
    "                answer = 'Placeholder answer for ' + ' file ' + str(int(file_index) + 2) + ' ' + str(q_index)\n",
    "                answers_dict.update({q_index: answer})\n",
    "            \n",
    "            #Own calculation of GPT costs for Placeholder answer fors\n",
    "\n",
    "            #Calculate capped file tokens\n",
    "\n",
    "            file_capped_tokens = min(file_triple['tokens_raw'], tokens_cap(gpt_model))\n",
    "\n",
    "            #Calculate questions tokens and cost\n",
    "\n",
    "            questions_tokens = num_tokens_from_string(json.dumps(questions_json), \"cl100k_base\")\n",
    "\n",
    "            #Calculate other instructions' tokens\n",
    "\n",
    "            other_instructions = system_instruction + 'The file is written in some language' + 'you will be given questions to answer in JSON form.' + ' Give responses in the following JSON form: '\n",
    "\n",
    "            other_tokens = num_tokens_from_string(other_instructions, \"cl100k_base\") + len(question_keys)*num_tokens_from_string(\"GPT question x:  Your answer to the question with index GPT question x. State specific page numbers or sections of the file.\", \"cl100k_base\")\n",
    "\n",
    "            #Calculate number of tokens of answers\n",
    "            answers_tokens = num_tokens_from_string(str(answers_dict), \"cl100k_base\")\n",
    "\n",
    "            input_tokens = file_capped_tokens + questions_tokens + other_tokens\n",
    "            \n",
    "            GPT_file_triple = [answers_dict, answers_tokens, input_tokens]\n",
    "\n",
    "        #Create GPT question headings and append answers to individual spreadsheets\n",
    "\n",
    "        for question_index in question_keys:\n",
    "            question_heading = question_index + ': ' + questions_json[question_index]\n",
    "            df_individual.loc[file_index, question_heading] = answers_dict[question_index]\n",
    "\n",
    "        #Calculate GPT costs\n",
    "\n",
    "        GPT_cost = GPT_file_triple[1]*gpt_output_cost(gpt_model) + GPT_file_triple[2]*gpt_input_cost(gpt_model)\n",
    "\n",
    "        #Calculate and append GPT cost to individual df\n",
    "        df_individual.loc[file_index, 'GPT cost estimate (USD excl GST)'] = GPT_cost\n",
    "    \n",
    "    return df_individual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3046c1e-5d6f-463a-92e4-84897573bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For gpt-4o vision\n",
    "\n",
    "def run_b64_own(df_master, uploaded_images):\n",
    "\n",
    "    df_master = df_master.fillna('')\n",
    "\n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    df_master['Enter your questions for GPT'] = df_master['Enter your questions for GPT'][0: question_characters_bound].apply(split_by_line)\n",
    "    df_master['questions_json'] = df_master['Enter your questions for GPT'].apply(GPT_label_dict)\n",
    "\n",
    "    #Obtain bounds and language\n",
    "\n",
    "    file_counter_bound = int(df_master.loc[0, 'Maximum number of files'])\n",
    "\n",
    "    page_bound = int(df_master.loc[0,'Maximum number of pages per file'])\n",
    "\n",
    "    language = df_master.loc[0, 'Language choice']\n",
    "    \n",
    "    #Convert uploaded documents to b64\n",
    "\n",
    "    file_counter = 1 \n",
    "    \n",
    "    #Create files file\n",
    "    Files_file = []\n",
    "\n",
    "    #Convert images to b64, then send to GPT\n",
    "    for uploaded_image in uploaded_images:\n",
    "        if file_counter <= file_counter_bound:\n",
    "            file_triple = image_to_b64_own(uploaded_image, language, page_bound)\n",
    "            Files_file.append(file_triple)\n",
    "            file_counter += 1\n",
    "\n",
    "    #Create and export json file with search results\n",
    "    json_individual = json.dumps(Files_file, indent=2)\n",
    "    \n",
    "    df_individual = pd.read_json(json_individual)\n",
    "\n",
    "    \n",
    "    #GPT model\n",
    "    if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "        \n",
    "        gpt_model = \"gpt-4o\"\n",
    "\n",
    "    #apply GPT_individual to each respondent's judgment spreadsheet\n",
    "    \n",
    "    GPT_activation = int(df_master.loc[0, 'Use GPT'])\n",
    "\n",
    "    questions_json = df_master.loc[0, 'questions_json']\n",
    "\n",
    "    #apply GPT_individual to each respondent's judgment spreadsheet\n",
    "\n",
    "    df_updated = engage_GPT_b64_json_own(questions_json, df_individual, GPT_activation, gpt_model, system_instruction)\n",
    "\n",
    "    #Remove redundant columns\n",
    "\n",
    "    for column in ['tokens_raw', 'b64_list']:\n",
    "        try:\n",
    "            df_updated.pop(column)\n",
    "        except:\n",
    "            print(f\"No {column} column.\")\n",
    "\n",
    "    return df_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d5dcd6",
   "metadata": {},
   "source": [
    "# Streamlit form, functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a2069c-e202-4c3e-abd4-e79483d8a5db",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Import functions and variables\n",
    "from common_functions import open_page, clear_cache_except_validation_df_master, tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f4f1ab-dc99-40a2-98cb-21bd59af47fc",
   "metadata": {},
   "source": [
    "## Initialize session states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256a4fc-c9ae-487d-a2ad-ca303dacb43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Page bound\n",
    "\n",
    "default_page_bound = 10\n",
    "\n",
    "print(f\"\\nThe maximum number of pages per file is {default_page_bound}.\")\n",
    "\n",
    "if 'page_bound' not in st.session_state:\n",
    "    st.session_state['page_bound'] = default_page_bound\n",
    "\n",
    "#Default file counter bound\n",
    "\n",
    "default_file_counter_bound = 10\n",
    "\n",
    "if 'file_counter_bound' not in st.session_state:\n",
    "    st.session_state['file_counter_bound'] = default_file_counter_bound\n",
    "\n",
    "print(f\"The default number of files to scrape per request is capped at {default_file_counter_bound}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd44ad7-935b-4e9b-896f-19894c9d3097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize default values\n",
    "\n",
    "if 'gpt_enhancement_entry' not in st.session_state:\n",
    "    st.session_state['gpt_enhancement_entry'] = False\n",
    "\n",
    "if 'gpt_api_key_validity' not in st.session_state:\n",
    "    st.session_state['gpt_api_key_validity'] = False\n",
    "\n",
    "if 'own_account' not in st.session_state:\n",
    "    st.session_state['own_account'] = False\n",
    "\n",
    "if 'need_resetting' not in st.session_state:\n",
    "        \n",
    "    st.session_state['need_resetting'] = 0\n",
    "\n",
    "if 'df_master' not in st.session_state:\n",
    "\n",
    "    st.session_state['df_master'] = pd.DataFrame([])\n",
    "\n",
    "if 'df_individual_output' not in st.session_state:\n",
    "\n",
    "    st.session_state['df_individual_output'] = pd.DataFrame([])\n",
    "\n",
    "#Disable toggles\n",
    "if 'disable_input' not in st.session_state:\n",
    "    st.session_state[\"disable_input\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1468b18e-ce36-4a3e-bf79-4731d5639898",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Try to carry over previously entered personal details    \n",
    "try:\n",
    "    st.session_state['gpt_api_key_entry'] = st.session_state.df_master.loc[0, 'Your GPT API key']\n",
    "except:\n",
    "    st.session_state['gpt_api_key_entry'] = ''\n",
    "\n",
    "try:\n",
    "    st.session_state['name_entry'] = st.session_state.df_master.loc[0, 'Your name']\n",
    "except:\n",
    "    st.session_state['name_entry'] = ''\n",
    "\n",
    "try:\n",
    "    st.session_state['email_entry'] = st.session_state.df_master.loc[0, '\"Your email address']\n",
    "    \n",
    "except:\n",
    "    st.session_state['email_entry'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffea44a-35e1-41a0-b8fa-1b8ba802e3b9",
   "metadata": {},
   "source": [
    "## Form before AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c683d9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create form\n",
    "\n",
    "return_button = st.button('RETURN to first page')\n",
    "\n",
    "st.header(f\"You have selected to study :blue[your own files].\")\n",
    "    \n",
    "st.write(f'**:green[Please upload your documents or images.]** By default, this program will extract text from up to {default_file_counter_bound} files, and process up to approximately {round(tokens_cap(\"gpt-4o-mini\")*3/4)} words from the first {default_page_bound} pages of each file.')\n",
    "\n",
    "st.write('This program works only if the text from your file(s) is displayed horizontally and neatly.')\n",
    "\n",
    "st.caption('During the pilot stage, the number of files and the number of words per file to be processed are capped. Please reach out to Ben Chen at ben.chen@sydney.edu.au should you wish to cover more files or more words per file.')\n",
    "\n",
    "st.subheader('Upload documents')\n",
    "\n",
    "st.markdown(\"\"\"Supported document formats: **searchable PDF**, **DOCX**, **TXT**, **JSON**, CS,  EPUB, MOBI, XML, XPS.\n",
    "\"\"\")\n",
    "\n",
    "uploaded_docs = st.file_uploader(\"Please choose your document(s).\", type = doc_types, accept_multiple_files=True)\n",
    "\n",
    "st.caption('DOC is not yet supported. Microsoft Word or a similar program can convert a DOC file to a DOCX file.')\n",
    "\n",
    "st.subheader('Upload images')\n",
    "\n",
    "st.markdown(\"\"\"Supported image formats: **non-searchable PDF**, **JPG**, **JPEG**, **PNG**, BMP, GIF, TIFF.\n",
    "\"\"\")\n",
    "uploaded_images = st.file_uploader(\"Please choose your image(s).\", type = image_types, accept_multiple_files=True)\n",
    "\n",
    "st.caption(\"By default, [Python-tesseract](https://pypi.org/project/pytesseract/) will extract text from images. This tool is based on [Googleâ€™s Tesseract-OCR Engine](https://github.com/tesseract-ocr/tesseract).\")\n",
    "\n",
    "st.subheader('Language of uploaded files')\n",
    "\n",
    "st.markdown(\"\"\"In what language is the text from your uploaded file(s) written?\"\"\")\n",
    "    \n",
    "language_entry = st.selectbox(\"Please choose a language.\", languages_list, index=0)\n",
    "\n",
    "st.caption('During the pilot stage, the languages supported are limited. Please reach out to Ben Chen at ben.chen@sydney.edu.au should you wish to choose a language which is not available under this menu.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eacfac-cb27-4abb-b5fc-94ce69696a56",
   "metadata": {},
   "source": [
    "## Form for AI and account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d8a8b7-9d6d-4b79-a89f-bb3623714f72",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "st.header(\"Use GPT as your research assistant\")\n",
    "\n",
    "#    st.markdown(\"**You have three (3) opportunities to engage with GPT through the Empirical Legal Research Kickstarter. Would you like to use one (1) of these opportunities now?**\")\n",
    "\n",
    "st.markdown(\"**:green[Would you like GPT to answer questions about your files?]**\")\n",
    "\n",
    "gpt_activation_entry = st.checkbox('Use GPT', value = False)\n",
    "\n",
    "st.caption(\"Use of GPT is costly and funded by a grant. For the model used by default (gpt-4o-mini), Ben's own experience suggests that it costs approximately USD \\$0.05 (excl GST) per judgment. The [exact cost](https://openai.com/pricing) for answering a question about a judgment depends on the length of the question, the length of the judgment, and the length of the answer produced. You will be given ex-post cost estimates.\")\n",
    "\n",
    "st.subheader(\"Enter your questions for each file\")\n",
    "\n",
    "st.markdown(\"\"\"Please enter one question **per line or per paragraph**. GPT will answer your questions for **each** file based only on information from **that** file. \"\"\")\n",
    "\n",
    "st.markdown(\"\"\"GPT is instructed to avoid giving answers which cannot be obtained from the relevant file itself. This is to minimise the risk of giving incorrect information (ie hallucination).\"\"\")\n",
    "\n",
    "if st.toggle('See the instruction given to GPT'):\n",
    "    st.write(f\"{intro_for_GPT[0]['content']}\")\n",
    "\n",
    "if st.toggle('Tips for using GPT'):\n",
    "    tips()\n",
    "\n",
    "gpt_questions_entry = st.text_area(f\"You may enter at most {question_characters_bound} characters.\", height= 200, max_chars=question_characters_bound) \n",
    "\n",
    "#Disable toggles while prompt is not entered or the same as the last processed prompt\n",
    "if gpt_activation_entry:\n",
    "    \n",
    "    if gpt_questions_entry:\n",
    "        st.session_state['disable_input'] = False\n",
    "        \n",
    "    else:\n",
    "        st.session_state['disable_input'] = True\n",
    "else:\n",
    "    st.session_state['disable_input'] = False\n",
    "    \n",
    "st.caption(f\"By default, answers to your questions will be generated by model gpt-4o-mini. Due to a technical limitation, this model will read up to approximately {round(tokens_cap('gpt-4o-mini')*3/4)} words from each judgment.\")\n",
    "\n",
    "if own_account_allowed() > 0:\n",
    "    \n",
    "    st.subheader(':orange[Enhance program capabilities]')\n",
    "    \n",
    "    st.markdown(\"\"\"Would you like to increase the quality and accuracy of answers from GPT, or increase the maximum nunber of judgments to process? You can do so with your own GPT account.\n",
    "    \"\"\")\n",
    "    \n",
    "    own_account_entry = st.toggle('Use my own GPT account',  disabled = st.session_state.disable_input)\n",
    "    \n",
    "    if own_account_entry:\n",
    "    \n",
    "        st.session_state[\"own_account\"] = True\n",
    "    \n",
    "        st.markdown(\"\"\"**:green[Please enter your name, email address and API key.]** You can sign up for a GPT account and pay for your own usage [here](https://platform.openai.com/signup). You can then create and find your API key [here](https://platform.openai.com/api-keys).\n",
    "    \"\"\")\n",
    "            \n",
    "        name_entry = st.text_input(label = \"Your name\", value = st.session_state.name_entry)\n",
    "    \n",
    "        email_entry = st.text_input(label = \"Your email address\", value = st.session_state.email_entry)\n",
    "        \n",
    "        gpt_api_key_entry = st.text_input(label = \"Your GPT API key (mandatory)\", value = st.session_state.gpt_api_key_entry)\n",
    "        \n",
    "        valdity_check = st.button('VALIDATE your API key')\n",
    "    \n",
    "        if valdity_check:\n",
    "            \n",
    "            api_key_valid = is_api_key_valid(gpt_api_key_entry)\n",
    "                    \n",
    "            if api_key_valid == False:\n",
    "                st.session_state['gpt_api_key_validity'] = False\n",
    "                st.error('Your API key is not valid.')\n",
    "                \n",
    "            else:\n",
    "                st.session_state['gpt_api_key_validity'] = True\n",
    "                st.success('Your API key is valid.')\n",
    "    \n",
    "        st.markdown(\"\"\"**:green[You can use the flagship version of GPT model (gpt-4o),]** which is :red[about 30 times more expensive, per character] than the default model (gpt-4o-mini) which you can use for free.\"\"\")  \n",
    "        \n",
    "        gpt_enhancement_entry = st.checkbox('Use the flagship GPT model', value = False)\n",
    "        st.caption('Click [here](https://openai.com/api/pricing) for pricing information on different GPT models.')\n",
    "        \n",
    "        if gpt_enhancement_entry == True:\n",
    "        \n",
    "            st.session_state.gpt_model = \"gpt-4o\"\n",
    "            st.session_state.gpt_enhancement_entry = True\n",
    "\n",
    "        else:\n",
    "            \n",
    "            st.session_state.gpt_model = \"gpt-4o-mini\"\n",
    "            st.session_state.gpt_enhancement_entry = False\n",
    "        \n",
    "        st.write(f'**:green[You can increase the maximum number of judgments to process.]** The default maximum is {default_judgment_counter_bound}.')\n",
    "        \n",
    "        #judgments_counter_bound_entry = round(st.number_input(label = 'Enter a whole number between 1 and 100', min_value=1, max_value=100, value=default_judgment_counter_bound))\n",
    "\n",
    "        #st.session_state.judgments_counter_bound = judgments_counter_bound_entry\n",
    "\n",
    "        judgments_counter_bound_entry = st.text_input(label = 'Enter a whole number between 1 and 100', value=str(default_judgment_counter_bound))\n",
    "\n",
    "        if judgments_counter_bound_entry:\n",
    "            wrong_number_warning = f'You have not entered a whole number between 1 and 100. The program will process up to {default_judgment_counter_bound} judgments instead.'\n",
    "            try:\n",
    "                st.session_state.judgments_counter_bound = int(judgments_counter_bound_entry)\n",
    "            except:\n",
    "                st.warning(wrong_number_warning)\n",
    "                st.session_state.judgments_counter_bound = default_judgment_counter_bound\n",
    "\n",
    "            if ((st.session_state.judgments_counter_bound <= 0) or (st.session_state.judgments_counter_bound > 100)):\n",
    "                st.warning(wrong_number_warning)\n",
    "                st.session_state.judgments_counter_bound = default_judgment_counter_bound\n",
    "    \n",
    "        st.write(f'*GPT model {st.session_state.gpt_model} will answer any questions based on up to approximately {round(tokens_cap(st.session_state.gpt_model)*3/4)} words from each judgment, for up to {st.session_state.judgments_counter_bound} judgments.*')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        st.session_state[\"own_account\"] = False\n",
    "    \n",
    "        st.session_state.gpt_model = \"gpt-4o-mini\"\n",
    "\n",
    "        st.session_state.gpt_enhancement_entry = False\n",
    "    \n",
    "        st.session_state.judgments_counter_bound = default_judgment_counter_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3644b48-7cf1-4f9a-81d0-dafd8ba910a4",
   "metadata": {},
   "source": [
    "## Consent and next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5550daa8-1f1c-4845-a0f6-2a519d66cb9e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "st.header(\"Consent\")\n",
    "\n",
    "st.markdown(\"\"\"By running this program, you agree that the data and/or information this form provides will be temporarily stored on one or more remote servers for the purpose of producing an output containing data in relation to your files. Any such data and/or information may also be given to an artificial intelligence provider for the same purpose.\"\"\")\n",
    "\n",
    "consent =  st.checkbox('Yes, I agree.', value = False, disabled = st.session_state.disable_input)\n",
    "\n",
    "st.markdown(\"\"\"If you do not agree, then please feel free to close this form.\"\"\")\n",
    "\n",
    "st.header(\"Next steps\")\n",
    "\n",
    "st.markdown(\"\"\"**:green[You can now run the Empirical Legal Research Kickstarter.]** A spreadsheet which hopefully has the data you seek will be available for download.\n",
    "\n",
    "You can also download a record of your entries.\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "#Warning\n",
    "if st.session_state.gpt_model == 'gpt-4o-mini':\n",
    "    st.warning('A low-cost GPT model will answer your questions. Please reach out to Ben Chen at ben.chen@sydney.edu.au if you would like to use the flagship model instead.')\n",
    "\n",
    "if st.session_state.gpt_model == \"gpt-4o\":\n",
    "    st.warning('An expensive GPT model will answer your questions. Please be cautious.')\n",
    "\n",
    "run_button = st.button('RUN the program')\n",
    "\n",
    "keep_button = st.button('DOWNLOAD your entries')\n",
    "\n",
    "reset_button = st.button(label='RESET to start afresh', type = 'primary',  help = \"Press to process new search terms or questions.\")\n",
    "    \n",
    "if ((st.session_state.gpt_model == \"gpt-4o\") and (uploaded_images)):\n",
    "\n",
    "    st.markdown(\"\"\"By default, this program will use an Optical Character Recognition (OCR) engine to extract text from images, and then send such text to GPT.\n",
    "\n",
    "Alternatively, you can send images directly to GPT. This alternative approach may produce better responses for \"untidy\" images, but tends to be slower and costlier than the default approach.\n",
    "\"\"\")\n",
    "    \n",
    "    #st.write('Not getting the best responses for your images? You can try a more costly')\n",
    "    #b64_help_text = 'GPT will process images directly, instead of text first extracted from images by an Optical Character Recognition engine. This only works for PNG, JPEG, JPG, GIF images.'\n",
    "    run_button_b64 = st.button(label = 'SEND images to GPT directly')\n",
    "\n",
    "#test_button = st.button('Test')\n",
    "\n",
    "#Display need resetting message if necessary\n",
    "if st.session_state.need_resetting == 1:\n",
    "    if ((len(st.session_state.df_master) > 0) and (len(st.session_state.df_individual_output) > 0)):\n",
    "        st.warning('You must :red[RESET] the program before processing new search terms or questions. Please press the :red[RESET] button above.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c72a80-7164-4525-b07a-0a79019ce5b6",
   "metadata": {},
   "source": [
    "## Previous responses and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f054d-f177-4766-805b-2a50fd122662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create placeholder download buttons if previous entries and results in st.session_state:\n",
    "\n",
    "if ((len(st.session_state.df_master) > 0) and (len(st.session_state.df_individual_output)>0)):\n",
    "    \n",
    "    #Load previous entries and results\n",
    "    \n",
    "    df_master = st.session_state.df_master\n",
    "    df_individual_output = st.session_state.df_individual_output\n",
    "\n",
    "    #Buttons for downloading entries\n",
    "    st.subheader('Looking for your previous entries and results?')\n",
    "\n",
    "    st.write('Previous entries')\n",
    "\n",
    "    entries_output_name = str(df_master.loc[0, 'Your name']) + '_' + str(today_in_nums) + '_entries'\n",
    "\n",
    "    csv = convert_df_to_csv(df_master)\n",
    "\n",
    "    ste.download_button(\n",
    "        label=\"Download your previous entries as a CSV (for use in Excel etc)\", \n",
    "        data = csv,\n",
    "        file_name=entries_output_name + '.csv', \n",
    "        mime= \"text/csv\", \n",
    "#            key='download-csv'\n",
    "    )\n",
    "\n",
    "    xlsx = convert_df_to_excel(df_master)\n",
    "    \n",
    "    ste.download_button(label='Download your previous entries as an Excel spreadsheet (XLSX)',\n",
    "                        data=xlsx,\n",
    "                        file_name=entries_output_name + '.xlsx', \n",
    "                        mime='application/vnd.ms-excel',\n",
    "                       )\n",
    "\n",
    "    json = convert_df_to_json(df_master)\n",
    "    \n",
    "    ste.download_button(\n",
    "        label=\"Download your previous entries as a JSON\", \n",
    "        data = json,\n",
    "        file_name= entries_output_name + '.json', \n",
    "        mime= \"application/json\", \n",
    "    )\n",
    "\n",
    "    st.write('Previous results')\n",
    "\n",
    "    output_name = str(df_master.loc[0, 'Your name']) + '_' + str(today_in_nums) + '_results'\n",
    "\n",
    "    csv_output = convert_df_to_csv(df_individual_output)\n",
    "    \n",
    "    ste.download_button(\n",
    "        label=\"Download your previous results as a CSV (for use in Excel etc)\", \n",
    "        data = csv_output,\n",
    "        file_name= output_name + '.csv', \n",
    "        mime= \"text/csv\", \n",
    "#            key='download-csv'\n",
    "    )\n",
    "\n",
    "    excel_xlsx = convert_df_to_excel(df_individual_output)\n",
    "    \n",
    "    ste.download_button(label='Download your previous results as an Excel spreadsheet (XLSX)',\n",
    "                        data=excel_xlsx,\n",
    "                        file_name= output_name + '.xlsx', \n",
    "                        mime='application/vnd.ms-excel',\n",
    "                       )\n",
    "    \n",
    "    json_output = convert_df_to_json(df_individual_output)\n",
    "    \n",
    "    ste.download_button(\n",
    "        label=\"Download your previous results as a JSON\", \n",
    "        data = json_output,\n",
    "        file_name= output_name + '.json', \n",
    "        mime= \"application/json\", \n",
    "    )\n",
    "\n",
    "    st.page_link('pages/AI.py', label=\"ANALYSE your previous spreadsheet with an AI\", icon = 'ðŸ¤”')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f71a3",
   "metadata": {},
   "source": [
    "# Save and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376a9fbd-ec89-4733-811d-f93ef508c6fb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#if test_button:\n",
    "    #for uploaded_doc in uploaded_docs:\n",
    "        #output = doc_to_text(uploaded_doc, language_entry, st.session_state.page_bound)\n",
    "        #st.write(output)\n",
    "\n",
    "#    for uploaded_image in uploaded_images:\n",
    "#        output = image_to_text(uploaded_image, language_entry, st.session_state.page_bound)\n",
    "#        st.write(output)\n",
    "\n",
    "    #for uploaded_image in uploaded_images:\n",
    "        #output = image_to_b64_own(uploaded_image, language_entry, st.session_state.page_bound)\n",
    "        #st.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3bbb09-b5e6-420f-a4cc-d64375f9f9b2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if run_button:\n",
    "\n",
    "    if ((len(uploaded_docs) == 0) and (len(uploaded_images) == 0)):\n",
    "\n",
    "        st.warning('You must upload some file(s).')\n",
    "\n",
    "    elif len(gpt_questions_entry) < 5:\n",
    "\n",
    "        st.warning('You must enter some questions for GPT.')\n",
    "\n",
    "    elif int(consent) == 0:\n",
    "        st.warning(\"You must click on 'Yes, I agree.' to run the program.\")\n",
    "    \n",
    "    elif ((len(st.session_state.df_master) > 0) and (len(st.session_state.df_individual_output)>0)):\n",
    "        st.warning('You must :red[RESET] the program before processing new files or questions. Please press the :red[RESET] button above.')\n",
    "                    \n",
    "        st.session_state['need_resetting'] = 1\n",
    "\n",
    "    elif ((st.session_state.own_account == True) and (st.session_state.gpt_api_key_validity == False)):\n",
    "            \n",
    "        st.warning('You have not validated your API key.')\n",
    "        quit()\n",
    "\n",
    "    elif ((st.session_state.own_account == True) and (len(gpt_api_key_entry) < 20)):\n",
    "\n",
    "        st.warning('You have not entered a valid API key.')\n",
    "        quit()  \n",
    "        \n",
    "    else:\n",
    "\n",
    "        st.markdown(\"\"\"Your results will be available for download soon. The estimated waiting time is about 2-3 minutes per 10 files. \"\"\")\n",
    "\n",
    "        with st.spinner(\"Running... Please :red[don't change] your entries (yet).\"):\n",
    "                \n",
    "            #Create spreadsheet of responses\n",
    "            df_master = create_df()\n",
    "        \n",
    "            #GPT model\n",
    "        \n",
    "            #if df_master.loc[0, 'Use flagship version of GPT'] == True:\n",
    "                #gpt_model = \"gpt-4o\"\n",
    "            #else:        \n",
    "                #gpt_model = \"gpt-4o-mini\"\n",
    "            \n",
    "            #Activate user's own key or mine\n",
    "            if st.session_state.own_account == True:\n",
    "                \n",
    "                API_key = df_master.loc[0, 'Your GPT API key']\n",
    "\n",
    "            else:\n",
    "                API_key = st.secrets[\"openai\"][\"gpt_api_key\"]\n",
    "\n",
    "            openai.api_key = API_key\n",
    "            \n",
    "            df_individual_output = run(df_master, uploaded_docs, uploaded_images)\n",
    "\n",
    "            #Keep results in session state\n",
    "            st.session_state[\"df_individual_output\"] = df_individual_output\n",
    "    \n",
    "            st.session_state[\"df_master\"] = df_master\n",
    "\n",
    "            #Change session states\n",
    "            st.session_state['need_resetting'] = 1\n",
    "            \n",
    "            st.session_state[\"page_from\"] = 'pages/OWN.py'\n",
    "    \n",
    "            #Write results\n",
    "    \n",
    "            st.success('Your results are now available for download. Thank you for using the Empirical Legal Research Kickstarter!')\n",
    "    \n",
    "            if df_master.loc[0, 'Language choice'] != 'English':\n",
    "    \n",
    "                st.warning(\"If your spreadsheet reader does not display non-English text properly, please change the encoding to UTF-8 Unicode.\")\n",
    "    \n",
    "            #Button for downloading results\n",
    "            output_name = str(df_master.loc[0, 'Your name']) + '_' + str(today_in_nums) + '_results'\n",
    "    \n",
    "            csv_output = convert_df_to_csv(df_individual_output)\n",
    "            \n",
    "            ste.download_button(\n",
    "                label=\"Download your results as a CSV (for use in Excel etc)\", \n",
    "                data = csv_output,\n",
    "                file_name= output_name + '.csv', \n",
    "                mime= \"text/csv\", \n",
    "    #            key='download-csv'\n",
    "            )\n",
    "    \n",
    "            excel_xlsx = convert_df_to_excel(df_individual_output)\n",
    "            \n",
    "            ste.download_button(label='Download your results as an Excel spreadsheet (XLSX)',\n",
    "                                data=excel_xlsx,\n",
    "                                file_name= output_name + '.xlsx', \n",
    "                                mime='application/vnd.ms-excel',\n",
    "                               )\n",
    "            \n",
    "            json_output = convert_df_to_json(df_individual_output)\n",
    "            \n",
    "            ste.download_button(\n",
    "                label=\"Download your results as a JSON\", \n",
    "                data = json_output,\n",
    "                file_name= output_name + '.json', \n",
    "                mime= \"application/json\", \n",
    "            )\n",
    "    \n",
    "            st.page_link('pages/AI.py', label=\"ANALYSE your spreadsheet with an AI\", icon = 'ðŸ¤”')\n",
    "\n",
    "            #Keep record on Google sheet\n",
    "            #Obtain google spreadsheet       \n",
    "            #conn = st.connection(\"gsheets_nsw\", type=GSheetsConnection)\n",
    "            #df_google = conn.read()\n",
    "            #df_google = df_google.fillna('')\n",
    "            #df_google=df_google[df_google[\"Processed\"]!='']\n",
    "            #df_master[\"Processed\"] = datetime.now()\n",
    "            #df_master.pop(\"Your GPT API key\")\n",
    "            #df_to_update = pd.concat([df_google, df_master])\n",
    "            #conn.update(worksheet=\"OWN\", data=df_to_update, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d036f750-9693-4b10-8cbe-6df501f5fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ((st.session_state.gpt_model == \"gpt-4o\") and (uploaded_images)):\n",
    "    \n",
    "    if run_button_b64:\n",
    "    \n",
    "        if len(uploaded_images) == 0:\n",
    "    \n",
    "            st.warning('You must upload some image(s).')\n",
    "    \n",
    "        elif len(gpt_questions_entry) < 5:\n",
    "    \n",
    "            st.warning('You must enter some questions for GPT.')\n",
    "    \n",
    "        elif int(consent) == 0:\n",
    "            st.warning(\"You must click on 'Yes, I agree.' to run the program.\")\n",
    "        \n",
    "        elif ((len(st.session_state.df_master) > 0) and (len(st.session_state.df_individual_output)>0)):\n",
    "            st.warning('You must :red[RESET] the program before processing new files or questions. Please press the :red[RESET] button above.')\n",
    "            \n",
    "            st.session_state['need_resetting'] = 1\n",
    "    \n",
    "        elif ((st.session_state.own_account == True) and (st.session_state.gpt_api_key_validity == False)):\n",
    "        \n",
    "            #if (st.session_state.gpt_api_key_validity == False):\n",
    "            \n",
    "            st.warning('You have not validated your API key. Please do so.')\n",
    "            #st.warning('You must :red[RESET] the program before processing new search terms or questions. Please press the :red[RESET] button above.')\n",
    "            quit()\n",
    "            \n",
    "        else:\n",
    "    \n",
    "            st.markdown(\"\"\"Your results will be available for download soon. The estimated waiting time is about 1-2 minutes per image. \"\"\")\n",
    "    \n",
    "            with st.spinner(\"Running... Please :red[don't change] your entries (yet).\"):\n",
    "                    \n",
    "                #Create spreadsheet of responses\n",
    "                df_master = create_df()\n",
    "\n",
    "                #Check for non-supported file types\n",
    "\n",
    "                if '.bmp' in str(df_master['Your uploaded files']).lower():\n",
    "                    st.error('This function does not support BMP images.')\n",
    "                    quit()\n",
    "                    \n",
    "                elif '.tiff' in str(df_master['Your uploaded files']).lower():\n",
    "                    st.error('This function does not support TIFF images.')\n",
    "                    quit()\n",
    "                \n",
    "                #Activate user's own key or mine\n",
    "                if st.session_state.own_account == True:\n",
    "                    \n",
    "                    API_key = df_master.loc[0, 'Your GPT API key']\n",
    "    \n",
    "                else:\n",
    "                    API_key = st.secrets[\"openai\"][\"gpt_api_key\"]\n",
    "    \n",
    "                openai.api_key = API_key\n",
    "                \n",
    "                df_individual_output = run_b64_own(df_master, uploaded_images)\n",
    "    \n",
    "                #Keep results in session state\n",
    "                st.session_state[\"df_individual_output\"] = df_individual_output\n",
    "        \n",
    "                st.session_state[\"df_master\"] = df_master\n",
    "                \n",
    "                st.session_state[\"page_from\"] = 'pages/OWN.py'\n",
    "        \n",
    "                #Write results\n",
    "        \n",
    "                st.success('Your results are now available for download. Thank you for using the Empirical Legal Research Kickstarter!')\n",
    "        \n",
    "                if df_master.loc[0, 'Language choice'] != 'English':\n",
    "        \n",
    "                    st.warning(\"If your spreadsheet reader does not display non-English text properly, please change the encoding to UTF-8 Unicode.\")\n",
    "        \n",
    "                #Button for downloading results\n",
    "                output_name = str(df_master.loc[0, 'Your name']) + '_' + str(today_in_nums) + '_results'\n",
    "        \n",
    "                csv_output = convert_df_to_csv(df_individual_output)\n",
    "                \n",
    "                ste.download_button(\n",
    "                    label=\"Download your results as a CSV (for use in Excel etc)\", \n",
    "                    data = csv_output,\n",
    "                    file_name= output_name + '.csv', \n",
    "                    mime= \"text/csv\", \n",
    "        #            key='download-csv'\n",
    "                )\n",
    "        \n",
    "                excel_xlsx = convert_df_to_excel(df_individual_output)\n",
    "                \n",
    "                ste.download_button(label='Download your results as an Excel spreadsheet (XLSX)',\n",
    "                                    data=excel_xlsx,\n",
    "                                    file_name= output_name + '.xlsx', \n",
    "                                    mime='application/vnd.ms-excel',\n",
    "                                   )\n",
    "                \n",
    "                json_output = convert_df_to_json(df_individual_output)\n",
    "                \n",
    "                ste.download_button(\n",
    "                    label=\"Download your results as a JSON\", \n",
    "                    data = json_output,\n",
    "                    file_name= output_name + '.json', \n",
    "                    mime= \"application/json\", \n",
    "                )\n",
    "        \n",
    "                st.page_link('pages/AI.py', label=\"ANALYSE your spreadsheet with an AI\", icon = 'ðŸ¤”')\n",
    "    \n",
    "                #Keep record on Google sheet\n",
    "                #Obtain google spreadsheet       \n",
    "                #conn = st.connection(\"gsheets_nsw\", type=GSheetsConnection)\n",
    "                #df_google = conn.read()\n",
    "                #df_google = df_google.fillna('')\n",
    "                #df_google=df_google[df_google[\"Processed\"]!='']\n",
    "                #df_master[\"Processed\"] = datetime.now()\n",
    "                #df_master.pop(\"Your GPT API key\")\n",
    "                #df_to_update = pd.concat([df_google, df_master])\n",
    "                #conn.update(worksheet=\"OWN\", data=df_to_update, )\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4873fffb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if keep_button:\n",
    "\n",
    "    if ((len(uploaded_docs) == 0) and (len(uploaded_images) == 0)):\n",
    "\n",
    "        st.warning('You must upload some file(s).')\n",
    "\n",
    "    elif len(gpt_questions_entry) < 5:\n",
    "\n",
    "        st.warning('You must enter some questions for GPT.')\n",
    "\n",
    "    elif ((len(st.session_state.df_master) > 0) and (len(st.session_state.df_individual_output)>0)):\n",
    "        st.warning('You must :red[RESET] the program before processing new files or questions. Please press the :red[RESET] button above.')\n",
    "                    \n",
    "        st.session_state['need_resetting'] = 1\n",
    "            \n",
    "    else:\n",
    "\n",
    "        df_master = create_df()\n",
    "    \n",
    "        df_master.pop(\"Your GPT API key\")\n",
    "    \n",
    "        df_master.pop(\"Processed\")\n",
    "    \n",
    "        responses_output_name = str(df_master.loc[0, 'Your name']) + '_' + str(today_in_nums) + '_responses'\n",
    "    \n",
    "        #Produce a file to download\n",
    "    \n",
    "        csv = convert_df_to_csv(df_master)\n",
    "        \n",
    "        ste.download_button(\n",
    "            label=\"Download as a CSV (for use in Excel etc)\", \n",
    "            data = csv,\n",
    "            file_name=responses_output_name + '.csv', \n",
    "            mime= \"text/csv\", \n",
    "    #            key='download-csv'\n",
    "        )\n",
    "\n",
    "        xlsx = convert_df_to_excel(df_master)\n",
    "        \n",
    "        ste.download_button(label='Download as an Excel spreadsheet (XLSX)',\n",
    "                            data=xlsx,\n",
    "                            file_name=responses_output_name + '.xlsx', \n",
    "                            mime='application/vnd.ms-excel',\n",
    "                           )\n",
    "    \n",
    "        json = convert_df_to_json(df_master)\n",
    "        \n",
    "        ste.download_button(\n",
    "            label=\"Download as a JSON\", \n",
    "            data = json,\n",
    "            file_name= responses_output_name + '.json', \n",
    "            mime= \"application/json\", \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995962d0-2a30-4327-9a0e-55b2f3e27b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if return_button:\n",
    "\n",
    "    st.switch_page(\"Home.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c823ead-52f0-430c-9e6a-9f471d5e703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reset_button:\n",
    "    clear_cache_except_validation_df_master()\n",
    "    st.rerun()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
