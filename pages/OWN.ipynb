{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b8b879",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2312235",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Preliminary modules\n",
    "import base64 \n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "from dateutil.relativedelta import *\n",
    "from datetime import timedelta\n",
    "import sys\n",
    "import pause\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "from math import ceil\n",
    "\n",
    "#Conversion to text\n",
    "import fitz\n",
    "#from io import StringIO\n",
    "from io import BytesIO\n",
    "import pdf2image\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import mammoth\n",
    "\n",
    "#Streamlit\n",
    "import streamlit as st\n",
    "#from streamlit_gsheets import GSheetsConnection\n",
    "from streamlit.components.v1 import html\n",
    "import streamlit_ext as ste\n",
    "from streamlit_extras.stylable_container import stylable_container\n",
    "\n",
    "#OpenAI\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "#Google\n",
    "#from google.oauth2 import service_account\n",
    "\n",
    "#Excel\n",
    "from pyxlsb import open_workbook as open_xlsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "651ed485-489f-4c0c-9412-06d15e64dfe0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By default, users are allowed to use their own account\n"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from functions.common_functions import own_account_allowed, batch_mode_allowed, convert_df_to_json, convert_df_to_csv, convert_df_to_excel, str_to_int, str_to_int_page, save_input\n",
    "#Import variables\n",
    "from functions.common_functions import today_in_nums, errors_list, scraper_pause_mean, default_judgment_counter_bound, default_page_bound\n",
    "\n",
    "if own_account_allowed() > 0:\n",
    "    print(f'By default, users are allowed to use their own account')\n",
    "else:\n",
    "    print(f'By default, users are NOT allowed to use their own account')\n",
    "\n",
    "print(f\"The pause between file scraping is {scraper_pause_mean} second.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d6279-572c-4595-8272-f496cc434635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Page bound\n",
    "\n",
    "default_page_bound = 100\n",
    "\n",
    "print(f\"\\nThe maximum number of pages per file is {default_page_bound}.\")\n",
    "\n",
    "#if 'page_bound' not in st.session_state:\n",
    "    #st.session_state['page_bound'] = default_page_bound\n",
    "\n",
    "#Default file counter bound\n",
    "\n",
    "default_file_counter_bound = default_judgment_counter_bound\n",
    "\n",
    "#if 'file_counter_bound' not in st.session_state:\n",
    "    #st.session_state['file_counter_bound'] = default_file_counter_bound\n",
    "\n",
    "print(f\"The default number of files to scrape per request is capped at {default_file_counter_bound}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e819e27",
   "metadata": {},
   "source": [
    "# Functions for Own Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e69f7f-1c1d-4fec-ae31-3024cd9f7ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.own_functions import doc_types, image_types, languages_dict, languages_list, doc_to_text, image_to_text, file_prompt, role_content_own, GPT_json_own, engage_GPT_json_own, run_own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56fe91c-5bfb-4ee0-a36e-1369cf3b180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create dataframe\n",
    "#@st.cache_data\n",
    "def own_create_df():\n",
    "\n",
    "    #submission time\n",
    "    timestamp = datetime.now()\n",
    "\n",
    "    #Personal info entries\n",
    "\n",
    "    name = ''\n",
    "    \n",
    "    email = ''\n",
    "\n",
    "    gpt_api_key = ''\n",
    "\n",
    "    try:\n",
    "        name = name_entry\n",
    "    except:\n",
    "        print('Name not entered')\n",
    "    \n",
    "    try:\n",
    "        email = email_entry\n",
    "    except:\n",
    "        print('Email not entered')\n",
    "\n",
    "    try:\n",
    "        gpt_api_key = gpt_api_key_entry\n",
    "    except:\n",
    "        print('API key not entered')\n",
    "\n",
    "    #Own account status\n",
    "    own_account = st.session_state.own_account\n",
    "    \n",
    "    #file counter bound\n",
    "    file_counter_bound = st.session_state['df_master'].loc[0, 'Maximum number of files']\n",
    "\n",
    "    #Page counter bound\n",
    "\n",
    "    page_bound = st.session_state['df_master'].loc[0,'Maximum number of pages per file']\n",
    "    \n",
    "    #GPT enhancement\n",
    "    try:\n",
    "        gpt_enhancement = gpt_enhancement_entry\n",
    "    except:\n",
    "        print('GPT enhancement not entered')\n",
    "        gpt_enhancement = False\n",
    "\n",
    "    #GPT choice and entry\n",
    "    try:\n",
    "        gpt_activation_status = gpt_activation_entry\n",
    "    except:\n",
    "        gpt_activation_status = False\n",
    "\n",
    "    gpt_questions = ''\n",
    "    \n",
    "    try:\n",
    "        gpt_questions = gpt_questions_entry\n",
    "    \n",
    "    except:\n",
    "        print('GPT questions not entered.')\n",
    "\n",
    "    #Get uploaded file names\n",
    "\n",
    "    file_names_list = []\n",
    "\n",
    "    for uploaded_doc in uploaded_docs:\n",
    "        file_names_list.append(uploaded_doc.name)\n",
    "\n",
    "    for uploaded_image in uploaded_images:\n",
    "        file_names_list.append(uploaded_image.name)\n",
    "\n",
    "    #Language choice\n",
    "\n",
    "    language = language_entry\n",
    "    \n",
    "    new_row = {'Processed': '',\n",
    "           'Timestamp': timestamp,\n",
    "           'Your name': name, \n",
    "           'Your email address': email, \n",
    "           'Your GPT API key': gpt_api_key, \n",
    "            'Your uploaded files' : str(file_names_list), \n",
    "           'Language choice': language, \n",
    "           'Maximum number of files': file_counter_bound, \n",
    "          'Maximum number of pages per file': page_bound, \n",
    "            'Use GPT': gpt_activation_status, \n",
    "           'Enter your questions for GPT': gpt_questions, \n",
    "            'Use own account': own_account,\n",
    "            'Use flagship version of GPT': gpt_enhancement\n",
    "          }\n",
    "\n",
    "    df_master_new = pd.DataFrame(new_row, index = [0])\n",
    "        \n",
    "    return df_master_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a2d594",
   "metadata": {},
   "source": [
    "# GPT functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bfae1df-142c-4583-ab1f-fd94500499b1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gpt_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Import functions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_by_line, GPT_label_dict, is_api_key_valid, gpt_input_cost, gpt_output_cost, tokens_cap, num_tokens_from_string  \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Import variables\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m question_characters_bound, default_judgment_counter_bound\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gpt_functions'"
     ]
    }
   ],
   "source": [
    "#Import functions\n",
    "from functions.gpt_functions import split_by_line, GPT_label_dict, is_api_key_valid, gpt_input_cost, gpt_output_cost, tokens_cap, max_output, num_tokens_from_string  \n",
    "#Import variables\n",
    "from functions.gpt_functions import question_characters_bound, judgment_batch_cutoff, judgment_batch_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab06b309-e9be-495e-96fd-1073b25291c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Questions for GPT are capped at {question_characters_bound} characters.\\n\")\n",
    "print(f\"The default number of files to scrape per request is capped at {default_file_counter_bound}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e8eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize default GPT settings\n",
    "\n",
    "if 'gpt_model' not in st.session_state:\n",
    "    st.session_state['gpt_model'] = \"gpt-4o-mini\"\n",
    "    \n",
    "#Initialize API key\n",
    "if 'gpt_api_key' not in st.session_state:\n",
    "\n",
    "    st.session_state['gpt_api_key'] = st.secrets[\"openai\"][\"gpt_api_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f7af6e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Define system role content for GPT\n",
    "system_instruction = role_content_own\n",
    "\n",
    "intro_for_GPT = [{\"role\": \"system\", \"content\": system_instruction}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb2c4cf-70cf-48cb-856b-1df1b044be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import functions for vision, own file only\n",
    "from functions.gpt_functions import get_image_dims, calculate_image_token_cost\n",
    "from functions.own_functions import image_to_b64_own, GPT_b64_json_own, run_b64_own, engage_GPT_b64_json_own"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d5dcd6",
   "metadata": {},
   "source": [
    "# Streamlit form, functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a2069c-e202-4c3e-abd4-e79483d8a5db",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Import functions and variables\n",
    "from functions.common_functions import open_page, clear_cache_except_validation_df_master, tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f4f1ab-dc99-40a2-98cb-21bd59af47fc",
   "metadata": {},
   "source": [
    "## Initialize session states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd44ad7-935b-4e9b-896f-19894c9d3097",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Initialize default values\n",
    "\n",
    "if 'jurisdiction_page' not in st.session_state:\n",
    "\n",
    "    st.session_state['jurisdiction_page'] = 'pages/OWN.py'\n",
    "\n",
    "if 'gpt_api_key_validity' not in st.session_state:\n",
    "    st.session_state['gpt_api_key_validity'] = False\n",
    "\n",
    "if 'own_account' not in st.session_state:\n",
    "    st.session_state['own_account'] = False\n",
    "\n",
    "if 'need_resetting' not in st.session_state:\n",
    "        \n",
    "    st.session_state['need_resetting'] = 0\n",
    "\n",
    "if 'df_master' not in st.session_state:\n",
    "\n",
    "    #Generally applicable\n",
    "    st.session_state['df_master'] = pd.DataFrame([])\n",
    "    st.session_state['df_master'].loc[0, 'Your name'] = ''\n",
    "    st.session_state['df_master'].loc[0, 'Your email address'] = ''\n",
    "    st.session_state['df_master'].loc[0, 'Your GPT API key'] = ''\n",
    "    st.session_state['df_master'].loc[0, 'Maximum number of files'] = default_judgment_counter_bound\n",
    "    st.session_state['df_master'].loc[0, 'Maximum number of pages per file'] = default_page_bound\n",
    "    st.session_state['df_master'].loc[0, 'Language choice'] = 'English'\n",
    "    st.session_state['df_master'].loc[0, 'Enter your questions for GPT'] = ''\n",
    "    st.session_state['df_master'].loc[0, 'Use GPT'] = False\n",
    "    st.session_state['df_master'].loc[0, 'Use own account'] = False\n",
    "    st.session_state['df_master'].loc[0, 'Use flagship version of GPT'] = False\n",
    "    \n",
    "if 'df_individual' not in st.session_state:\n",
    "\n",
    "    st.session_state['df_individual'] = pd.DataFrame([])\n",
    "\n",
    "#Disable toggles\n",
    "if 'disable_input' not in st.session_state:\n",
    "    st.session_state[\"disable_input\"] = True\n",
    "\n",
    "#default_judgment_counter_bound < judgment_batch_cutoff < judgment_batch_max\n",
    "\n",
    "#Instant mode max/batch mode threshold\n",
    "if 'judgment_batch_cutoff' not in st.session_state:\n",
    "    if own_account_allowed() > 0:\n",
    "        st.session_state[\"judgment_batch_cutoff\"] = judgment_batch_cutoff\n",
    "    else:\n",
    "        st.session_state[\"judgment_batch_cutoff\"] = default_judgment_counter_bound\n",
    "\n",
    "#Maximum number of judgments to process under any mode\n",
    "if \"judgment_counter_max\" not in st.session_state:\n",
    "\n",
    "    if ((batch_mode_allowed() > 0) and (st.session_state.jurisdiction_page in ['pages/HCA.py', 'pages/FCA.py', 'pages/NSW.py'])):\n",
    "\n",
    "        if own_account_allowed() > 0:\n",
    "            st.session_state[\"judgment_counter_max\"] = judgment_batch_max\n",
    "        \n",
    "        else:\n",
    "            st.session_state[\"judgment_counter_max\"] = judgment_batch_cutoff\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        if own_account_allowed() > 0:\n",
    "            st.session_state[\"judgment_counter_max\"] = judgment_batch_cutoff\n",
    "        \n",
    "        else:\n",
    "            st.session_state[\"judgment_counter_max\"] = default_judgment_counter_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffea44a-35e1-41a0-b8fa-1b8ba802e3b9",
   "metadata": {},
   "source": [
    "## Form before AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c683d9af",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Create form\n",
    "\n",
    "return_button = st.button('RETURN to first page')\n",
    "\n",
    "st.header(f\"You have selected to study :blue[your own files].\")\n",
    "    \n",
    "st.write(f'**:green[Please upload your documents or images.]** By default, this app will extract text from up to {default_file_counter_bound} files, and process up to approximately {round(tokens_cap(\"gpt-4o-mini\")*3/4)} words from the first {default_page_bound} pages of each file.')\n",
    "\n",
    "st.write('This app works only if the text from your file(s) is displayed horizontally and neatly.')\n",
    "\n",
    "st.caption('During the pilot stage, the number of files and the number of words per file to be processed are capped. Please reach out to Ben Chen at ben.chen@sydney.edu.au should you wish to cover more files or more words per file.')\n",
    "\n",
    "st.subheader('Upload documents')\n",
    "\n",
    "st.markdown(\"\"\"Supported document formats: **searchable PDF**, **DOCX**, **TXT**, **JSON**, CS,  EPUB, MOBI, XML, HTML, XPS.\n",
    "\"\"\")\n",
    "\n",
    "uploaded_docs = st.file_uploader(\"Please choose your document(s).\", type = doc_types, accept_multiple_files=True)\n",
    "\n",
    "st.caption('DOC is not yet supported. Microsoft Word or a similar software can convert a DOC file to a DOCX file.')\n",
    "\n",
    "st.subheader('Upload images')\n",
    "\n",
    "st.markdown(\"\"\"Supported image formats: **non-searchable PDF**, **JPG**, **JPEG**, **PNG**, BMP, GIF, TIFF.\n",
    "\"\"\")\n",
    "\n",
    "uploaded_images = st.file_uploader(\"Please choose your image(s).\", type = image_types, accept_multiple_files=True)\n",
    "\n",
    "st.caption(\"By default, [Python-tesseract](https://pypi.org/project/pytesseract/) will extract text from images. This tool is based on [Google’s Tesseract-OCR Engine](https://github.com/tesseract-ocr/tesseract).\")\n",
    "\n",
    "st.subheader('Language of uploaded files')\n",
    "\n",
    "st.markdown(\"\"\"In what language is the text from your uploaded file(s) written?\"\"\")\n",
    "    \n",
    "language_entry = st.selectbox(\"Please choose a language.\", languages_list, index=0)\n",
    "\n",
    "st.caption('During the pilot stage, the languages supported are limited. Please reach out to Ben Chen at ben.chen@sydney.edu.au should you wish to choose a language which is not available under this menu.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eacfac-cb27-4abb-b5fc-94ce69696a56",
   "metadata": {},
   "source": [
    "## Form for AI and account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d8a8b7-9d6d-4b79-a89f-bb3623714f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.header(\"Use GPT as your research assistant\")\n",
    "\n",
    "st.markdown(\"**:green[Would you like GPT to answer questions about your files?]**\")\n",
    "\n",
    "gpt_activation_entry = st.checkbox(label = 'Use GPT', value = st.session_state['df_master'].loc[0, 'Use GPT'])\n",
    "\n",
    "if gpt_activation_entry:\n",
    "    \n",
    "    st.session_state['df_master'].loc[0, 'Use GPT'] = gpt_activation_entry\n",
    "    \n",
    "st.caption(\"Use of GPT is costly and funded by a grant. For the model used by default (gpt-4o-mini), Ben's own experience suggests that it costs approximately USD \\$0.01 (excl GST) per file. The [exact cost](https://openai.com/pricing) for answering a question about a file depends on the length of the question, the length of the file, and the length of the answer produced. You will be given ex-post cost estimates.\")\n",
    "\n",
    "st.subheader(\"Enter your questions for each file\")\n",
    "\n",
    "st.markdown(\"\"\"Please enter one question **per line or per paragraph**. GPT will answer your questions for **each** file based only on information from **that** file. \"\"\")\n",
    "\n",
    "st.markdown(\"\"\"GPT is instructed to avoid giving answers which cannot be obtained from the relevant file itself. This is to minimise the risk of giving incorrect information (ie hallucination).\"\"\")\n",
    "\n",
    "#if st.toggle('See the instruction given to GPT'):\n",
    "    #st.write(f\"{intro_for_GPT[0]['content']}\")\n",
    "\n",
    "if st.toggle('Tips for using GPT'):\n",
    "    tips()\n",
    "\n",
    "gpt_questions_entry = st.text_area(label = f\"You may enter at most {question_characters_bound} characters.\", height= 200, max_chars=question_characters_bound, value = st.session_state['df_master'].loc[0, 'Enter your questions for GPT']) \n",
    "\n",
    "if gpt_questions_entry:\n",
    "    \n",
    "    st.session_state['df_master'].loc[0, 'Enter your questions for GPT'] = gpt_questions_entry\n",
    "\n",
    "#Disable toggles while prompt is not entered or the same as the last processed prompt\n",
    "\n",
    "if gpt_activation_entry:\n",
    "    \n",
    "    if gpt_questions_entry:\n",
    "        st.session_state['disable_input'] = False\n",
    "        \n",
    "    else:\n",
    "        st.session_state['disable_input'] = True\n",
    "else:\n",
    "    st.session_state['disable_input'] = False\n",
    "    \n",
    "st.caption(f\"By default, answers to your questions will be generated by model gpt-4o-mini. Due to a technical limitation, this model will read up to approximately {round(tokens_cap('gpt-4o-mini')*3/4)} words from each file.\")\n",
    "\n",
    "if own_account_allowed() > 0:\n",
    "    \n",
    "    st.subheader(':orange[Enhance app capabilities]')\n",
    "    \n",
    "    st.markdown(\"\"\"Would you like to increase the quality and accuracy of answers from GPT, or increase the maximum nunber of files to process? You can do so with your own GPT account.\n",
    "    \"\"\")\n",
    "    \n",
    "    own_account_entry = st.toggle(label = 'Use my own GPT account',  disabled = st.session_state.disable_input, value = st.session_state['df_master'].loc[0, 'Use own account'])\n",
    "    \n",
    "    if own_account_entry:\n",
    "    \n",
    "        st.session_state['df_master'].loc[0, 'Use own account'] = own_account_entry\n",
    "        \n",
    "        st.session_state[\"own_account\"] = True\n",
    "    \n",
    "        st.markdown(\"\"\"**:green[Please enter your name, email address and API key.]** You can sign up for a GPT account and pay for your own usage [here](https://platform.openai.com/signup). You can then create and find your API key [here](https://platform.openai.com/api-keys).\n",
    "    \"\"\")\n",
    "        \n",
    "        name_entry = st.text_input(label = \"Your name\", value = st.session_state['df_master'].loc[0, 'Your name'])\n",
    "\n",
    "        if name_entry:\n",
    "            \n",
    "            st.session_state['df_master'].loc[0, 'Your name'] = name_entry\n",
    "        \n",
    "        email_entry = st.text_input(label = \"Your email address\", value =  st.session_state['df_master'].loc[0, 'Your email address'])\n",
    "\n",
    "        if email_entry:\n",
    "            \n",
    "            st.session_state['df_master'].loc[0, 'Your email address'] = email_entry\n",
    "        \n",
    "        gpt_api_key_entry = st.text_input(label = \"Your GPT API key (mandatory)\", value = st.session_state['df_master'].loc[0, 'Your GPT API key'])\n",
    "        \n",
    "        if gpt_api_key_entry:\n",
    "            \n",
    "            st.session_state['df_master'].loc[0, 'Your GPT API key'] = gpt_api_key_entry\n",
    "\n",
    "            if ((len(gpt_api_key_entry) < 40) or (gpt_api_key_entry[0:2] != 'sk')):\n",
    "                \n",
    "                st.warning('This key is not valid.')\n",
    "                \n",
    "        st.markdown(\"\"\"**:green[You can use the flagship version of GPT (model gpt-4o),]** which is :red[significantly more expensive] than the default model (gpt-4o-mini) which you can use for free.\"\"\")  \n",
    "        \n",
    "        gpt_enhancement_entry = st.checkbox('Use the flagship GPT model', value = st.session_state['df_master'].loc[0, 'Use flagship version of GPT'])\n",
    "        \n",
    "        st.caption('Click [here](https://openai.com/api/pricing) for pricing information on different GPT models.')\n",
    "\n",
    "        if gpt_enhancement_entry == True:\n",
    "        \n",
    "            st.session_state.gpt_model = \"gpt-4o-2024-08-06\"\n",
    "            st.session_state['df_master'].loc[0, 'Use flagship version of GPT'] = True\n",
    "\n",
    "        else:\n",
    "            \n",
    "            st.session_state.gpt_model = 'gpt-4o-mini'\n",
    "            st.session_state['df_master'].loc[0, 'Use flagship version of GPT'] = False\n",
    "        \n",
    "        st.write(f'**:green[You can increase the maximum number of files to process.]** The default maximum is {default_file_counter_bound}.')\n",
    "\n",
    "        file_counter_bound_entry = st.number_input(label = f'Up to {st.session_state[\"judgment_counter_max\"]}', min_value = 1, max_value = st.session_state[\"judgment_counter_max\"], step = 1, value = str_to_int(st.session_state['df_master'].loc[0, 'Maximum number of files']))\n",
    "\n",
    "        if file_counter_bound_entry:\n",
    "            \n",
    "            st.session_state['df_master'].loc[0, 'Maximum number of files'] = file_counter_bound_entry\n",
    "        \n",
    "        st.write(f'**:orange[You can increase the maximum number of pages per file to process.]** The default maximum is {default_page_bound}.')\n",
    "        \n",
    "        page_bound_entry = st.number_input(label = 'Enter a number between 1 and 100', min_value = 1, max_value = 100, step = 1, value = str_to_int_page(st.session_state['df_master'].loc[0, 'Maximum number of pages per file']))\n",
    "\n",
    "        if page_bound_entry:\n",
    "            \n",
    "            st.session_state['df_master'].loc[0, 'Maximum number of pages per file'] = page_bound_entry\n",
    "        \n",
    "        st.write(f\"*GPT model {st.session_state.gpt_model} will answer any questions based on up to approximately {int(round(tokens_cap(st.session_state.gpt_model)*3/4))} words from the first  {int(st.session_state['df_master'].loc[0,'Maximum number of pages per file'])} page(s) of each file, for up to {int(st.session_state['df_master'].loc[0, 'Maximum number of files'])} file(s).*\")\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        st.session_state[\"own_account\"] = False\n",
    "\n",
    "        st.session_state['df_master'].loc[0, 'Use own account'] = False\n",
    "    \n",
    "        st.session_state.gpt_model = \"gpt-4o-mini\"\n",
    "\n",
    "        st.session_state['df_master'].loc[0, 'Use flagship version of GPT'] = False\n",
    "    \n",
    "        st.session_state['df_master'].loc[0, 'Maximum number of files'] = default_file_counter_bound\n",
    "\n",
    "        st.session_state['df_master'].loc[0,'Maximum number of pages per file'] = default_page_bound\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd61bdce-8fc2-4835-b4d7-1bd4f07eb320",
   "metadata": {},
   "source": [
    "## Save entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34fe3779-93b5-4899-91aa-2a894dd4cdd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'st' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gpt_keep_button \u001b[38;5;241m=\u001b[39m \u001b[43mst\u001b[49m\u001b[38;5;241m.\u001b[39mbutton(label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOWNLOAD entries\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gpt_keep_button:\n\u001b[1;32m      4\u001b[0m     st\u001b[38;5;241m.\u001b[39msuccess(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScroll down to download your entries.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'st' is not defined"
     ]
    }
   ],
   "source": [
    "keep_button = st.button(label = 'DOWNLOAD entries')\n",
    "\n",
    "if keep_button:\n",
    "    st.success('Scroll down to download your entries.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3644b48-7cf1-4f9a-81d0-dafd8ba910a4",
   "metadata": {},
   "source": [
    "## Consent and next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5550daa8-1f1c-4845-a0f6-2a519d66cb9e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "st.header(\"Consent\")\n",
    "\n",
    "st.markdown(\"\"\"By using this app, you agree that the data and/or information this form provides will be temporarily stored on one or more remote servers for the purpose of producing an output containing data in relation to your files. Any such data and/or information may also be given to an artificial intelligence provider for the same purpose.\"\"\")\n",
    "\n",
    "consent =  st.checkbox('Yes, I agree.', value = False, disabled = st.session_state.disable_input)\n",
    "\n",
    "if consent:\n",
    "    st.session_state['df_master'].loc[0, 'Consent'] = consent\n",
    "\n",
    "st.markdown(\"\"\"If you do not agree, then please feel free to close this form.\"\"\")\n",
    "\n",
    "st.header(\"Next steps\")\n",
    "\n",
    "st.markdown(\"\"\"You can now press :green[PRODUCE data] to obtain a spreadsheet which hopefully has the data you seek.\n",
    "\n",
    "You can also download a record of your entries.\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "#Warning\n",
    "if st.session_state.gpt_model == 'gpt-4o-mini':\n",
    "    st.warning('A low-cost GPT model will answer your questions. Please reach out to Ben Chen at ben.chen@sydney.edu.au if you would like to use the flagship model instead.')\n",
    "\n",
    "if st.session_state.gpt_model == \"gpt-4o-2024-08-06\":\n",
    "    st.warning('An expensive GPT model will answer your questions. Please be cautious.')\n",
    "\n",
    "with stylable_container(\n",
    "    \"green\",\n",
    "    css_styles=\"\"\"\n",
    "    button {\n",
    "        background-color: #00FF00;\n",
    "        color: black;\n",
    "    }\"\"\",\n",
    "):\n",
    "\n",
    "    run_button = st.button('PRODUCE data')\n",
    "\n",
    "reset_button = st.button(label='REMOVE data', type = 'primary', disabled = not bool(st.session_state.need_resetting))\n",
    "\n",
    "#reset_button = st.button(label='RESET', type = 'primary',  help = \"Press to process new search terms or questions.\")\n",
    "    \n",
    "if ((st.session_state.own_account == True) and (uploaded_images)):\n",
    "\n",
    "    st.markdown(\"\"\"By default, this app will use an Optical Character Recognition (OCR) engine to extract text from images, and then send such text to GPT.\n",
    "\n",
    "Alternatively, you can send images directly to GPT. This alternative approach may produce better responses for \"untidy\" images, but tends to be slower and costlier than the default approach.\n",
    "\"\"\")\n",
    "    \n",
    "    #st.write('Not getting the best responses for your images? You can try a more costly')\n",
    "    #b64_help_text = 'GPT will process images directly, instead of text first extracted from images by an Optical Character Recognition engine. This only works for PNG, JPEG, JPG, GIF images.'\n",
    "    run_button_b64 = st.button(label = 'SEND images to GPT directly')\n",
    "\n",
    "#test_button = st.button('Test')\n",
    "\n",
    "#Display need resetting message if necessary\n",
    "if st.session_state.need_resetting == 1:\n",
    "    if ((len(st.session_state.df_master) > 0) and (len(st.session_state.df_individual) > 0)):\n",
    "        st.warning('You must :red[REMOVE] the data previously produced before producing new data.')\n",
    "        #st.warning('You must :red[RESET] the app before producing new data. Please press the :red[RESET] button above.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c72a80-7164-4525-b07a-0a79019ce5b6",
   "metadata": {},
   "source": [
    "## Previous responses and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f054d-f177-4766-805b-2a50fd122662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create placeholder download buttons if previous entries and output in st.session_state:\n",
    "\n",
    "if ((len(st.session_state.df_master) > 0) and (len(st.session_state.df_individual)>0)):\n",
    "    \n",
    "    #Load previous entries and output\n",
    "    \n",
    "    df_master = st.session_state.df_master\n",
    "    df_individual = st.session_state.df_individual\n",
    "\n",
    "    #Buttons for downloading entries\n",
    "    st.subheader('Looking for your previous entries and output?')\n",
    "\n",
    "    st.write('Previous entries')\n",
    "\n",
    "    entries_output_name = str(df_master.loc[0, 'Your name']) + '_' + str(today_in_nums) + '_entries'\n",
    "\n",
    "    csv = convert_df_to_csv(df_master)\n",
    "\n",
    "    ste.download_button(\n",
    "        label=\"Download your previous entries as a CSV (for use in Excel etc)\", \n",
    "        data = csv,\n",
    "        file_name=entries_output_name + '.csv', \n",
    "        mime= \"text/csv\", \n",
    "#            key='download-csv'\n",
    "    )\n",
    "\n",
    "    xlsx = convert_df_to_excel(df_master)\n",
    "    \n",
    "    ste.download_button(label='Download your previous entries as an Excel spreadsheet (XLSX)',\n",
    "                        data=xlsx,\n",
    "                        file_name=entries_output_name + '.xlsx', \n",
    "                        mime='application/vnd.ms-excel',\n",
    "                       )\n",
    "\n",
    "    json = convert_df_to_json(df_master)\n",
    "    \n",
    "    ste.download_button(\n",
    "        label=\"Download your previous entries as a JSON\", \n",
    "        data = json,\n",
    "        file_name= entries_output_name + '.json', \n",
    "        mime= \"application/json\", \n",
    "    )\n",
    "\n",
    "    st.write('Previous output')\n",
    "\n",
    "    output_name = str(df_master.loc[0, 'Your name']) + '_' + str(today_in_nums) + '_output'\n",
    "\n",
    "    csv_output = convert_df_to_csv(df_individual)\n",
    "    \n",
    "    ste.download_button(\n",
    "        label=\"Download your previous output as a CSV (for use in Excel etc)\", \n",
    "        data = csv_output,\n",
    "        file_name= output_name + '.csv', \n",
    "        mime= \"text/csv\", \n",
    "#            key='download-csv'\n",
    "    )\n",
    "\n",
    "    excel_xlsx = convert_df_to_excel(df_individual)\n",
    "    \n",
    "    ste.download_button(label='Download your previous output as an Excel spreadsheet (XLSX)',\n",
    "                        data=excel_xlsx,\n",
    "                        file_name= output_name + '.xlsx', \n",
    "                        mime='application/vnd.ms-excel',\n",
    "                       )\n",
    "    \n",
    "    json_output = convert_df_to_json(df_individual)\n",
    "    \n",
    "    ste.download_button(\n",
    "        label=\"Download your previous output as a JSON\", \n",
    "        data = json_output,\n",
    "        file_name= output_name + '.json', \n",
    "        mime= \"application/json\", \n",
    "    )\n",
    "\n",
    "    st.page_link('pages/AI.py', label=\"ANALYSE your previous spreadsheet with an AI\", icon = '🤔')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f71a3",
   "metadata": {},
   "source": [
    "# Save and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376a9fbd-ec89-4733-811d-f93ef508c6fb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#if test_button:\n",
    "    #for uploaded_doc in uploaded_docs:\n",
    "        #output = doc_to_text(uploaded_doc, language_entry, st.session_state['df_master'].loc[0,'Maximum number of pages per file'])\n",
    "        #st.write(output)\n",
    "\n",
    "#    for uploaded_image in uploaded_images:\n",
    "#        output = image_to_text(uploaded_image, language_entry, st.session_state['df_master'].loc[0,'Maximum number of pages per file'])\n",
    "#        st.write(output)\n",
    "\n",
    "    #for uploaded_image in uploaded_images:\n",
    "        #output = image_to_b64_own(uploaded_image, language_entry, st.session_state['df_master'].loc[0,'Maximum number of pages per file'])\n",
    "        #st.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3bbb09-b5e6-420f-a4cc-d64375f9f9b2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if run_button:\n",
    "\n",
    "    if ((len(uploaded_docs) == 0) and (len(uploaded_images) == 0)):\n",
    "\n",
    "        st.warning('You must upload some file(s).')\n",
    "\n",
    "    elif len(gpt_questions_entry) < 5:\n",
    "\n",
    "        st.warning('You must enter some questions for GPT.')\n",
    "\n",
    "    elif int(consent) == 0:\n",
    "        \n",
    "        st.warning(\"You must tick '[y]es, I agree[]' to use the app.\")\n",
    "    \n",
    "    elif len(st.session_state.df_individual)>0:\n",
    "        \n",
    "        st.warning('You must :red[REMOVE] the data already produced before producing new data.')\n",
    "\n",
    "    elif ((st.session_state.own_account == True) and (st.session_state.gpt_api_key_validity == False)):\n",
    "                \n",
    "        if is_api_key_valid(gpt_api_key_entry) == False:\n",
    "            \n",
    "            st.session_state['gpt_api_key_validity'] = False\n",
    "            \n",
    "            st.error('Your API key is not valid.')\n",
    "\n",
    "            quit()\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            st.session_state['gpt_api_key_validity'] = True\n",
    "        \n",
    "    else:\n",
    "\n",
    "        with st.spinner(r\"$\\textsf{\\normalsize \\textbf{In progress...} The estimated waiting time is 2-3 minutes per 10 files.}$\"):\n",
    "                \n",
    "            #Create spreadsheet of responses\n",
    "            df_master = own_create_df()\n",
    "            \n",
    "            #Activate user's own key or mine\n",
    "            if st.session_state.own_account == True:\n",
    "                \n",
    "                API_key = df_master.loc[0, 'Your GPT API key']\n",
    "\n",
    "            else:\n",
    "                API_key = st.secrets[\"openai\"][\"gpt_api_key\"]\n",
    "\n",
    "            openai.api_key = API_key\n",
    "            \n",
    "            df_individual = run_own(df_master, uploaded_docs, uploaded_images)\n",
    "\n",
    "            #Keep output in session state\n",
    "            st.session_state[\"df_individual\"] = df_individual\n",
    "    \n",
    "            st.session_state[\"df_master\"] = df_master\n",
    "\n",
    "            #Change session states\n",
    "            st.session_state['need_resetting'] = 1\n",
    "            \n",
    "            st.session_state[\"page_from\"] = 'pages/OWN.py'\n",
    "    \n",
    "            #Write output\n",
    "    \n",
    "            st.success('Your output are now available for download. Thank you for using LawtoData!')\n",
    "    \n",
    "            if df_master.loc[0, 'Language choice'] != 'English':\n",
    "    \n",
    "                st.warning(\"If your spreadsheet reader does not display non-English text properly, please change the encoding to UTF-8 Unicode.\")\n",
    "    \n",
    "            #Button for downloading output\n",
    "            output_name = str(df_master.loc[0, 'Your name']) + '_' + str(today_in_nums) + '_output'\n",
    "    \n",
    "            csv_output = convert_df_to_csv(df_individual)\n",
    "            \n",
    "            ste.download_button(\n",
    "                label=\"Download your output as a CSV (for use in Excel etc)\", \n",
    "                data = csv_output,\n",
    "                file_name= output_name + '.csv', \n",
    "                mime= \"text/csv\", \n",
    "    #            key='download-csv'\n",
    "            )\n",
    "    \n",
    "            excel_xlsx = convert_df_to_excel(df_individual)\n",
    "            \n",
    "            ste.download_button(label='Download your output as an Excel spreadsheet (XLSX)',\n",
    "                                data=excel_xlsx,\n",
    "                                file_name= output_name + '.xlsx', \n",
    "                                mime='application/vnd.ms-excel',\n",
    "                               )\n",
    "            \n",
    "            json_output = convert_df_to_json(df_individual)\n",
    "            \n",
    "            ste.download_button(\n",
    "                label=\"Download your output as a JSON\", \n",
    "                data = json_output,\n",
    "                file_name= output_name + '.json', \n",
    "                mime= \"application/json\", \n",
    "            )\n",
    "    \n",
    "            st.page_link('pages/AI.py', label=\"ANALYSE your spreadsheet with an AI\", icon = '🤔')\n",
    "\n",
    "            #Keep record on Google sheet\n",
    "            #Obtain google spreadsheet       \n",
    "            #conn = st.connection(\"gsheets_nsw\", type=GSheetsConnection)\n",
    "            #df_google = conn.read()\n",
    "            #df_google = df_google.fillna('')\n",
    "            #df_google=df_google[df_google[\"Processed\"]!='']\n",
    "            #df_master[\"Processed\"] = datetime.now()\n",
    "            #df_master.pop(\"Your GPT API key\")\n",
    "            #df_to_update = pd.concat([df_google, df_master])\n",
    "            #conn.update(worksheet=\"OWN\", data=df_to_update, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d036f750-9693-4b10-8cbe-6df501f5fecc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if ((st.session_state.own_account == True) and (uploaded_images)):\n",
    "    \n",
    "    if run_button_b64:\n",
    "    \n",
    "        if len(uploaded_images) == 0:\n",
    "    \n",
    "            st.warning('You must upload some image(s).')\n",
    "    \n",
    "        elif len(gpt_questions_entry) < 5:\n",
    "    \n",
    "            st.warning('You must enter some questions for GPT.')\n",
    "    \n",
    "        elif int(consent) == 0:\n",
    "            st.warning(\"You must tick '[y]es, I agree[]' to use the app.\")\n",
    "        \n",
    "        elif len(st.session_state.df_individual)>0:\n",
    "            st.warning('You must :red[REMOVE] the data already produced before producing new data.')\n",
    "    \n",
    "        elif ((st.session_state.own_account == True) and (st.session_state.gpt_api_key_validity == False)):\n",
    "                    \n",
    "            if is_api_key_valid(gpt_api_key_entry) == False:\n",
    "                \n",
    "                st.session_state['gpt_api_key_validity'] = False\n",
    "                \n",
    "                st.error('Your API key is not valid.')\n",
    "    \n",
    "                quit()\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                st.session_state['gpt_api_key_validity'] = True\n",
    "       \n",
    "        else:\n",
    "    \n",
    "            with st.spinner(r\"$\\textsf{\\normalsize \\textbf{In progress...} The estimated waiting time is 2-3 minutes per 10 files.}$\"):\n",
    "                    \n",
    "                #Create spreadsheet of responses\n",
    "                df_master = own_create_df()\n",
    "\n",
    "                #Check for non-supported file types\n",
    "\n",
    "                if '.bmp' in str(df_master['Your uploaded files']).lower():\n",
    "                    st.error('This function does not support BMP images.')\n",
    "                    quit()\n",
    "                    \n",
    "                elif '.tiff' in str(df_master['Your uploaded files']).lower():\n",
    "                    st.error('This function does not support TIFF images.')\n",
    "                    quit()\n",
    "                \n",
    "                #Activate user's own key or mine\n",
    "                if st.session_state.own_account == True:\n",
    "                    \n",
    "                    API_key = df_master.loc[0, 'Your GPT API key']\n",
    "    \n",
    "                else:\n",
    "                    API_key = st.secrets[\"openai\"][\"gpt_api_key\"]\n",
    "    \n",
    "                openai.api_key = API_key\n",
    "                \n",
    "                df_individual = run_b64_own(df_master, uploaded_images)\n",
    "    \n",
    "                #Keep output in session state\n",
    "\n",
    "                st.session_state[\"df_individual\"] = df_individual\n",
    "        \n",
    "                st.session_state[\"df_master\"] = df_master\n",
    "\n",
    "                #Change session states\n",
    "                st.session_state['need_resetting'] = 1\n",
    "                \n",
    "                st.session_state[\"page_from\"] = 'pages/OWN.py'\n",
    "\n",
    "                #Write output\n",
    "        \n",
    "                st.success('Your output are now available for download. Thank you for using LawtoData!')\n",
    "        \n",
    "                if df_master.loc[0, 'Language choice'] != 'English':\n",
    "        \n",
    "                    st.warning(\"If your spreadsheet reader does not display non-English text properly, please change the encoding to UTF-8 Unicode.\")\n",
    "        \n",
    "                #Button for downloading output\n",
    "                output_name = str(df_master.loc[0, 'Your name']) + '_' + str(today_in_nums) + '_output'\n",
    "        \n",
    "                csv_output = convert_df_to_csv(df_individual)\n",
    "                \n",
    "                ste.download_button(\n",
    "                    label=\"Download your output as a CSV (for use in Excel etc)\", \n",
    "                    data = csv_output,\n",
    "                    file_name= output_name + '.csv', \n",
    "                    mime= \"text/csv\", \n",
    "        #            key='download-csv'\n",
    "                )\n",
    "        \n",
    "                excel_xlsx = convert_df_to_excel(df_individual)\n",
    "                \n",
    "                ste.download_button(label='Download your output as an Excel spreadsheet (XLSX)',\n",
    "                                    data=excel_xlsx,\n",
    "                                    file_name= output_name + '.xlsx', \n",
    "                                    mime='application/vnd.ms-excel',\n",
    "                                   )\n",
    "                \n",
    "                json_output = convert_df_to_json(df_individual)\n",
    "                \n",
    "                ste.download_button(\n",
    "                    label=\"Download your output as a JSON\", \n",
    "                    data = json_output,\n",
    "                    file_name= output_name + '.json', \n",
    "                    mime= \"application/json\", \n",
    "                )\n",
    "        \n",
    "                st.page_link('pages/AI.py', label=\"ANALYSE your spreadsheet with an AI\", icon = '🤔')\n",
    "    \n",
    "                #Keep record on Google sheet\n",
    "                #Obtain google spreadsheet       \n",
    "                #conn = st.connection(\"gsheets_nsw\", type=GSheetsConnection)\n",
    "                #df_google = conn.read()\n",
    "                #df_google = df_google.fillna('')\n",
    "                #df_google=df_google[df_google[\"Processed\"]!='']\n",
    "                #df_master[\"Processed\"] = datetime.now()\n",
    "                #df_master.pop(\"Your GPT API key\")\n",
    "                #df_to_update = pd.concat([df_google, df_master])\n",
    "                #conn.update(worksheet=\"OWN\", data=df_to_update, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4873fffb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if keep_button:\n",
    "\n",
    "    if ((len(uploaded_docs) == 0) and (len(uploaded_images) == 0)):\n",
    "\n",
    "        st.warning('You must upload some file(s).')\n",
    "\n",
    "    elif len(gpt_questions_entry) < 5:\n",
    "\n",
    "        st.warning('You must enter some questions for GPT.')\n",
    "            \n",
    "    else:\n",
    "\n",
    "        st.subheader('Your entries are now available for download.')\n",
    "\n",
    "        df_master = own_create_df()\n",
    "\n",
    "        st.session_state[\"df_master\"] = df_master\n",
    "\n",
    "        df_master.pop(\"Your GPT API key\")\n",
    "    \n",
    "        df_master.pop(\"Processed\")\n",
    "    \n",
    "        responses_output_name = str(df_master.loc[0, 'Your name']) + '_' + str(today_in_nums) + '_responses'\n",
    "    \n",
    "        #Produce a file to download\n",
    "    \n",
    "        csv = convert_df_to_csv(df_master)\n",
    "        \n",
    "        ste.download_button(\n",
    "            label=\"Download as a CSV (for use in Excel etc)\", \n",
    "            data = csv,\n",
    "            file_name=responses_output_name + '.csv', \n",
    "            mime= \"text/csv\", \n",
    "    #            key='download-csv'\n",
    "        )\n",
    "\n",
    "        xlsx = convert_df_to_excel(df_master)\n",
    "        \n",
    "        ste.download_button(label='Download as an Excel spreadsheet (XLSX)',\n",
    "                            data=xlsx,\n",
    "                            file_name=responses_output_name + '.xlsx', \n",
    "                            mime='application/vnd.ms-excel',\n",
    "                           )\n",
    "    \n",
    "        json = convert_df_to_json(df_master)\n",
    "        \n",
    "        ste.download_button(\n",
    "            label=\"Download as a JSON\", \n",
    "            data = json,\n",
    "            file_name= responses_output_name + '.json', \n",
    "            mime= \"application/json\", \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995962d0-2a30-4327-9a0e-55b2f3e27b78",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if return_button:\n",
    "\n",
    "    try:\n",
    "\n",
    "        df_master = own_create_df()\n",
    "    \n",
    "        save_input(df_master)\n",
    "    except:\n",
    "        print('df_master not created.')\n",
    "\n",
    "    st.session_state[\"page_from\"] = 'pages/OWN.py'\n",
    "\n",
    "    st.switch_page(\"Home.py\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c823ead-52f0-430c-9e6a-9f471d5e703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reset_button:\n",
    "    \n",
    "    st.session_state['df_individual'] = pd.DataFrame([])\n",
    "    \n",
    "    st.session_state['need_resetting'] = 0\n",
    "\n",
    "    #clear_cache_except_validation_df_master()\n",
    "    st.rerun()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
