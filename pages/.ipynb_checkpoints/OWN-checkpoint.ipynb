{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36c8896b-c733-444a-ad1a-058d13834dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#streamlit run Dropbox/Python/GitHub/au-uk-empirical-legal-research/pages/OWN.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b8b879",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2312235",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Preliminary modules\n",
    "import base64 \n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "from dateutil.relativedelta import *\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import pause\n",
    "import os\n",
    "import io\n",
    "\n",
    "#Conversion to text\n",
    "import fitz\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "import pdf2image\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import mammoth\n",
    "\n",
    "#Streamlit\n",
    "import streamlit as st\n",
    "from streamlit_gsheets import GSheetsConnection\n",
    "from streamlit.components.v1 import html\n",
    "import streamlit_ext as ste\n",
    "\n",
    "#OpenAI\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "#Google\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb182b0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Get current directory\n",
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfd94f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#today\n",
    "today_in_nums = str(datetime.now())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9921c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate placeholder list of errors\n",
    "errors_list = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8964258",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#Create function for saving responses and results\n",
    "def convert_df_to_json(df):\n",
    "    return df.to_json(orient = 'split', compression = 'infer')\n",
    "\n",
    "def convert_df_to_csv(df):\n",
    "   return df.to_csv(index=False).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c94f73e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Title of webpage\n",
    "st.set_page_config(\n",
    "   page_title=\"Empirical Legal Research Kickstarter (OWN)\",\n",
    "   page_icon=\"ðŸ§Š\",\n",
    "   layout=\"centered\",\n",
    "   initial_sidebar_state=\"collapsed\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a9c9a8e-aaf9-4529-92cd-33c7c84e09a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The pause between GPT prompting is 5 second.\n"
     ]
    }
   ],
   "source": [
    "#Pause\n",
    "\n",
    "scraper_pause = 5\n",
    "\n",
    "print(f\"\\nThe pause between GPT prompting is {scraper_pause} second.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c02ecaec-310e-4c29-a08b-1f6a13090ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The maximum number of pages per file is 10.\n"
     ]
    }
   ],
   "source": [
    "#Page bound\n",
    "\n",
    "page_bound = 10\n",
    "\n",
    "print(f\"\\nThe maximum number of pages per file is {page_bound}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e819e27",
   "metadata": {},
   "source": [
    "# Functions for Own Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f841231-e0ad-4647-b5c2-244594a4cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create dataframe\n",
    "#@st.cache_data\n",
    "def create_df():\n",
    "\n",
    "    #submission time\n",
    "    timestamp = datetime.now()\n",
    "\n",
    "    #Personal info entries\n",
    "    \n",
    "    name = name_entry\n",
    "    email = email_entry\n",
    "    gpt_api_key = gpt_api_key_entry\n",
    "\n",
    "    #File counter bound\n",
    "    \n",
    "    files_counter_bound_ticked = files_counter_bound_entry\n",
    "    if int(files_counter_bound_ticked) > 0:\n",
    "        files_counter_bound = 10\n",
    "    else:\n",
    "        files_counter_bound = 10000\n",
    "\n",
    "    #GPT choice and entry\n",
    "    gpt_activation_status = gpt_activation_entry\n",
    "    gpt_questions = gpt_questions_entry[0: 1000]\n",
    "\n",
    "    #Get uploaded file names\n",
    "\n",
    "    file_names_list = []\n",
    "\n",
    "    for uploaded_doc in uploaded_docs:\n",
    "        file_names_list.append(uploaded_doc.name)\n",
    "\n",
    "    for uploaded_image in uploaded_images:\n",
    "        file_names_list.append(uploaded_image.name)\n",
    "\n",
    "    #Language choice\n",
    "\n",
    "    language = language_entry\n",
    "    \n",
    "    new_row = {'Processed': '',\n",
    "           'Timestamp': timestamp,\n",
    "           'Your name': name, \n",
    "           'Your email address': email, \n",
    "           'Your GPT API key': gpt_api_key, \n",
    "            'Your uploaded files' : str(file_names_list), \n",
    "           'Language choice': language, \n",
    "           'Maximum number of files': files_counter_bound, \n",
    "           'Enter your question(s) for GPT': gpt_questions, \n",
    "          }\n",
    "\n",
    "    df_master_new = pd.DataFrame(new_row, index = [0])\n",
    "    \n",
    "#    df_master_new.to_json(current_dir + '/df_master.json', orient = 'split', compression = 'infer')\n",
    "#    df_master_new.to_excel(current_dir + '/df_master.xlsx', index=False)\n",
    "\n",
    "#    if len(df_master_new) > 0:\n",
    "        \n",
    "    return df_master_new\n",
    "\n",
    "#    else:\n",
    "#        return 'Error: spreadsheet of reponses NOT generated.' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e330af89-11fb-42ec-88fb-ef40580028eb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#File types and languages for processing\n",
    "doc_types = [\"pdf\", \"txt\", 'docx', \"xps\", \"epub\", \"mobi\", 'cs', 'xml', 'json'] #\"fb2\", \"cbz\", \"svg\",\n",
    "image_types = [\"pdf\", \"jpg\", \"jpeg\", \"png\", \"bmp\", \"gif\", \"tiff\"] #, \"pnm\", \"pgm\", \"pbm\", \"ppm\", \"pam\", \"jxr\", \"jpx\", \"jp2\", \"psd\"]\n",
    "languages_dict = {'English': 'eng', \n",
    "                  'English, Middle (1100-1500)': 'enm', \n",
    "                  'Chinese - Simplified': 'chi_sim', \n",
    "                  'Chinese - Traditional': 'chi_tra', \n",
    "                  'French': 'fra', \n",
    "                  'German' : 'deu',\n",
    "                  'Greek, Modern (1453-)': 'ell', \n",
    "                  'Greek, Ancient (-1453)': 'grc', \n",
    "                  'Hebrew' : 'heb', \n",
    "                  'Hindi' : 'hin', \n",
    "                  'Hungarian': 'hun', \n",
    "                  'Indonesian': 'ind', \n",
    "                  'Italian': 'ita', \n",
    "                  'Italian - Old': 'ita_old', \n",
    "                  'Japanese': 'jpn', \n",
    "                  'Korean': 'kor', \n",
    "                  'Malay': 'msa', \n",
    "                  'Panjabi; Punjabi': 'pan', \n",
    "                  'Polish': 'pol', \n",
    "                  'Portuguese': 'por', \n",
    "                  'Russian': 'rus', \n",
    "                  'Spanish; Castilian': 'spa', \n",
    "                  'Spanish; Castilian - Old': 'spa_old', \n",
    "                  'Swedish': 'swe', \n",
    "                  'Thai': 'tha', \n",
    "                  'Turkish': 'tur', \n",
    "                  'Uighur; Uyghur': 'uig', \n",
    "                  'Ukrainian': 'ukr', \n",
    "                  'Vietnamese': 'vie', \n",
    "                  'Yiddish': 'yid'\n",
    "                 }\n",
    "languages_list = list(languages_dict.keys())\n",
    "\n",
    "#languages_words = ', '.join(languages_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "035a5c8b-f936-4100-9ddf-b7547e4c5d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define format functions for GPT questions    \n",
    "\n",
    "#Create function to split a string into a list by line\n",
    "def split_by_line(x):\n",
    "    y = x.split('\\n')\n",
    "    for i in y:\n",
    "        if len(i) == 0:\n",
    "            y.remove(i)\n",
    "    return y\n",
    "\n",
    "#Create function to split a list into a dictionary for list items longer than 10 characters\n",
    "\n",
    "#Apply split_by_line() before the following function\n",
    "def GPT_label_dict(x_list):\n",
    "    GPT_dict = {}\n",
    "    for i in x_list:\n",
    "        if len(i) > 10:\n",
    "            GPT_index = x_list.index(i) + 1\n",
    "            i_label = 'GPT question ' + f'{GPT_index}'\n",
    "            GPT_dict.update({i_label: i})\n",
    "    return GPT_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b4492d7-d779-44f2-bf47-aafce3baa108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert each uploaded file to file name, text\n",
    "#@st.cache_data\n",
    "def doc_to_text(uploaded_doc, language):\n",
    "    file_triple = {'File name' : '', 'Language choice': language, 'Page length': '', 'file_text': '', \n",
    "#                  'Page 2': '' #Test page\n",
    "                  }\n",
    "    \n",
    "    #Get file name\n",
    "    file_triple['File name']=uploaded_doc.name\n",
    "    \n",
    "    #Get file data\n",
    "    bytes_data = uploaded_doc.getvalue()\n",
    "\n",
    "    #Get file extension\n",
    "    extension = file_triple['File name'].split('.')[-1].lower()\n",
    "\n",
    "    #Create list of pages\n",
    "    text_list = []\n",
    "\n",
    "    #Word format\n",
    "    if extension == 'docx':\n",
    "        doc_string = mammoth.convert_to_html(BytesIO(bytes_data)).value\n",
    "        text_list.append(doc_string)\n",
    "\n",
    "        file_triple['Page length'] = 1\n",
    "        \n",
    "    else:\n",
    "        #text formats\n",
    "        if extension in ['txt', 'cs', 'xml', 'json']:\n",
    "            doc = fitz.open(stream=bytes_data, filetype=\"txt\")\n",
    "\n",
    "        #Other formats\n",
    "        else:\n",
    "            doc = fitz.open(stream=bytes_data)\n",
    "\n",
    "        max_doc_number=min(len(doc), page_bound)\n",
    "        \n",
    "        for page_index in list(range(0, max_doc_number)):\n",
    "            page = doc.load_page(page_index)\n",
    "            text_page = page.get_text() \n",
    "            text_list.append(text_page)\n",
    "\n",
    "        #Length of pages\n",
    "        file_triple['Page length'] = len(doc)\n",
    "\n",
    "    file_triple['file_text'] = str(text_list)\n",
    "\n",
    "    #Test page\n",
    "#    file_triple['Page 2'] = doc.load_page(1).get_text()\n",
    "    \n",
    "    return file_triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9087b68-2c2b-4240-b04a-a02bf3580a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for images to text\n",
    "#@st.cache_data\n",
    "def image_to_text(uploaded_image, language):\n",
    "    file_triple = {'File name' : '', 'Language choice': language, 'Page length': '', 'file_text': '', \n",
    "#                  'Page 2': '' #Test page\n",
    "                  }\n",
    "\n",
    "    #Get file name\n",
    "    file_triple['File name']=uploaded_image.name\n",
    "\n",
    "    #Get file data\n",
    "    bytes_data = uploaded_image.read()\n",
    "\n",
    "    #Get file extension\n",
    "    extension = file_triple['File name'].split('.')[-1].lower()\n",
    "\n",
    "    #Obtain images from uploaded file\n",
    "    if extension == 'pdf':\n",
    "        try:\n",
    "            images = pdf2image.convert_from_bytes(bytes_data, timeout=30)\n",
    "        except PDFPopplerTimeoutError as pdf2image_timeout_error:\n",
    "            print(f\"pdf2image error: {pdf2image_timeout_error}.\")\n",
    "\n",
    "    else:\n",
    "        images = []\n",
    "        image_raw = Image.open(BytesIO(bytes_data))\n",
    "        images.append(image_raw)\n",
    "        \n",
    "    #Extract text from images\n",
    "    text_list = []\n",
    "    \n",
    "    max_images_number=min(len(images), page_bound)\n",
    "\n",
    "    for image in images[ : max_images_number]:\n",
    "        try:\n",
    "            text_page = pytesseract.image_to_string(image, lang=languages_dict[language], timeout=30)\n",
    "            text_list.append(text_page)\n",
    "            \n",
    "        except RuntimeError as pytesseract_timeout_error:\n",
    "            print(f\"pytesseract error: {pytesseract_timeout_error}.\")\n",
    "\n",
    "    file_triple['file_text'] = str(text_list)\n",
    "\n",
    "    #Length of pages\n",
    "    file_triple['Page length'] = len(images)\n",
    "\n",
    "    #Test page\n",
    "#    file_triple['Page 2'] = pytesseract.image_to_string(images[1], lang=languages_dict[language], timeout=30)\n",
    "        \n",
    "    return file_triple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a2d594",
   "metadata": {},
   "source": [
    "# GPT functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a03e8eaf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prior number of GPT uses is capped at 3 times.\n",
      "\n",
      "Questions for GPT are capped at 1000 characters.\n",
      "\n",
      "Number of files to process per request is capped at 10.\n",
      "\n",
      "The lower bound on lenth of file text to process is 500 tokens.\n"
     ]
    }
   ],
   "source": [
    "#Module and costs\n",
    "\n",
    "GPT_model = \"gpt-3.5-turbo-0125\"\n",
    "\n",
    "GPT_input_cost = 1/1000*0.0005 \n",
    "GPT_output_cost = 1/1000*0.0015\n",
    "\n",
    "#Upperbound on number of engagements with GPT\n",
    "\n",
    "GPT_use_bound = 3\n",
    "\n",
    "print(f\"\\nPrior number of GPT uses is capped at {GPT_use_bound} times.\")\n",
    "\n",
    "#Upperbound on the length of questions for GPT\n",
    "\n",
    "answers_characters_bound = 1000\n",
    "\n",
    "print(f\"\\nQuestions for GPT are capped at {answers_characters_bound} characters.\")\n",
    "\n",
    "#Upperbound on number of files to scrape\n",
    "\n",
    "files_counter_bound = 10\n",
    "\n",
    "print(f\"\\nNumber of files to process per request is capped at {files_counter_bound}.\")\n",
    "\n",
    "#Lowerbound on length of file text to proccess, in tokens\n",
    "\n",
    "file_text_lower_bound = 500\n",
    "\n",
    "print(f\"\\nThe lower bound on lenth of file text to process is {file_text_lower_bound} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e463db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to determine eligibility for GPT use\n",
    "\n",
    "#Define a list of privileged email addresses with unlimited GPT uses\n",
    "\n",
    "privileged_emails = st.secrets[\"secrets\"][\"privileged_emails\"].replace(' ', '').split(',')\n",
    "\n",
    "def prior_GPT_uses(email_address, df_online):\n",
    "    # df_online variable should be the online df_online\n",
    "    prior_use_counter = 0\n",
    "    for i in df_online.index:\n",
    "        if ((df_online.loc[i, \"Your email address\"] == email_address) \n",
    "            and (len(df_online.loc[i, \"Processed\"])>0)\n",
    "           ):\n",
    "            prior_use_counter += 1\n",
    "    if email_address in privileged_emails:\n",
    "        return 0\n",
    "    else:\n",
    "        return prior_use_counter\n",
    "\n",
    "#Define function to check whether email is educational or government\n",
    "def check_edu_gov(email_address):\n",
    "    #Return 1 if educational or government, return 0 otherwise\n",
    "    end=email_address.split('@')[1]\n",
    "    if (('.gov' in end) or ('.edu' in end) or ('.ac' in end)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef483929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokens estimate preliminaries\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "#Tokens estimate function\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "#Define file input function for JSON approach\n",
    "\n",
    "#Token limit covering both GTP input and GPT output is 16385, each token is about 4 characters\n",
    "tokens_cap = int(16385 - 3000)\n",
    "\n",
    "def file_prompt(file_triple):\n",
    "                \n",
    "    file_content = 'Based on the following document:  \"\"\"'+ file_triple['file_text'] + '\"\"\",'\n",
    "\n",
    "    file_content_tokens = num_tokens_from_string(file_content, \"cl100k_base\")\n",
    "    \n",
    "    if file_content_tokens <= tokens_cap:\n",
    "        \n",
    "        return file_content\n",
    "\n",
    "    else:\n",
    "                \n",
    "        file_chars_capped = int(tokens_cap*4)\n",
    "        \n",
    "        file_string_trimmed = file_triple['file_text'][ :int(file_chars_capped/2)] + file_triple['file_text'][-int(file_chars_capped/2): ]\n",
    "        \n",
    "        file_content_capped = 'Based on the following document:  \"\"\"'+ file_string_trimmed + '\"\"\",'\n",
    "        \n",
    "        return file_content_capped\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36f7af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define system role content for GPT\n",
    "role_content = 'You are a legal research assistant helping an academic researcher to answer questions about a document. You will be provided with the document in text form. Please answer questions based only on information contained in the document. Where your answer comes from a specific page or section of the document, provide the page number or section as part of your answer. If you cannot answer any of the questions based on the document, do not make up information, but instead write \"answer not found\".'\n",
    "\n",
    "#intro_for_GPT = [{\"role\": \"system\", \"content\": role_content}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d95be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT answer function for answers in json form, YES TOKENS\n",
    "#IN USE\n",
    "\n",
    "def GPT_json_tokens(questions_json, file_triple, API_key):\n",
    "    #'question_json' variable is a json of questions to GPT\n",
    "\n",
    "    file_for_GPT = [{\"role\": \"user\", \"content\": file_prompt(file_triple) + 'you will be given questions to answer in JSON form.'}]\n",
    "        \n",
    "    #Create answer format\n",
    "    \n",
    "    q_keys = [*questions_json]\n",
    "    \n",
    "    answers_json = {}\n",
    "    \n",
    "    for q_index in q_keys:\n",
    "        answers_json.update({q_index: 'Your answer to the question with index ' + q_index + '. State specific page numbers or sections of the file.'})\n",
    "    \n",
    "    #Create questions, which include the answer format\n",
    "    \n",
    "    question_for_GPT = [{\"role\": \"user\", \"content\": str(questions_json).replace(\"\\'\", '\"') + ' Give responses in the following JSON form: ' + str(answers_json).replace(\"\\'\", '\"')}]\n",
    "    \n",
    "    #Create messages in one prompt for GPT\n",
    "    language_content = f\"The document is written in {file_triple['Language choice']}.\"\n",
    "\n",
    "    intro_for_GPT = [{\"role\": \"system\", \"content\": role_content + language_content}] \n",
    "\n",
    "    messages_for_GPT = intro_for_GPT + file_for_GPT + question_for_GPT\n",
    "    \n",
    "#   return messages_for_GPT\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    try:\n",
    "        #completion = client.chat.completions.create(\n",
    "        completion = openai.chat.completions.create(\n",
    "            model=GPT_model,\n",
    "            messages=messages_for_GPT, \n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "#        return completion.choices[0].message.content #This gives answers as a string containing a dictionary\n",
    "        \n",
    "        #To obtain a json directly, use below\n",
    "        answers_dict = json.loads(completion.choices[0].message.content)\n",
    "        \n",
    "        #Obtain tokens\n",
    "        output_tokens = completion.usage.completion_tokens\n",
    "        \n",
    "        prompt_tokens = completion.usage.prompt_tokens\n",
    "        \n",
    "        return [answers_dict, output_tokens, prompt_tokens]\n",
    "\n",
    "    except Exception as error:\n",
    "        \n",
    "        for q_index in q_keys:\n",
    "            answers_json[q_index] = error\n",
    "        \n",
    "        return [answers_json, 0, 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80714830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GPT function for each respondent's dataframe, index by file then question, with input and output tokens given by GPT itself\n",
    "#IN USE\n",
    "\n",
    "#The following function DOES NOT check for existence of questions for GPT\n",
    "    # To so check, active line marked as #*\n",
    "def engage_GPT_json_tokens(questions_json, df_individual, GPT_activation, API_key):\n",
    "    # Variable questions_json refers to the json of questions\n",
    "    # Variable df_individual refers to each respondent's df\n",
    "    # Variable activation refers to status of GPT activation (real or test)\n",
    "    # The output is a new JSON for the relevant respondent with new columns re:\n",
    "        # \"Length of first 10 pages in tokens (up to 13385 given to GPT)\"\n",
    "        # 'GPT cost estimate (USD excl GST)'\n",
    "        # 'GPT time estimate (seconds)'\n",
    "        # GPT questions/answers\n",
    "\n",
    "    #os.environ[\"OPENAI_API_KEY\"] = API_key\n",
    "\n",
    "    openai.api_key = API_key\n",
    "    \n",
    "    #client = OpenAI()\n",
    "    \n",
    "    question_keys = [*questions_json]\n",
    "    \n",
    "    for file_index in df_individual.index:\n",
    "        \n",
    "        file_triple = df_individual.to_dict('index')[file_index]\n",
    "        \n",
    "        #Calculate and append number of tokens of file, regardless of whether given to GPT\n",
    "        file_tokens = num_tokens_from_string(str(file_triple), \"cl100k_base\")\n",
    "        df_individual.loc[file_index, \"Length of first 10 pages in tokens (up to 13385 given to GPT)\"] = file_tokens       \n",
    "\n",
    "        #Indicate whether file truncated\n",
    "        \n",
    "        df_individual.loc[file_index, \"File truncated (if given to GPT)?\"] = ''       \n",
    "        \n",
    "        if file_tokens <= tokens_cap:\n",
    "            \n",
    "            df_individual.loc[file_index, \"File truncated (if given to GPT)?\"] = 'No'\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            df_individual.loc[file_index, \"File truncated (if given to GPT)?\"] = 'Yes'\n",
    "\n",
    "        #Create columns for respondent's GPT cost, time\n",
    "        df_individual.loc[file_index, 'GPT cost estimate (USD excl GST)'] = ''\n",
    "        df_individual.loc[file_index, 'GPT time estimate (seconds)'] = ''\n",
    "                \n",
    "        #Calculate GPT start time\n",
    "\n",
    "        GPT_start_time = datetime.now()\n",
    "\n",
    "        #Depending on activation status, apply GPT_json function to each file, gives answers as a string containing a dictionary\n",
    "\n",
    "        if int(GPT_activation) > 0:\n",
    "            GPT_file_triple = GPT_json_tokens(questions_json, file_triple, API_key) #Gives [answers as a JSON, output tokens, input tokens]\n",
    "            answers_dict = GPT_file_triple[0]\n",
    "        \n",
    "        else:\n",
    "            answers_dict = {}    \n",
    "            for q_index in question_keys:\n",
    "                #Increases file index by 2 to ensure consistency with Excel spreadsheet\n",
    "                answer = 'Placeholder answer for ' + ' file ' + str(int(file_index) + 2) + ' ' + str(q_index)\n",
    "                answers_dict.update({q_index: answer})\n",
    "            \n",
    "            #Own calculation of GPT costs for Placeholder answer fors\n",
    "\n",
    "            #Calculate capped file tokens\n",
    "\n",
    "            file_capped_tokens = num_tokens_from_string(file_prompt(file_triple), \"cl100k_base\")\n",
    "\n",
    "            #Calculate questions tokens and cost\n",
    "\n",
    "            questions_tokens = num_tokens_from_string(str(questions_json), \"cl100k_base\")\n",
    "\n",
    "            #Calculate other instructions' tokens\n",
    "\n",
    "            other_instructions = role_content + 'The document is written in some language' + 'you will be given questions to answer in JSON form.' + ' Give responses in the following JSON form: '\n",
    "\n",
    "            other_tokens = num_tokens_from_string(other_instructions, \"cl100k_base\") + len(question_keys)*num_tokens_from_string(\"GPT question x:  Your answer to the question with index GPT question x. State specific page numbers or sections of the file.\", \"cl100k_base\")\n",
    "\n",
    "            #Calculate number of tokens of answers\n",
    "            answers_tokens = num_tokens_from_string(str(answers_dict), \"cl100k_base\")\n",
    "\n",
    "            input_tokens = file_capped_tokens + questions_tokens + other_tokens\n",
    "            \n",
    "            GPT_file_triple = [answers_dict, answers_tokens, input_tokens]\n",
    "\n",
    "        #Create GPT question headings and append answers to individual spreadsheets\n",
    "\n",
    "        for question_index in question_keys:\n",
    "            question_heading = question_index + ': ' + questions_json[question_index]\n",
    "            df_individual.loc[file_index, question_heading] = answers_dict[question_index]\n",
    "\n",
    "        #Calculate and append GPT finish time and time difference to individual df\n",
    "        GPT_finish_time = datetime.now()\n",
    "        \n",
    "        GPT_time_difference = GPT_finish_time - GPT_start_time\n",
    "\n",
    "        df_individual.loc[file_index, 'GPT time estimate (seconds)'] = GPT_time_difference.total_seconds()\n",
    "\n",
    "        #Calculate GPT costs\n",
    "\n",
    "        GPT_cost = GPT_file_triple[1]*GPT_output_cost + GPT_file_triple[2]*GPT_input_cost\n",
    "\n",
    "        #Calculate and append GPT cost to individual df\n",
    "        df_individual.loc[file_index, 'GPT cost estimate (USD excl GST)'] = GPT_cost\n",
    "    \n",
    "    return df_individual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83981e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain parameters\n",
    "\n",
    "def run(df_master, uploaded_docs, uploaded_images):\n",
    "    df_master = df_master.fillna('')\n",
    "\n",
    "    #Apply split and format functions for headnotes choice, court choice and GPT questions\n",
    "     \n",
    "    df_master['Enter your question(s) for GPT'] = df_master['Enter your question(s) for GPT'][0: answers_characters_bound].apply(split_by_line)\n",
    "    df_master['questions_json'] = df_master['Enter your question(s) for GPT'].apply(GPT_label_dict)\n",
    "    \n",
    "    #Create files file\n",
    "    Files_file = []\n",
    "    \n",
    "    #Convert uploaded documents to text\n",
    "    \n",
    "    files_counter_bound = int(df_master.loc[0, 'Maximum number of files'])\n",
    "\n",
    "    file_counter = 1 \n",
    "    \n",
    "    for uploaded_doc in uploaded_docs:\n",
    "        if file_counter <= files_counter_bound:\n",
    "            file_triple = doc_to_text(uploaded_doc, df_master.loc[0, 'Language choice'])\n",
    "            Files_file.append(file_triple)\n",
    "            file_counter += 1\n",
    "\n",
    "    #Convert uploaded images to text\n",
    "\n",
    "    for uploaded_image in uploaded_images:\n",
    "        if file_counter <= files_counter_bound:\n",
    "            file_triple = image_to_text(uploaded_image, df_master.loc[0, 'Language choice'])\n",
    "            Files_file.append(file_triple)\n",
    "            file_counter += 1\n",
    "    \n",
    "    #Create and export json file with search results\n",
    "    json_individual = json.dumps(Files_file, indent=2)\n",
    "    \n",
    "    df_individual = pd.read_json(json_individual)\n",
    "    \n",
    "    #Instruct GPT\n",
    "    \n",
    "    API_key = df_master.loc[0, 'Your GPT API key'] \n",
    "    \n",
    "    #apply GPT_individual to each respondent's file spreadsheet\n",
    "    \n",
    "    GPT_activation = gpt_activation_entry\n",
    "\n",
    "    questions_json = df_master.loc[0, 'questions_json']\n",
    "            \n",
    "    #Engage GPT\n",
    "    df_updated = engage_GPT_json_tokens(questions_json, df_individual, GPT_activation, API_key)\n",
    "\n",
    "    df_updated.pop('file_text')\n",
    "    \n",
    "    return df_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d5dcd6",
   "metadata": {},
   "source": [
    "# Streamlit form, functions and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c683d9af",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-26 18:44:10.333 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/Ben/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-04-26 18:44:10.334 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n"
     ]
    }
   ],
   "source": [
    "#Create form\n",
    "\n",
    "with st.form(\"GPT_input_form\") as df_responses:\n",
    "    return_button = st.form_submit_button('RETURN to previous page')\n",
    "    \n",
    "    st.header(f\"You have selected to study :blue[your own files].\")\n",
    "        \n",
    "    st.markdown(\"\"\"**Please upload your files.** This program will extract text from up to 10 files, and process up to approximately 10,000 words from the first 10 pages of each file.\n",
    "\n",
    "This program works only if the text from your file(s) is displayed horizontally and neatly.\n",
    "\n",
    "You may upload documents or images.\n",
    "\"\"\")\n",
    "\n",
    "    st.caption('During the pilot stage, the number of files and the number of words per file to read are capped. Please reach out to Ben at ben.chen@sydney.edu.au should you wish to cover more files or more words per file.')\n",
    "\n",
    "    st.subheader('Upload Documents')\n",
    "    \n",
    "    st.markdown(\"\"\"Supported document formats: **searchable PDF**, **DOCX**, **TXT**, **JSON**, CS,  EPUB, MOBI, XML, XPS.\n",
    "\"\"\")\n",
    "\n",
    "    uploaded_docs = st.file_uploader(\"Please choose your document(s).\", type = doc_types, accept_multiple_files=True)\n",
    "\n",
    "    st.caption('DOC is not yet supported. Microsoft Word or a similar program can convert a DOC file to a DOCX file.')\n",
    "    \n",
    "    st.subheader('Upload Images')\n",
    "    \n",
    "    st.markdown(\"\"\"Supported image formats: **non-searchable PDF**, **JPG**, **JPEG**, **PNG**, BMP, GIF, TIFF.\n",
    "\"\"\")\n",
    "    uploaded_images = st.file_uploader(\"Please choose your image(s).\", type = image_types, accept_multiple_files=True)\n",
    "\n",
    "    st.subheader('Language of Uploaded files')\n",
    "    \n",
    "    st.markdown(\"\"\"In what language is the text from your uploaded file(s) written?\"\"\")\n",
    "        \n",
    "    language_entry = st.selectbox(\"Please choose a language.\", languages_list, index=0)\n",
    "\n",
    "    st.caption('English is chosen by default.')\n",
    "\n",
    "    #Bound on number of files to process\n",
    "    files_counter_bound_entry = files_counter_bound\n",
    "\n",
    "    st.header(\"Use GPT as Your Research Assistant\")\n",
    "\n",
    "    st.markdown(\"**GPT can answer questions about each file uploaded by you.**\")\n",
    "    \n",
    "#    st.markdown(\"**You have three (3) opportunities to engage with GPT through the Empirical Legal Research Kickstarter. Would you like to use one (1) of these opportunities now?**\")\n",
    "\n",
    "    #gpt_activation_entry = st.checkbox('Tick to use GPT', value = False)\n",
    "\n",
    "    gpt_activation_entry = 1\n",
    "\n",
    "    st.markdown(\"\"\"You must enter your name and email address if you wish to use GPT.\n",
    "\"\"\")\n",
    "    #    st.markdown(\"\"\"You must enter an API key if you wish to use GPT to analyse more than 10 files. \n",
    "#To obtain an API key, first sign up for an account with OpenAI at \n",
    "#https://platform.openai.com/signup. You can then find your API key at https://platform.openai.com/api-keys.\n",
    "#\"\"\")\n",
    "    \n",
    "    name_entry = st.text_input(\"Your name\")\n",
    "    email_entry = st.text_input(\"Your email address\")\n",
    "#    gpt_api_key_entry = st.text_input(\"Your GPT API key\")\n",
    "\n",
    "    st.caption(\"Released by OpenAI, GPT is a family of large language models (ie a generative AI that works on language). Engagement with GPT is costly and funded by a grant.  Ben's own experience suggests that it costs approximately USD \\$0.003-\\$0.008 (excl GST) per file. The exact cost for answering a question about a file depends on the length of the question, the length of the file, and the length of the answer produced (as elaborated at https://openai.com/pricing for model gpt-3.5-turbo-0125). You will be given ex-post cost estimates.\")\n",
    "\n",
    "    st.subheader(\"Enter your question(s) for GPT\")\n",
    "    \n",
    "    st.markdown(\"\"\"You may enter one or more questions. **Please enter one question per line or per paragraph.**\n",
    "\n",
    "GPT is instructed to avoid giving answers which cannot be obtained from the relevant file itself. This is to minimise the risk of giving incorrect information (ie hallucination).\n",
    "\n",
    "You may enter at most 1000 characters here.\n",
    "    \"\"\")\n",
    "\n",
    "    gpt_questions_entry = st.text_area(\"\", height= 200, max_chars=1000) \n",
    "\n",
    "    st.caption(\"Answers to your questions will be generated by model gpt-3.5-turbo-0125. Due to a technical limitation, the model will be instructed to 'read' up to approximately 10,000 words from each file.\")\n",
    "\n",
    "    st.header(\"Consent\")\n",
    "\n",
    "    st.markdown(\"\"\"By running the Empirical Legal Research Kickstarter, you agree that the data and/or information this form provides will be temporarily stored on one or more of Ben Chen's electronic devices and/or one or more remote servers for the purpose of producing an output containing data in relation to your uploaded file(s). Any such data and/or information may also be given to GPT for the same purpose should you choose to use GPT.\n",
    "\"\"\")\n",
    "    \n",
    "    consent =  st.checkbox('Yes, I agree.', value = False)\n",
    "\n",
    "    st.markdown(\"\"\"If you do not agree, then please feel free to close this form. Any data or information this form provides will neither be received by Ben Chen nor be sent to GPT.\n",
    "\"\"\")\n",
    "\n",
    "    st.header(\"Next Steps\")\n",
    "\n",
    "    st.markdown(\"\"\"**:orange[Once your files are uploaded,] you can run the Empirical Legal Research Kickstarter.** A spreadsheet which hopefully has the data you seek will be available for download in about 2-3 minutes.\n",
    "\n",
    "You can also download a record of your responses.\n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "    run_button = st.form_submit_button('RUN the Empirical Legal Research Kickstarter')\n",
    "\n",
    "    keep_button = st.form_submit_button('DOWNLOAD your form responses')\n",
    "\n",
    "#    test_button = st.form_submit_button('Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f71a3",
   "metadata": {},
   "source": [
    "# Save and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "376a9fbd-ec89-4733-811d-f93ef508c6fb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#if test_button:\n",
    "#    for uploaded_doc in uploaded_docs:\n",
    "#        output = doc_to_text(uploaded_doc, language_entry)\n",
    "#        st.write(output)\n",
    "\n",
    "#    for uploaded_image in uploaded_images:\n",
    "#        output = image_to_text(uploaded_image, language_entry)\n",
    "#        st.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98a8fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_button:\n",
    "\n",
    "    if ((len(uploaded_docs) == 0) and (len(uploaded_images) == 0)):\n",
    "\n",
    "        st.write('You must upload some file(s).')\n",
    "\n",
    "    elif int(consent) == 0:\n",
    "        st.write(\"You must click on 'Yes, I agree.' to run the Empirical Legal Research Kickstarter.\")\n",
    "\n",
    "    elif len(gpt_questions_entry) < 5:\n",
    "\n",
    "        st.write('You must enter some question(s) for GPT.')\n",
    "\n",
    "    elif '@' not in str(email_entry):\n",
    "        st.write('You must enter a valid email address to use GPT.')\n",
    "\n",
    "    else:\n",
    "\n",
    "        st.markdown(\"\"\"Your results will be available for download soon. The estimated waiting time is about 2-3 minutes. Please be patient if you have uploaded a very large file.\n",
    "\n",
    "If this program produces an error (in red) or an unexpected spreadsheet, please double-check your uploaded file(s) and language choice and try again.\n",
    "\"\"\")\n",
    "        \n",
    "        #Using own GPT\n",
    "    \n",
    "        gpt_api_key_entry = st.secrets[\"openai\"][\"gpt_api_key\"]\n",
    "    \n",
    "        #Create spreadsheet of responses\n",
    "        df_master = create_df()\n",
    "    \n",
    "        #Obtain google spreadsheet\n",
    "    \n",
    "       # conn = st.connection(\"gsheets_uk\", type=GSheetsConnection)\n",
    "        #df_google = conn.read()\n",
    "        #df_google = df_google.fillna('')\n",
    "        #df_google=df_google[df_google[\"Processed\"]!='']\n",
    "        \n",
    "        #Upload placeholder record onto Google sheet\n",
    "        #df_plaeceholdeer = pd.concat([df_google, df_master])\n",
    "        #conn.update(worksheet=\"UK\", data=df_plaeceholdeer, )\n",
    "\n",
    "        #Produce results\n",
    "\n",
    "        df_individual_output = run(df_master, uploaded_docs, uploaded_images)\n",
    "\n",
    "        #Keep record on Google sheet\n",
    "        \n",
    "        df_master[\"Processed\"] = datetime.now()\n",
    "\n",
    "        df_master.pop(\"Your GPT API key\")\n",
    "        \n",
    "        #df_to_update = pd.concat([df_google, df_master])\n",
    "        \n",
    "        #conn.update(worksheet=\"UK\", data=df_to_update, )\n",
    "\n",
    "        st.write(\"Your results are now available for download. Thank you for using the Empirical Legal Research Kickstarter.\")\n",
    "        \n",
    "        #Button for downloading results\n",
    "        output_name = df_master.loc[0, 'Your name'] + '_' + str(today_in_nums) + '_results'\n",
    "\n",
    "        csv_output = convert_df_to_csv(df_individual_output)\n",
    "        \n",
    "        ste.download_button(\n",
    "            label=\"Download your results as a CSV (for use in Excel etc)\", \n",
    "            data = csv_output,\n",
    "            file_name= output_name + '.csv', \n",
    "            mime= \"text/csv\", \n",
    "#            key='download-csv'\n",
    "        )\n",
    "\n",
    "        json_output = convert_df_to_json(df_individual_output)\n",
    "        \n",
    "        ste.download_button(\n",
    "            label=\"Download your results as a JSON\", \n",
    "            data = json_output,\n",
    "            file_name= output_name + '.json', \n",
    "            mime= \"application/json\", \n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4873fffb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if keep_button:\n",
    "\n",
    "    if ((len(uploaded_docs) == 0) and (len(uploaded_images) == 0)):\n",
    "\n",
    "        st.write('You must upload some file(s).')\n",
    "\n",
    "    elif len(gpt_questions_entry) < 5:\n",
    "\n",
    "        st.write('You must enter some question(s) for GPT.')\n",
    "\n",
    "    else:\n",
    "\n",
    "        #Using own GPT API key here\n",
    "    \n",
    "        gpt_api_key_entry = ''\n",
    "        \n",
    "        df_master = create_df()\n",
    "    \n",
    "        df_master.pop(\"Your GPT API key\")\n",
    "    \n",
    "        df_master.pop(\"Processed\")\n",
    "    \n",
    "        responses_output_name = df_master.loc[0, 'Your name'] + '_' + str(today_in_nums) + '_responses'\n",
    "    \n",
    "        #Produce a file to download\n",
    "    \n",
    "        csv = convert_df_to_csv(df_master)\n",
    "        \n",
    "        ste.download_button(\n",
    "            label=\"Download as a CSV (for use in Excel etc)\", \n",
    "            data = csv,\n",
    "            file_name=responses_output_name + '.csv', \n",
    "            mime= \"text/csv\", \n",
    "    #            key='download-csv'\n",
    "        )\n",
    "    \n",
    "        json = convert_df_to_json(df_master)\n",
    "        \n",
    "        ste.download_button(\n",
    "            label=\"Download as a JSON\", \n",
    "            data = json,\n",
    "            file_name= responses_output_name + '.json', \n",
    "            mime= \"application/json\", \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "995962d0-2a30-4327-9a0e-55b2f3e27b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if return_button:\n",
    "\n",
    "    st.switch_page(\"Home.py\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
